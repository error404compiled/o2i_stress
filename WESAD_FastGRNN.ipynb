{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WESAD FastGRNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapted from Microsoft's notebooks, available at https://github.com/microsoft/EdgeML authored by Dennis et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "import os\n",
    "import datetime as datetime\n",
    "import pickle as pkl\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pathlib\n",
    "from os import mkdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(dirname):\n",
    "    x_train = np.load(dirname + '/' + 'x_train.npy')\n",
    "    y_train = np.load(dirname + '/' + 'y_train.npy')\n",
    "    x_test = np.load(dirname + '/' + 'x_test.npy')\n",
    "    y_test = np.load(dirname + '/' + 'y_test.npy')\n",
    "    x_val = np.load(dirname + '/' + 'x_val.npy')\n",
    "    y_val = np.load(dirname + '/' + 'y_val.npy')\n",
    "    return x_train, y_train, x_test, y_test, x_val, y_val\n",
    "def makeEMIData(subinstanceLen, subinstanceStride, sourceDir, outDir):\n",
    "    x_train, y_train, x_test, y_test, x_val, y_val = loadData(sourceDir)\n",
    "    x, y = bagData(x_train, y_train, subinstanceLen, subinstanceStride)\n",
    "    np.save(outDir + '/x_train.npy', x)\n",
    "    np.save(outDir + '/y_train.npy', y)\n",
    "    print('Num train %d' % len(x))\n",
    "    x, y = bagData(x_test, y_test, subinstanceLen, subinstanceStride)\n",
    "    np.save(outDir + '/x_test.npy', x)\n",
    "    np.save(outDir + '/y_test.npy', y)\n",
    "    print('Num test %d' % len(x))\n",
    "    x, y = bagData(x_val, y_val, subinstanceLen, subinstanceStride)\n",
    "    np.save(outDir + '/x_val.npy', x)\n",
    "    np.save(outDir + '/y_val.npy', y)\n",
    "    print('Num val %d' % len(x))\n",
    "def bagData(X, Y, subinstanceLen, subinstanceStride):\n",
    "    numClass = 3\n",
    "    numSteps = 175\n",
    "    numFeats = 8\n",
    "    assert X.ndim == 3\n",
    "    assert X.shape[1] == numSteps\n",
    "    assert X.shape[2] == numFeats\n",
    "    assert subinstanceLen <= numSteps\n",
    "    assert subinstanceLen > 0\n",
    "    assert subinstanceStride <= numSteps\n",
    "    assert subinstanceStride >= 0\n",
    "    assert len(X) == len(Y)\n",
    "    assert Y.ndim == 2\n",
    "    assert Y.shape[1] == numClass\n",
    "    x_bagged = []\n",
    "    y_bagged = []\n",
    "    for i, point in enumerate(X[:, :, :]):\n",
    "        instanceList = []\n",
    "        start = 0\n",
    "        end = subinstanceLen\n",
    "        while True:\n",
    "            x = point[start:end, :]\n",
    "            if len(x) < subinstanceLen:\n",
    "                x_ = np.zeros([subinstanceLen, x.shape[1]])\n",
    "                x_[:len(x), :] = x[:, :]\n",
    "                x = x_\n",
    "            instanceList.append(x)\n",
    "            if end >= numSteps:\n",
    "                break\n",
    "            start += subinstanceStride\n",
    "            end += subinstanceStride\n",
    "        bag = np.array(instanceList)\n",
    "        numSubinstance = bag.shape[0]\n",
    "        label = Y[i]\n",
    "        label = np.argmax(label)\n",
    "        labelBag = np.zeros([numSubinstance, numClass])\n",
    "        labelBag[:, label] = 1\n",
    "        x_bagged.append(bag)\n",
    "        label = np.array(labelBag)\n",
    "        y_bagged.append(label)\n",
    "    return np.array(x_bagged), np.array(y_bagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num train 6108\n",
      "Num test 1697\n",
      "Num val 679\n"
     ]
    }
   ],
   "source": [
    "subinstanceLen=88\n",
    "subinstanceStride=30\n",
    "extractedDir = '/home/deepin/Desktop/projects/hrv/WESAD/'\n",
    "mkdir('/home/deepin/Desktop/projects/hrv/WESAD/fast_grnn/88_30')\n",
    "rawDir = extractedDir + '/RAW'\n",
    "sourceDir = rawDir\n",
    "outDir = extractedDir + 'fast_grnn' '/%d_%d/' % (subinstanceLen, subinstanceStride)\n",
    "makeEMIData(subinstanceLen, subinstanceStride, sourceDir, outDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/deepin/Desktop/projects/hrv/WESAD/fast_grnn/88_30/'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outDir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-19T12:39:06.272261Z",
     "start_time": "2018-08-19T12:39:05.330668Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-02 09:27:15.366172: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-02 09:27:16.527333: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-03-02 09:27:18.246683: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-02 09:27:18.246841: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-02 09:27:18.246856: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] ='0'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-19T12:39:06.292205Z",
     "start_time": "2018-08-19T12:39:06.274254Z"
    }
   },
   "outputs": [],
   "source": [
    "# Network parameters for our FastGRNN + FC Layer\n",
    "NUM_HIDDEN = 128\n",
    "NUM_TIMESTEPS = 88\n",
    "NUM_FEATS = 8\n",
    "FORGET_BIAS = 1.0\n",
    "NUM_OUTPUT = 3\n",
    "USE_DROPOUT = False\n",
    "KEEP_PROB = 0.9\n",
    "\n",
    "# Non-linearities can be chosen among \"tanh, sigmoid, relu, quantTanh, quantSigm\"\n",
    "UPDATE_NL = \"quantTanh\"\n",
    "GATE_NL = \"quantSigm\"\n",
    "\n",
    "# Ranks of Parameter matrices for low-rank parameterisation to compress models.\n",
    "WRANK = 5\n",
    "URANK = 6\n",
    "\n",
    "# For dataset API\n",
    "PREFETCH_NUM = 5\n",
    "BATCH_SIZE = 175\n",
    "\n",
    "# Number of epochs in *one iteration*\n",
    "NUM_EPOCHS = 3\n",
    "\n",
    "# Number of iterations in *one round*. After each iteration,\n",
    "# the model is dumped to disk. At the end of the current\n",
    "# round, the best model among all the dumped models in the\n",
    "# current round is picked up..\n",
    "NUM_ITER = 4\n",
    "\n",
    "# A round consists of multiple training iterations and a belief\n",
    "# update step using the best model from all of these iterations\n",
    "NUM_ROUNDS = 6\n",
    "\n",
    "# A staging direcory to store models\n",
    "MODEL_PREFIX = '/home/deepin/Desktop/projects/hrv/WESAD/Fast_GRNN/88_30/models/model-fgrnn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-19T12:39:06.410372Z",
     "start_time": "2018-08-19T12:39:06.294014Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape is: (6108, 4, 88, 8)\n",
      "y_train shape is: (6108, 4, 3)\n",
      "x_test shape is: (1697, 4, 88, 8)\n",
      "y_test shape is: (1697, 4, 3)\n"
     ]
    }
   ],
   "source": [
    "# Loading the data\n",
    "path='/home/deepin/Desktop/projects/hrv/WESAD/fast_grnn//88_30/'\n",
    "x_train, y_train = np.load(path + 'x_train.npy'), np.load(path + 'y_train.npy')\n",
    "x_test, y_test = np.load(path + 'x_test.npy'), np.load(path + 'y_test.npy')\n",
    "x_val, y_val = np.load(path + 'x_val.npy'), np.load(path + 'y_val.npy')\n",
    "\n",
    "# BAG_TEST, BAG_TRAIN, BAG_VAL represent bag_level labels. These are used for the label update\n",
    "# step of EMI/MI RNN\n",
    "BAG_TEST = np.argmax(y_test[:, 0, :], axis=1)\n",
    "BAG_TRAIN = np.argmax(y_train[:, 0, :], axis=1)\n",
    "BAG_VAL = np.argmax(y_val[:, 0, :], axis=1)\n",
    "NUM_SUBINSTANCE = x_train.shape[1]\n",
    "print(\"x_train shape is:\", x_train.shape)\n",
    "print(\"y_train shape is:\", y_train.shape)\n",
    "print(\"x_test shape is:\", x_test.shape)\n",
    "print(\"y_test shape is:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "# Licensed under the MIT license.\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import init_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import variable_scope as vs\n",
    "from tensorflow.python.ops import gen_math_ops\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops.rnn_cell_impl import RNNCell\n",
    "\n",
    "\n",
    "def gen_non_linearity(A, non_linearity):\n",
    "    '''\n",
    "    Returns required activation for a tensor based on the inputs\n",
    "\n",
    "    non_linearity is either a callable or a value in\n",
    "        ['tanh', 'sigmoid', 'relu', 'quantTanh', 'quantSigm', 'quantSigm4']\n",
    "    '''\n",
    "    if non_linearity == \"tanh\":\n",
    "        return math_ops.tanh(A)\n",
    "    elif non_linearity == \"sigmoid\":\n",
    "        return math_ops.sigmoid(A)\n",
    "    elif non_linearity == \"relu\":\n",
    "        return gen_math_ops.maximum(A, 0.0)\n",
    "    elif non_linearity == \"quantTanh\":\n",
    "        return gen_math_ops.maximum(gen_math_ops.minimum(A, 1.0), -1.0)\n",
    "    elif non_linearity == \"quantSigm\":\n",
    "        A = (A + 1.0) / 2.0\n",
    "        return gen_math_ops.maximum(gen_math_ops.minimum(A, 1.0), 0.0)\n",
    "    elif non_linearity == \"quantSigm4\":\n",
    "        A = (A + 2.0) / 4.0\n",
    "        return gen_math_ops.maximum(gen_math_ops.minimum(A, 1.0), 0.0)\n",
    "    else:\n",
    "        # non_linearity is a user specified function\n",
    "        if not callable(non_linearity):\n",
    "            raise ValueError(\"non_linearity is either a callable or a value \" +\n",
    "                             + \"['tanh', 'sigmoid', 'relu', 'quantTanh', \" +\n",
    "                             \"'quantSigm'\")\n",
    "        return non_linearity(A)\n",
    "\n",
    "\n",
    "class FastGRNNCell(RNNCell):\n",
    "    '''\n",
    "    FastGRNN Cell with Both Full Rank and Low Rank Formulations\n",
    "    Has multiple activation functions for the gates\n",
    "    hidden_size = # hidden units\n",
    "\n",
    "    gate_non_linearity = nonlinearity for the gate can be chosen from\n",
    "    [tanh, sigmoid, relu, quantTanh, quantSigm]\n",
    "    update_non_linearity = nonlinearity for final rnn update\n",
    "    can be chosen from [tanh, sigmoid, relu, quantTanh, quantSigm]\n",
    "\n",
    "    wRank = rank of W matrix (creates two matrices if not None)\n",
    "    uRank = rank of U matrix (creates two matrices if not None)\n",
    "    zetaInit = init for zeta, the scale param\n",
    "    nuInit = init for nu, the translation param\n",
    "\n",
    "    FastGRNN architecture and compression techniques are found in\n",
    "    FastGRNN(LINK) paper\n",
    "\n",
    "    Basic architecture is like:\n",
    "\n",
    "    z_t = gate_nl(Wx_t + Uh_{t-1} + B_g)\n",
    "    h_t^ = update_nl(Wx_t + Uh_{t-1} + B_h)\n",
    "    h_t = z_t*h_{t-1} + (sigmoid(zeta)(1-z_t) + sigmoid(nu))*h_t^\n",
    "\n",
    "    W and U can further parameterised into low rank version by\n",
    "    W = matmul(W_1, W_2) and U = matmul(U_1, U_2)\n",
    "    '''\n",
    "\n",
    "    def __init__(self, hidden_size, gate_non_linearity=\"sigmoid\",\n",
    "                 update_non_linearity=\"tanh\", wRank=None, uRank=None,\n",
    "                 zetaInit=1.0, nuInit=-4.0, name=\"FastGRNN\", reuse=None):\n",
    "        super(FastGRNNCell, self).__init__(_reuse=reuse)\n",
    "        self._hidden_size = hidden_size\n",
    "        self._gate_non_linearity = gate_non_linearity\n",
    "        self._update_non_linearity = update_non_linearity\n",
    "        self._num_weight_matrices = [1, 1]\n",
    "        self._wRank = wRank\n",
    "        self._uRank = uRank\n",
    "        self._zetaInit = zetaInit\n",
    "        self._nuInit = nuInit\n",
    "        if wRank is not None:\n",
    "            self._num_weight_matrices[0] += 1\n",
    "        if uRank is not None:\n",
    "            self._num_weight_matrices[1] += 1\n",
    "        self._name = name\n",
    "        self._reuse = reuse\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return self._hidden_size\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self._hidden_size\n",
    "\n",
    "    @property\n",
    "    def gate_non_linearity(self):\n",
    "        return self._gate_non_linearity\n",
    "\n",
    "    @property\n",
    "    def update_non_linearity(self):\n",
    "        return self._update_non_linearity\n",
    "\n",
    "    @property\n",
    "    def wRank(self):\n",
    "        return self._wRank\n",
    "\n",
    "    @property\n",
    "    def uRank(self):\n",
    "        return self._uRank\n",
    "\n",
    "    @property\n",
    "    def num_weight_matrices(self):\n",
    "        return self._num_weight_matrices\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return self._name\n",
    "\n",
    "    @property\n",
    "    def cellType(self):\n",
    "        return \"FastGRNN\"\n",
    "\n",
    "    def call(self, inputs, state):\n",
    "        with vs.variable_scope(self._name + \"/FastGRNNcell\"):\n",
    "\n",
    "            if self._wRank is None:\n",
    "                W_matrix_init = init_ops.random_normal_initializer(\n",
    "                    mean=0.0, stddev=0.1, dtype=tf.float32)\n",
    "                self.W = vs.get_variable(\n",
    "                    \"W\", [inputs.get_shape()[-1], self._hidden_size],\n",
    "                    initializer=W_matrix_init)\n",
    "                wComp = math_ops.matmul(inputs, self.W)\n",
    "            else:\n",
    "                W_matrix_1_init = init_ops.random_normal_initializer(\n",
    "                    mean=0.0, stddev=0.1, dtype=tf.float32)\n",
    "                self.W1 = vs.get_variable(\n",
    "                    \"W1\", [inputs.get_shape()[-1], self._wRank],\n",
    "                    initializer=W_matrix_1_init)\n",
    "                W_matrix_2_init = init_ops.random_normal_initializer(\n",
    "                    mean=0.0, stddev=0.1, dtype=tf.float32)\n",
    "                self.W2 = vs.get_variable(\n",
    "                    \"W2\", [self._wRank, self._hidden_size],\n",
    "                    initializer=W_matrix_2_init)\n",
    "                wComp = math_ops.matmul(\n",
    "                    math_ops.matmul(inputs, self.W1), self.W2)\n",
    "\n",
    "            if self._uRank is None:\n",
    "                U_matrix_init = init_ops.random_normal_initializer(\n",
    "                    mean=0.0, stddev=0.1, dtype=tf.float32)\n",
    "                self.U = vs.get_variable(\n",
    "                    \"U\", [self._hidden_size, self._hidden_size],\n",
    "                    initializer=U_matrix_init)\n",
    "                uComp = math_ops.matmul(state, self.U)\n",
    "            else:\n",
    "                U_matrix_1_init = init_ops.random_normal_initializer(\n",
    "                    mean=0.0, stddev=0.1, dtype=tf.float32)\n",
    "                self.U1 = vs.get_variable(\n",
    "                    \"U1\", [self._hidden_size, self._uRank],\n",
    "                    initializer=U_matrix_1_init)\n",
    "                U_matrix_2_init = init_ops.random_normal_initializer(\n",
    "                    mean=0.0, stddev=0.1, dtype=tf.float32)\n",
    "                self.U2 = vs.get_variable(\n",
    "                    \"U2\", [self._uRank, self._hidden_size],\n",
    "                    initializer=U_matrix_2_init)\n",
    "                uComp = math_ops.matmul(\n",
    "                    math_ops.matmul(state, self.U1), self.U2)\n",
    "            # Init zeta to 6.0 and nu to -6.0 if this doesn't give good\n",
    "            # results. The inits are hyper-params.\n",
    "            zeta_init = init_ops.constant_initializer(\n",
    "                self._zetaInit, dtype=tf.float32)\n",
    "            self.zeta = vs.get_variable(\"zeta\", [1, 1], initializer=zeta_init)\n",
    "\n",
    "            nu_init = init_ops.constant_initializer(\n",
    "                self._nuInit, dtype=tf.float32)\n",
    "            self.nu = vs.get_variable(\"nu\", [1, 1], initializer=nu_init)\n",
    "\n",
    "            pre_comp = wComp + uComp\n",
    "\n",
    "            bias_gate_init = init_ops.constant_initializer(\n",
    "                1.0, dtype=tf.float32)\n",
    "            self.bias_gate = vs.get_variable(\n",
    "                \"B_g\", [1, self._hidden_size], initializer=bias_gate_init)\n",
    "            z = gen_non_linearity(pre_comp + self.bias_gate,\n",
    "                                  self._gate_non_linearity)\n",
    "\n",
    "            bias_update_init = init_ops.constant_initializer(\n",
    "                1.0, dtype=tf.float32)\n",
    "            self.bias_update = vs.get_variable(\n",
    "                \"B_h\", [1, self._hidden_size], initializer=bias_update_init)\n",
    "            c = gen_non_linearity(\n",
    "                pre_comp + self.bias_update, self._update_non_linearity)\n",
    "            new_h = z * state + (math_ops.sigmoid(self.zeta) * (1.0 - z) +\n",
    "                                 math_ops.sigmoid(self.nu)) * c\n",
    "        return new_h, new_h\n",
    "\n",
    "    def getVars(self):\n",
    "        Vars = []\n",
    "        if self._num_weight_matrices[0] == 1:\n",
    "            Vars.append(self.W)\n",
    "        else:\n",
    "            Vars.extend([self.W1, self.W2])\n",
    "\n",
    "        if self._num_weight_matrices[1] == 1:\n",
    "            Vars.append(self.U)\n",
    "        else:\n",
    "            Vars.extend([self.U1, self.U2])\n",
    "\n",
    "        Vars.extend([self.bias_gate, self.bias_update])\n",
    "        Vars.extend([self.zeta, self.nu])\n",
    "\n",
    "        return Vars\n",
    "\n",
    "\n",
    "class FastRNNCell(RNNCell):\n",
    "    '''\n",
    "    FastRNN Cell with Both Full Rank and Low Rank Formulations\n",
    "    Has multiple activation functions for the gates\n",
    "    hidden_size = # hidden units\n",
    "\n",
    "    update_non_linearity = nonlinearity for final rnn update\n",
    "    can be chosen from [tanh, sigmoid, relu, quantTanh, quantSigm]\n",
    "\n",
    "    wRank = rank of W matrix (creates two matrices if not None)\n",
    "    uRank = rank of U matrix (creates two matrices if not None)\n",
    "    alphaInit = init for alpha, the update scalar\n",
    "    betaInit = init for beta, the weight for previous state\n",
    "\n",
    "    FastRNN architecture and compression techniques are found in\n",
    "    FastGRNN(LINK) paper\n",
    "\n",
    "    Basic architecture is like:\n",
    "\n",
    "    h_t^ = update_nl(Wx_t + Uh_{t-1} + B_h)\n",
    "    h_t = sigmoid(beta)*h_{t-1} + sigmoid(alpha)*h_t^\n",
    "\n",
    "    W and U can further parameterised into low rank version by\n",
    "    W = matmul(W_1, W_2) and U = matmul(U_1, U_2)\n",
    "    '''\n",
    "\n",
    "    def __init__(self, hidden_size, update_non_linearity=\"tanh\",\n",
    "                 wRank=None, uRank=None, alphaInit=-3.0, betaInit=3.0,\n",
    "                 name=\"FastRNN\", reuse=None):\n",
    "        super(FastRNNCell, self).__init__(_reuse=reuse)\n",
    "        self._hidden_size = hidden_size\n",
    "        self._update_non_linearity = update_non_linearity\n",
    "        self._num_weight_matrices = [1, 1]\n",
    "        self._wRank = wRank\n",
    "        self._uRank = uRank\n",
    "        self._alphaInit = alphaInit\n",
    "        self._betaInit = betaInit\n",
    "        if wRank is not None:\n",
    "            self._num_weight_matrices[0] += 1\n",
    "        if uRank is not None:\n",
    "            self._num_weight_matrices[1] += 1\n",
    "        self._name = name\n",
    "        self._reuse = reuse\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return self._hidden_size\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self._hidden_size\n",
    "\n",
    "    @property\n",
    "    def update_non_linearity(self):\n",
    "        return self._update_non_linearity\n",
    "\n",
    "    @property\n",
    "    def wRank(self):\n",
    "        return self._wRank\n",
    "\n",
    "    @property\n",
    "    def uRank(self):\n",
    "        return self._uRank\n",
    "\n",
    "    @property\n",
    "    def num_weight_matrices(self):\n",
    "        return self._num_weight_matrices\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return self._name\n",
    "\n",
    "    @property\n",
    "    def cellType(self):\n",
    "        return \"FastRNN\"\n",
    "\n",
    "    def call(self, inputs, state):\n",
    "        with vs.variable_scope(self._name + \"/FastRNNcell\"):\n",
    "\n",
    "            if self._wRank is None:\n",
    "                W_matrix_init = init_ops.random_normal_initializer(\n",
    "                    mean=0.0, stddev=0.1, dtype=tf.float32)\n",
    "                self.W = vs.get_variable(\n",
    "                    \"W\", [inputs.get_shape()[-1], self._hidden_size],\n",
    "                    initializer=W_matrix_init)\n",
    "                wComp = math_ops.matmul(inputs, self.W)\n",
    "            else:\n",
    "                W_matrix_1_init = init_ops.random_normal_initializer(\n",
    "                    mean=0.0, stddev=0.1, dtype=tf.float32)\n",
    "                self.W1 = vs.get_variable(\n",
    "                    \"W1\", [inputs.get_shape()[-1], self._wRank],\n",
    "                    initializer=W_matrix_1_init)\n",
    "                W_matrix_2_init = init_ops.random_normal_initializer(\n",
    "                    mean=0.0, stddev=0.1, dtype=tf.float32)\n",
    "                self.W2 = vs.get_variable(\n",
    "                    \"W2\", [self._wRank, self._hidden_size],\n",
    "                    initializer=W_matrix_2_init)\n",
    "                wComp = math_ops.matmul(\n",
    "                    math_ops.matmul(inputs, self.W1), self.W2)\n",
    "\n",
    "            if self._uRank is None:\n",
    "                U_matrix_init = init_ops.random_normal_initializer(\n",
    "                    mean=0.0, stddev=0.1, dtype=tf.float32)\n",
    "                self.U = vs.get_variable(\n",
    "                    \"U\", [self._hidden_size, self._hidden_size],\n",
    "                    initializer=U_matrix_init)\n",
    "                uComp = math_ops.matmul(state, self.U)\n",
    "            else:\n",
    "                U_matrix_1_init = init_ops.random_normal_initializer(\n",
    "                    mean=0.0, stddev=0.1, dtype=tf.float32)\n",
    "                self.U1 = vs.get_variable(\n",
    "                    \"U1\", [self._hidden_size, self._uRank],\n",
    "                    initializer=U_matrix_1_init)\n",
    "                U_matrix_2_init = init_ops.random_normal_initializer(\n",
    "                    mean=0.0, stddev=0.1, dtype=tf.float32)\n",
    "                self.U2 = vs.get_variable(\n",
    "                    \"U2\", [self._uRank, self._hidden_size],\n",
    "                    initializer=U_matrix_2_init)\n",
    "                uComp = math_ops.matmul(\n",
    "                    math_ops.matmul(state, self.U1), self.U2)\n",
    "\n",
    "            alpha_init = init_ops.constant_initializer(\n",
    "                self._alphaInit, dtype=tf.float32)\n",
    "            self.alpha = vs.get_variable(\n",
    "                \"alpha\", [1, 1], initializer=alpha_init)\n",
    "\n",
    "            beta_init = init_ops.constant_initializer(\n",
    "                self._betaInit, dtype=tf.float32)\n",
    "            self.beta = vs.get_variable(\"beta\", [1, 1], initializer=beta_init)\n",
    "\n",
    "            pre_comp = wComp + uComp\n",
    "\n",
    "            bias_update_init = init_ops.constant_initializer(\n",
    "                1.0, dtype=tf.float32)\n",
    "            self.bias_update = vs.get_variable(\n",
    "                \"B_h\", [1, self._hidden_size], initializer=bias_update_init)\n",
    "            c = gen_non_linearity(\n",
    "                pre_comp + self.bias_update, self._update_non_linearity)\n",
    "\n",
    "            new_h = math_ops.sigmoid(self.beta) * \\\n",
    "                state + math_ops.sigmoid(self.alpha) * c\n",
    "        return new_h, new_h\n",
    "\n",
    "    def getVars(self):\n",
    "        Vars = []\n",
    "        if self._num_weight_matrices[0] == 1:\n",
    "            Vars.append(self.W)\n",
    "        else:\n",
    "            Vars.extend([self.W1, self.W2])\n",
    "\n",
    "        if self._num_weight_matrices[1] == 1:\n",
    "            Vars.append(self.U)\n",
    "        else:\n",
    "            Vars.extend([self.U1, self.U2])\n",
    "\n",
    "        Vars.extend([self.bias_update])\n",
    "        Vars.extend([self.alpha, self.beta])\n",
    "\n",
    "        return Vars\n",
    "\n",
    "\n",
    "class LSTMLRCell(RNNCell):\n",
    "    '''\n",
    "    LR - Low Rank\n",
    "    LSTM LR Cell with Both Full Rank and Low Rank Formulations\n",
    "    Has multiple activation functions for the gates\n",
    "    hidden_size = # hidden units\n",
    "\n",
    "    gate_non_linearity = nonlinearity for the gate can be chosen from\n",
    "    [tanh, sigmoid, relu, quantTanh, quantSigm]\n",
    "    update_non_linearity = nonlinearity for final rnn update\n",
    "    can be chosen from [tanh, sigmoid, relu, quantTanh, quantSigm]\n",
    "\n",
    "    wRank = rank of all W matrices\n",
    "    (creates 5 matrices if not None else creates 4 matrices)\n",
    "    uRank = rank of all U matrices\n",
    "    (creates 5 matrices if not None else creates 4 matrices)\n",
    "\n",
    "    LSTM architecture and compression techniques are found in\n",
    "    LSTM paper\n",
    "\n",
    "    Basic architecture is like:\n",
    "\n",
    "    f_t = gate_nl(W1x_t + U1h_{t-1} + B_f)\n",
    "    i_t = gate_nl(W2x_t + U2h_{t-1} + B_i)\n",
    "    C_t^ = update_nl(W3x_t + U3h_{t-1} + B_c)\n",
    "    o_t = gate_nl(W4x_t + U4h_{t-1} + B_o)\n",
    "    C_t = f_t*C_{t-1} + i_t*C_t^\n",
    "    h_t = o_t*update_nl(C_t)\n",
    "\n",
    "    Wi and Ui can further parameterised into low rank version by\n",
    "    Wi = matmul(W, W_i) and Ui = matmul(U, U_i)\n",
    "    '''\n",
    "\n",
    "    def __init__(self, hidden_size, gate_non_linearity=\"sigmoid\",\n",
    "                 update_non_linearity=\"tanh\", wRank=None, uRank=None,\n",
    "                 name=\"LSTMLR\", reuse=None):\n",
    "        super(LSTMLRCell, self).__init__(_reuse=reuse)\n",
    "        self._hidden_size = hidden_size\n",
    "        self._gate_non_linearity = gate_non_linearity\n",
    "        self._update_non_linearity = update_non_linearity\n",
    "        self._num_weight_matrices = [4, 4]\n",
    "        self._wRank = wRank\n",
    "        self._uRank = uRank\n",
    "        if wRank is not None:\n",
    "            self._num_weight_matrices[0] += 1\n",
    "        if uRank is not None:\n",
    "            self._num_weight_matrices[1] += 1\n",
    "        self._name = name\n",
    "        self._reuse = reuse\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return 2 * self._hidden_size\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self._hidden_size\n",
    "\n",
    "    @property\n",
    "    def gate_non_linearity(self):\n",
    "        return self._gate_non_linearity\n",
    "\n",
    "    @property\n",
    "    def update_non_linearity(self):\n",
    "        return self._update_non_linearity\n",
    "\n",
    "    @property\n",
    "    def wRank(self):\n",
    "        return self._wRank\n",
    "\n",
    "    @property\n",
    "    def uRank(self):\n",
    "        return self._uRank\n",
    "\n",
    "    @property\n",
    "    def num_weight_matrices(self):\n",
    "        return self._num_weight_matrices\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return self._name\n",
    "\n",
    "    @property\n",
    "    def cellType(self):\n",
    "        return \"LSTMLR\"\n",
    "\n",
    "    def call(self, inputs, state):\n",
    "        c, h = array_ops.split(value=state, num_or_size_splits=2, axis=1)\n",
    "        with vs.variable_scope(self._name + \"/LSTMLRCell\"):\n",
    "\n",
    "            if self._wRank is None:\n",
    "                W1_matrix_init = init_ops.random_normal_initializer(\n",
    "                    mean=0.0, stddev=0.1, dtype=tf.float32)\n",
    "                self.W1 = vs.get_variable(\n",
    "                    \"W1\", [inputs.get_shape()[-1], self._hidden_size],\n",
    "                    initializer=W1_matrix_init)\n",
    "                W2_matrix_init = init_ops.random_normal_initializer(\n",
    "                    mean=0.0, stddev=0.1, dtype=tf.float32)\n",
    "                self.W2 = vs.get_variable(\n",
    "                    \"W2\", [inputs.get_shape()[-1], self._hidden_size],\n",
    "                    initializer=W2_matrix_init)\n",
    "                W3_matrix_init = init_ops.random_normal_initializer(\n",
    "                    mean=0.0, stddev=0.1, dtype=tf.float32)\n",
    "                self.W3 = vs.get_variable(\n",
    "                    \"W3\", [inputs.get_shape()[-1], self._hidden_size],\n",
    "                    initializer=W3_matrix_init)\n",
    "                W4_matrix_init = init_ops.random_normal_initializer(\n",
    "                    mean=0.0, stddev=0.1, dtype=tf.float32)\n",
    "                self.W4 = vs.get_variable(\n",
    "                    \"W4\", [inputs.get_shape()[-1], self._hidden_size],\n",
    "                    initializer=W4_matrix_init)\n",
    "                wComp1 = math_ops.matmul(inputs, self.W1)\n",
    "                wComp2 = math_ops.matmul(inputs, self.W2)\n",
    "                wComp3 = math_ops.matmul(inputs, self.W3)\n",
    "                wComp4 = math_ops.matmul(inputs, self.W4)\n",
    "            else:\n",
    "                W_matrix_r_init = init_ops.random_normal_initializer(\n",
    "                    mean=0.0, stddev=0.1, dtype=tf.float32)\n",
    "                self.W = vs.get_variable(\n",
    "                    \"W\", [inputs.get_shape()[-1], self._wRank],\n",
    "                    initializer=W_matrix_r_init)\n",
    "                W1_matrix_init = init_ops.random_normal_initializer(\n",
    "                    mean=0.0, stddev=0.1, dtype=tf.float32)\n",
    "                self.W1 = vs.get_variable(\n",
    "                    \"W1\", [self._wRank, self._hidden_size],\n",
    "                    initializer=W1_matrix_init)\n",
    "                W2_matrix_init = init_ops.random_normal_initializer(\n",
    "                    mean=0.0, stddev=0.1, dtype=tf.float32)\n",
    "                self.W2 = vs.get_variable(\n",
    "                    \"W2\", [self._wRank, self._hidden_size],\n",
    "                    initializer=W2_matrix_init)\n",
    "                W3_matrix_init = init_ops.random_normal_initializer(\n",
    "                    mean=0.0, stddev=0.1, dtype=tf.float32)\n",
    "                self.W3 = vs.get_variable(\n",
    "                    \"W3\", [self._wRank, self._hidden_size],\n",
    "                    initializer=W3_matrix_init)\n",
    "                W4_matrix_init = init_ops.random_normal_initializer(\n",
    "                    mean=0.0, stddev=0.1, dtype=tf.float32)\n",
    "                self.W4 = vs.get_variable(\n",
    "                    \"W4\", [self._wRank, self._hidden_size],\n",
    "                    initializer=W4_matrix_init)\n",
    "                wComp1 = math_ops.matmul(\n",
    "                    math_ops.matmul(inputs, self.W), self.W1)\n",
    "                wComp2 = math_ops.matmul(\n",
    "                    math_ops.matmul(inputs, self.W), self.W2)\n",
    "                wComp3 = math_ops.matmul(\n",
    "                    math_ops.matmul(inputs, self.W), self.W3)\n",
    "                wComp4 = math_ops.matmul(\n",
    "                    math_ops.matmul(inputs, self.W), self.W4)\n",
    "            if self._uRank is None:\n",
    "                U1_matrix_init = init_ops.random_normal_initializer(\n",
    "                    mean=0.0, stddev=0.1, dtype=tf.float32)\n",
    "                self.U1 = vs.get_variable(\n",
    "                    \"U1\", [self._hidden_size, self._hidden_size],\n",
    "                    initializer=U1_matrix_init)\n",
    "                U2_matrix_init = init_ops.random_normal_initializer(\n",
    "                    mean=0.0, stddev=0.1, dtype=tf.float32)\n",
    "                self.U2 = vs.get_variable(\n",
    "                    \"U2\", [self._hidden_size, self._hidden_size],\n",
    "                    initializer=U2_matrix_init)\n",
    "                U3_matrix_init = init_ops.random_normal_initializer(\n",
    "                    mean=0.0, stddev=0.1, dtype=tf.float32)\n",
    "                self.U3 = vs.get_variable(\n",
    "                    \"U3\", [self._hidden_size, self._hidden_size],\n",
    "                    initializer=U3_matrix_init)\n",
    "                U4_matrix_init = init_ops.random_normal_initializer(\n",
    "                    mean=0.0, stddev=0.1, dtype=tf.float32)\n",
    "                self.U4 = vs.get_variable(\n",
    "                    \"U4\", [self._hidden_size, self._hidden_size],\n",
    "                    initializer=U4_matrix_init)\n",
    "                uComp1 = math_ops.matmul(h, self.U1)\n",
    "                uComp2 = math_ops.matmul(h, self.U2)\n",
    "                uComp3 = math_ops.matmul(h, self.U3)\n",
    "                uComp4 = math_ops.matmul(h, self.U4)\n",
    "            else:\n",
    "                U_matrix_r_init = init_ops.random_normal_initializer(\n",
    "                    mean=0.0, stddev=0.1, dtype=tf.float32)\n",
    "                self.U = vs.get_variable(\n",
    "                    \"U\", [self._hidden_size, self._uRank],\n",
    "                    initializer=U_matrix_r_init)\n",
    "                U1_matrix_init = init_ops.random_normal_initializer(\n",
    "                    mean=0.0, stddev=0.1, dtype=tf.float32)\n",
    "                self.U1 = vs.get_variable(\n",
    "                    \"U1\", [self._uRank, self._hidden_size],\n",
    "                    initializer=U1_matrix_init)\n",
    "                U2_matrix_init = init_ops.random_normal_initializer(\n",
    "                    mean=0.0, stddev=0.1, dtype=tf.float32)\n",
    "                self.U2 = vs.get_variable(\n",
    "                    \"U2\", [self._uRank, self._hidden_size],\n",
    "                    initializer=U2_matrix_init)\n",
    "                U3_matrix_init = init_ops.random_normal_initializer(\n",
    "                    mean=0.0, stddev=0.1, dtype=tf.float32)\n",
    "                self.U3 = vs.get_variable(\n",
    "                    \"U3\", [self._uRank, self._hidden_size],\n",
    "                    initializer=U3_matrix_init)\n",
    "                U4_matrix_init = init_ops.random_normal_initializer(\n",
    "                    mean=0.0, stddev=0.1, dtype=tf.float32)\n",
    "                self.U4 = vs.get_variable(\n",
    "                    \"U4\", [self._uRank, self._hidden_size],\n",
    "                    initializer=U4_matrix_init)\n",
    "\n",
    "                uComp1 = math_ops.matmul(\n",
    "                    math_ops.matmul(h, self.U), self.U1)\n",
    "                uComp2 = math_ops.matmul(\n",
    "                    math_ops.matmul(h, self.U), self.U2)\n",
    "                uComp3 = math_ops.matmul(\n",
    "                    math_ops.matmul(h, self.U), self.U3)\n",
    "                uComp4 = math_ops.matmul(\n",
    "                    math_ops.matmul(h, self.U), self.U4)\n",
    "\n",
    "            pre_comp1 = wComp1 + uComp1\n",
    "            pre_comp2 = wComp2 + uComp2\n",
    "            pre_comp3 = wComp3 + uComp3\n",
    "            pre_comp4 = wComp4 + uComp4\n",
    "\n",
    "            bias_gate_init = init_ops.constant_initializer(\n",
    "                1.0, dtype=tf.float32)\n",
    "            self.bias_f = vs.get_variable(\n",
    "                \"B_f\", [1, self._hidden_size], initializer=bias_gate_init)\n",
    "            self.bias_i = vs.get_variable(\n",
    "                \"B_i\", [1, self._hidden_size], initializer=bias_gate_init)\n",
    "            self.bias_c = vs.get_variable(\n",
    "                \"B_c\", [1, self._hidden_size], initializer=bias_gate_init)\n",
    "            self.bias_o = vs.get_variable(\n",
    "                \"B_o\", [1, self._hidden_size], initializer=bias_gate_init)\n",
    "\n",
    "            f = gen_non_linearity(pre_comp1 + self.bias_f,\n",
    "                                  self._gate_non_linearity)\n",
    "            i = gen_non_linearity(pre_comp2 + self.bias_i,\n",
    "                                  self._gate_non_linearity)\n",
    "            o = gen_non_linearity(pre_comp4 + self.bias_o,\n",
    "                                  self._gate_non_linearity)\n",
    "\n",
    "            c_ = gen_non_linearity(\n",
    "                pre_comp3 + self.bias_c, self._update_non_linearity)\n",
    "\n",
    "            new_c = f * c + i * c_\n",
    "            new_h = o * gen_non_linearity(new_c, self._update_non_linearity)\n",
    "            new_state = array_ops.concat([new_c, new_h], 1)\n",
    "\n",
    "        return new_h, new_state\n",
    "\n",
    "    def getVars(self):\n",
    "        Vars = []\n",
    "        if self._num_weight_matrices[0] == 4:\n",
    "            Vars.extend([self.W1, self.W2, self.W3, self.W4])\n",
    "        else:\n",
    "            Vars.extend([self.W, self.W1, self.W2, self.W3, self.W4])\n",
    "\n",
    "        if self._num_weight_matrices[1] == 4:\n",
    "            Vars.extend([self.U1, self.U2, self.U3, self.U4])\n",
    "        else:\n",
    "            Vars.extend([self.U, self.U1, self.U2, self.U3, self.U4])\n",
    "\n",
    "        Vars.extend([self.bias_f, self.bias_i, self.bias_c, self.bias_o])\n",
    "\n",
    "        return Vars\n",
    "\n",
    "\n",
    "class GRULRCell(RNNCell):\n",
    "    '''\n",
    "    GRU LR Cell with Both Full Rank and Low Rank Formulations\n",
    "    Has multiple activation functions for the gates\n",
    "    hidden_size = # hidden units\n",
    "\n",
    "    gate_non_linearity = nonlinearity for the gate can be chosen from\n",
    "    [tanh, sigmoid, relu, quantTanh, quantSigm]\n",
    "    update_non_linearity = nonlinearity for final rnn update\n",
    "    can be chosen from [tanh, sigmoid, relu, quantTanh, quantSigm]\n",
    "\n",
    "    wRank = rank of W matrix\n",
    "    (creates 4 matrices if not None else creates 3 matrices)\n",
    "    uRank = rank of U matrix\n",
    "    (creates 4 matrices if not None else creates 3 matrices)\n",
    "\n",
    "    GRU architecture and compression techniques are found in\n",
    "    GRU(LINK) paper\n",
    "\n",
    "    Basic architecture is like:\n",
    "\n",
    "    r_t = gate_nl(W1x_t + U1h_{t-1} + B_r)\n",
    "    z_t = gate_nl(W2x_t + U2h_{t-1} + B_g)\n",
    "    h_t^ = update_nl(W3x_t + r_t*U3(h_{t-1}) + B_h)\n",
    "    h_t = z_t*h_{t-1} + (1-z_t)*h_t^\n",
    "\n",
    "    Wi and Ui can further parameterised into low rank version by\n",
    "    Wi = matmul(W, W_i) and Ui = matmul(U, U_i)\n",
    "    '''\n",
    "\n",
    "    def __init__(self, hidden_size, gate_non_linearity=\"sigmoid\",\n",
    "                 update_non_linearity=\"tanh\", wRank=None, uRank=None,\n",
    "                 name=\"GRULR\", reuse=None):\n",
    "        super(GRULRCell, self).__init__(_reuse=reuse)\n",
    "        self._hidden_size = hidden_size\n",
    "        self._gate_non_linearity = gate_non_linearity\n",
    "        self._update_non_linearity = update_non_linearity\n",
    "        self._num_weight_matrices = [3, 3]\n",
    "        self._wRank = wRank\n",
    "        self._uRank = uRank\n",
    "        if wRank is not None:\n",
    "            self._num_weight_matrices[0] += 1\n",
    "        if uRank is not None:\n",
    "            self._num_weight_matrices[1] += 1\n",
    "        self._name = name\n",
    "        self._reuse = reuse\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return self._hidden_size\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self._hidden_size\n",
    "\n",
    "    @property\n",
    "    def gate_non_linearity(self):\n",
    "        return self._gate_non_linearity\n",
    "\n",
    "    @property\n",
    "    def update_non_linearity(self):\n",
    "        return self._update_non_linearity\n",
    "\n",
    "    @property\n",
    "    def wRank(self):\n",
    "        return self._wRank\n",
    "\n",
    "    @property\n",
    "    def uRank(self):\n",
    "        return self._uRank\n",
    "\n",
    "    @property\n",
    "    def num_weight_matrices(self):\n",
    "        return self._num_weight_matrices\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return self._name\n",
    "\n",
    "    @property\n",
    "    def cellType(self):\n",
    "        return \"GRULR\"\n",
    "\n",
    "    def call(self, inputs, state):\n",
    "        with vs.variable_scope(self._name + \"/GRULRCell\"):\n",
    "\n",
    "            if self._wRank is None:\n",
    "                W1_matrix_init = init_ops.random_normal_initializer(\n",
    "                    mean=0.0, stddev=0.1, dtype=tf.float32)\n",
    "                self.W1 = vs.get_variable(\n",
    "                    \"W1\", [inputs.get_shape()[-1], self._hidden_size],\n",
    "                    initializer=W1_matrix_init)\n",
    "                W2_matrix_init = init_ops.random_normal_initializer(\n",
    "                    mean=0.0, stddev=0.1, dtype=tf.float32)\n",
    "                self.W2 = vs.get_variable(\n",
    "                    \"W2\", [inputs.get_shape()[-1], self._hidden_size],\n",
    "                    initializer=W2_matrix_init)\n",
    "                W3_matrix_init = init_ops.random_normal_initializer(\n",
    "                    mean=0.0, stddev=0.1, dtype=tf.float32)\n",
    "                self.W3 = vs.get_variable(\n",
    "                    \"W3\", [inputs.get_shape()[-1], self._hidden_size],\n",
    "                    initializer=W3_matrix_init)\n",
    "                wComp1 = math_ops.matmul(inputs, self.W1)\n",
    "                wComp2 = math_ops.matmul(inputs, self.W2)\n",
    "                wComp3 = math_ops.matmul(inputs, self.W3)\n",
    "            else:\n",
    "                W_matrix_r_init = init_ops.random_normal_initializer(\n",
    "                    mean=0.0, stddev=0.1, dtype=tf.float32)\n",
    "                self.W = vs.get_variable(\n",
    "                    \"W\", [inputs.get_shape()[-1], self._wRank],\n",
    "                    initializer=W_matrix_r_init)\n",
    "                W1_matrix_init = init_ops.random_normal_initializer(\n",
    "                    mean=0.0, stddev=0.1, dtype=tf.float32)\n",
    "                self.W1 = vs.get_variable(\n",
    "                    \"W1\", [self._wRank, self._hidden_size],\n",
    "                    initializer=W1_matrix_init)\n",
    "                W2_matrix_init = init_ops.random_normal_initializer(\n",
    "                    mean=0.0, stddev=0.1, dtype=tf.float32)\n",
    "                self.W2 = vs.get_variable(\n",
    "                    \"W2\", [self._wRank, self._hidden_size],\n",
    "                    initializer=W2_matrix_init)\n",
    "                W3_matrix_init = init_ops.random_normal_initializer(\n",
    "                    mean=0.0, stddev=0.1, dtype=tf.float32)\n",
    "                self.W3 = vs.get_variable(\n",
    "                    \"W3\", [self._wRank, self._hidden_size],\n",
    "                    initializer=W3_matrix_init)\n",
    "                wComp1 = math_ops.matmul(\n",
    "                    math_ops.matmul(inputs, self.W), self.W1)\n",
    "                wComp2 = math_ops.matmul(\n",
    "                    math_ops.matmul(inputs, self.W), self.W2)\n",
    "                wComp3 = math_ops.matmul(\n",
    "                    math_ops.matmul(inputs, self.W), self.W3)\n",
    "\n",
    "            if self._uRank is None:\n",
    "                U1_matrix_init = init_ops.random_normal_initializer(\n",
    "                    mean=0.0, stddev=0.1, dtype=tf.float32)\n",
    "                self.U1 = vs.get_variable(\n",
    "                    \"U1\", [self._hidden_size, self._hidden_size],\n",
    "                    initializer=U1_matrix_init)\n",
    "                U2_matrix_init = init_ops.random_normal_initializer(\n",
    "                    mean=0.0, stddev=0.1, dtype=tf.float32)\n",
    "                self.U2 = vs.get_variable(\n",
    "                    \"U2\", [self._hidden_size, self._hidden_size],\n",
    "                    initializer=U2_matrix_init)\n",
    "                U3_matrix_init = init_ops.random_normal_initializer(\n",
    "                    mean=0.0, stddev=0.1, dtype=tf.float32)\n",
    "                self.U3 = vs.get_variable(\n",
    "                    \"U3\", [self._hidden_size, self._hidden_size],\n",
    "                    initializer=U3_matrix_init)\n",
    "                uComp1 = math_ops.matmul(state, self.U1)\n",
    "                uComp2 = math_ops.matmul(state, self.U2)\n",
    "            else:\n",
    "                U_matrix_r_init = init_ops.random_normal_initializer(\n",
    "                    mean=0.0, stddev=0.1, dtype=tf.float32)\n",
    "                self.U = vs.get_variable(\n",
    "                    \"U\", [self._hidden_size, self._uRank],\n",
    "                    initializer=U_matrix_r_init)\n",
    "                U1_matrix_init = init_ops.random_normal_initializer(\n",
    "                    mean=0.0, stddev=0.1, dtype=tf.float32)\n",
    "                self.U1 = vs.get_variable(\n",
    "                    \"U1\", [self._uRank, self._hidden_size],\n",
    "                    initializer=U1_matrix_init)\n",
    "                U2_matrix_init = init_ops.random_normal_initializer(\n",
    "                    mean=0.0, stddev=0.1, dtype=tf.float32)\n",
    "                self.U2 = vs.get_variable(\n",
    "                    \"U2\", [self._uRank, self._hidden_size],\n",
    "                    initializer=U2_matrix_init)\n",
    "                U3_matrix_init = init_ops.random_normal_initializer(\n",
    "                    mean=0.0, stddev=0.1, dtype=tf.float32)\n",
    "                self.U3 = vs.get_variable(\n",
    "                    \"U3\", [self._uRank, self._hidden_size],\n",
    "                    initializer=U3_matrix_init)\n",
    "                uComp1 = math_ops.matmul(\n",
    "                    math_ops.matmul(state, self.U), self.U1)\n",
    "                uComp2 = math_ops.matmul(\n",
    "                    math_ops.matmul(state, self.U), self.U2)\n",
    "\n",
    "            pre_comp1 = wComp1 + uComp1\n",
    "            pre_comp2 = wComp2 + uComp2\n",
    "\n",
    "            bias_r_init = init_ops.constant_initializer(\n",
    "                1.0, dtype=tf.float32)\n",
    "            self.bias_r = vs.get_variable(\n",
    "                \"B_r\", [1, self._hidden_size], initializer=bias_r_init)\n",
    "            r = gen_non_linearity(pre_comp1 + self.bias_r,\n",
    "                                  self._gate_non_linearity)\n",
    "\n",
    "            bias_gate_init = init_ops.constant_initializer(\n",
    "                1.0, dtype=tf.float32)\n",
    "            self.bias_gate = vs.get_variable(\n",
    "                \"B_g\", [1, self._hidden_size], initializer=bias_gate_init)\n",
    "            z = gen_non_linearity(pre_comp2 + self.bias_gate,\n",
    "                                  self._gate_non_linearity)\n",
    "\n",
    "            if self._uRank is None:\n",
    "                pre_comp3 = wComp3 + math_ops.matmul(r * state, self.U3)\n",
    "            else:\n",
    "                pre_comp3 = wComp3 + \\\n",
    "                    math_ops.matmul(math_ops.matmul(\n",
    "                        r * state, self.U), self.U3)\n",
    "\n",
    "            bias_update_init = init_ops.constant_initializer(\n",
    "                1.0, dtype=tf.float32)\n",
    "            self.bias_update = vs.get_variable(\n",
    "                \"B_h\", [1, self._hidden_size], initializer=bias_update_init)\n",
    "            c = gen_non_linearity(\n",
    "                pre_comp3 + self.bias_update, self._update_non_linearity)\n",
    "\n",
    "            new_h = z * state + (1.0 - z) * c\n",
    "\n",
    "        return new_h, new_h\n",
    "\n",
    "    def getVars(self):\n",
    "        Vars = []\n",
    "        if self._num_weight_matrices[0] == 3:\n",
    "            Vars.extend([self.W1, self.W2, self.W3])\n",
    "        else:\n",
    "            Vars.extend([self.W, self.W1, self.W2, self.W3])\n",
    "\n",
    "        if self._num_weight_matrices[1] == 3:\n",
    "            Vars.extend([self.U1, self.U2, self.U3])\n",
    "        else:\n",
    "            Vars.extend([self.U, self.U1, self.U2, self.U3])\n",
    "\n",
    "        Vars.extend([self.bias_r, self.bias_gate, self.bias_update])\n",
    "\n",
    "        return Vars\n",
    "\n",
    "\n",
    "class UGRNNLRCell(RNNCell):\n",
    "    '''\n",
    "    UGRNN LR Cell with Both Full Rank and Low Rank Formulations\n",
    "    Has multiple activation functions for the gates\n",
    "    hidden_size = # hidden units\n",
    "\n",
    "    gate_non_linearity = nonlinearity for the gate can be chosen from\n",
    "    [tanh, sigmoid, relu, quantTanh, quantSigm]\n",
    "    update_non_linearity = nonlinearity for final rnn update\n",
    "    can be chosen from [tanh, sigmoid, relu, quantTanh, quantSigm]\n",
    "\n",
    "    wRank = rank of W matrix\n",
    "    (creates 3 matrices if not None else creates 2 matrices)\n",
    "    uRank = rank of U matrix\n",
    "    (creates 3 matrices if not None else creates 2 matrices)\n",
    "\n",
    "    UGRNN architecture and compression techniques are found in\n",
    "    UGRNN(LINK) paper\n",
    "\n",
    "    Basic architecture is like:\n",
    "\n",
    "    z_t = gate_nl(W1x_t + U1h_{t-1} + B_g)\n",
    "    h_t^ = update_nl(W1x_t + U1h_{t-1} + B_h)\n",
    "    h_t = z_t*h_{t-1} + (1-z_t)*h_t^\n",
    "\n",
    "    Wi and Ui can further parameterised into low rank version by\n",
    "    Wi = matmul(W, W_i) and Ui = matmul(U, U_i)\n",
    "    '''\n",
    "\n",
    "    def __init__(self, hidden_size, gate_non_linearity=\"sigmoid\",\n",
    "                 update_non_linearity=\"tanh\", wRank=None, uRank=None,\n",
    "                 name=\"UGRNNLR\", reuse=None):\n",
    "        super(UGRNNLRCell, self).__init__(_reuse=reuse)\n",
    "        self._hidden_size = hidden_size\n",
    "        self._gate_non_linearity = gate_non_linearity\n",
    "        self._update_non_linearity = update_non_linearity\n",
    "        self._num_weight_matrices = [2, 2]\n",
    "        self._wRank = wRank\n",
    "        self._uRank = uRank\n",
    "        if wRank is not None:\n",
    "            self._num_weight_matrices[0] += 1\n",
    "        if uRank is not None:\n",
    "            self._num_weight_matrices[1] += 1\n",
    "        self._name = name\n",
    "        self._reuse = reuse\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return self._hidden_size\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self._hidden_size\n",
    "\n",
    "    @property\n",
    "    def gate_non_linearity(self):\n",
    "        return self._gate_non_linearity\n",
    "\n",
    "    @property\n",
    "    def update_non_linearity(self):\n",
    "        return self._update_non_linearity\n",
    "\n",
    "    @property\n",
    "    def wRank(self):\n",
    "        return self._wRank\n",
    "\n",
    "    @property\n",
    "    def uRank(self):\n",
    "        return self._uRank\n",
    "\n",
    "    @property\n",
    "    def num_weight_matrices(self):\n",
    "        return self._num_weight_matrices\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return self._name\n",
    "\n",
    "    @property\n",
    "    def cellType(self):\n",
    "        return \"UGRNNLR\"\n",
    "\n",
    "    def call(self, inputs, state):\n",
    "        with vs.variable_scope(self._name + \"/UGRNNLRCell\"):\n",
    "\n",
    "            if self._wRank is None:\n",
    "                W1_matrix_init = init_ops.random_normal_initializer(\n",
    "                    mean=0.0, stddev=0.1, dtype=tf.float32)\n",
    "                self.W1 = vs.get_variable(\n",
    "                    \"W1\", [inputs.get_shape()[-1], self._hidden_size],\n",
    "                    initializer=W1_matrix_init)\n",
    "                W2_matrix_init = init_ops.random_normal_initializer(\n",
    "                    mean=0.0, stddev=0.1, dtype=tf.float32)\n",
    "                self.W2 = vs.get_variable(\n",
    "                    \"W2\", [inputs.get_shape()[-1], self._hidden_size],\n",
    "                    initializer=W2_matrix_init)\n",
    "                wComp1 = math_ops.matmul(inputs, self.W1)\n",
    "                wComp2 = math_ops.matmul(inputs, self.W2)\n",
    "            else:\n",
    "                W_matrix_r_init = init_ops.random_normal_initializer(\n",
    "                    mean=0.0, stddev=0.1, dtype=tf.float32)\n",
    "                self.W = vs.get_variable(\n",
    "                    \"W\", [inputs.get_shape()[-1], self._wRank],\n",
    "                    initializer=W_matrix_r_init)\n",
    "                W1_matrix_init = init_ops.random_normal_initializer(\n",
    "                    mean=0.0, stddev=0.1, dtype=tf.float32)\n",
    "                self.W1 = vs.get_variable(\n",
    "                    \"W1\", [self._wRank, self._hidden_size],\n",
    "                    initializer=W1_matrix_init)\n",
    "                W2_matrix_init = init_ops.random_normal_initializer(\n",
    "                    mean=0.0, stddev=0.1, dtype=tf.float32)\n",
    "                self.W2 = vs.get_variable(\n",
    "                    \"W2\", [self._wRank, self._hidden_size],\n",
    "                    initializer=W2_matrix_init)\n",
    "                wComp1 = math_ops.matmul(\n",
    "                    math_ops.matmul(inputs, self.W), self.W1)\n",
    "                wComp2 = math_ops.matmul(\n",
    "                    math_ops.matmul(inputs, self.W), self.W2)\n",
    "\n",
    "            if self._uRank is None:\n",
    "                U1_matrix_init = init_ops.random_normal_initializer(\n",
    "                    mean=0.0, stddev=0.1, dtype=tf.float32)\n",
    "                self.U1 = vs.get_variable(\n",
    "                    \"U1\", [self._hidden_size, self._hidden_size],\n",
    "                    initializer=U1_matrix_init)\n",
    "                U2_matrix_init = init_ops.random_normal_initializer(\n",
    "                    mean=0.0, stddev=0.1, dtype=tf.float32)\n",
    "                self.U2 = vs.get_variable(\n",
    "                    \"U2\", [self._hidden_size, self._hidden_size],\n",
    "                    initializer=U2_matrix_init)\n",
    "                uComp1 = math_ops.matmul(state, self.U1)\n",
    "                uComp2 = math_ops.matmul(state, self.U2)\n",
    "            else:\n",
    "                U_matrix_r_init = init_ops.random_normal_initializer(\n",
    "                    mean=0.0, stddev=0.1, dtype=tf.float32)\n",
    "                self.U = vs.get_variable(\n",
    "                    \"U\", [self._hidden_size, self._uRank],\n",
    "                    initializer=U_matrix_r_init)\n",
    "                U1_matrix_init = init_ops.random_normal_initializer(\n",
    "                    mean=0.0, stddev=0.1, dtype=tf.float32)\n",
    "                self.U1 = vs.get_variable(\n",
    "                    \"U1\", [self._uRank, self._hidden_size],\n",
    "                    initializer=U1_matrix_init)\n",
    "                U2_matrix_init = init_ops.random_normal_initializer(\n",
    "                    mean=0.0, stddev=0.1, dtype=tf.float32)\n",
    "                self.U2 = vs.get_variable(\n",
    "                    \"U2\", [self._uRank, self._hidden_size],\n",
    "                    initializer=U2_matrix_init)\n",
    "                uComp1 = math_ops.matmul(\n",
    "                    math_ops.matmul(state, self.U), self.U1)\n",
    "                uComp2 = math_ops.matmul(\n",
    "                    math_ops.matmul(state, self.U), self.U2)\n",
    "\n",
    "            pre_comp1 = wComp1 + uComp1\n",
    "            pre_comp2 = wComp2 + uComp2\n",
    "\n",
    "            bias_gate_init = init_ops.constant_initializer(\n",
    "                1.0, dtype=tf.float32)\n",
    "            self.bias_gate = vs.get_variable(\n",
    "                \"B_g\", [1, self._hidden_size], initializer=bias_gate_init)\n",
    "            z = gen_non_linearity(pre_comp1 + self.bias_gate,\n",
    "                                  self._gate_non_linearity)\n",
    "\n",
    "            bias_update_init = init_ops.constant_initializer(\n",
    "                1.0, dtype=tf.float32)\n",
    "            self.bias_update = vs.get_variable(\n",
    "                \"B_h\", [1, self._hidden_size], initializer=bias_update_init)\n",
    "            c = gen_non_linearity(\n",
    "                pre_comp2 + self.bias_update, self._update_non_linearity)\n",
    "\n",
    "            new_h = z * state + (1.0 - z) * c\n",
    "\n",
    "        return new_h, new_h\n",
    "\n",
    "    def getVars(self):\n",
    "        Vars = []\n",
    "        if self._num_weight_matrices[0] == 2:\n",
    "            Vars.extend([self.W1, self.W2])\n",
    "        else:\n",
    "            Vars.extend([self.W, self.W1, self.W2])\n",
    "\n",
    "        if self._num_weight_matrices[1] == 2:\n",
    "            Vars.extend([self.U1, self.U2])\n",
    "        else:\n",
    "            Vars.extend([self.U, self.U1, self.U2])\n",
    "\n",
    "        Vars.extend([self.bias_gate, self.bias_update])\n",
    "\n",
    "        return Vars\n",
    "\n",
    "\n",
    "class EMI_DataPipeline():\n",
    "    '''\n",
    "    The data input block for EMI-RNN training. Since EMI-RNN is an expensive\n",
    "    algorithm due to the multiple rounds of updates that are to be performed,\n",
    "    we avoid using feed dict to feed data into tensorflow and rather,\n",
    "    exploit the dataset API. This class abstracts away most of the dataset API\n",
    "    implementation details and provides a module that ingests data in numpy\n",
    "    matrices and serves them to the remainder of the computation graph.\n",
    "\n",
    "    This class uses reinitializable iterators. Please refer to the dataset API\n",
    "    docs for more information.\n",
    "\n",
    "    This class supports resuming from checkpoint files. Provide the restored\n",
    "    meta graph as an argument to __init__ to enable this behaviour.\n",
    "\n",
    "    Usage:\n",
    "        Step 1: Create a data input pipeline object and obtain the x_batch and\n",
    "        y_batch tensors. These should be fed to other parts of the graph that\n",
    "        are supposed to act on the input data.\n",
    "        ```\n",
    "            inputPipeline = EMI_DataPipeline(NUM_SUBINSTANCE, NUM_TIMESTEPS,\n",
    "                                             NUM_FEATS, NUM_OUTPUT)\n",
    "            x_batch, y_batch = inputPipeline()\n",
    "            # feed to emiLSTM or some other computation subgraph\n",
    "            y_cap = emiLSTM(x_batch)\n",
    "        ```\n",
    "\n",
    "        Step 2:  Create other parts of the computation graph (loss operations,\n",
    "        training ops etc). After the graph construction is complete and after\n",
    "        initializing the Tensorflow graph with global_variables_initializer,\n",
    "        initialize the iterator with the input data by calling:\n",
    "            inputPipeline.runInitializer(x_train, y_trian, ...)\n",
    "\n",
    "        Step 3: You can now iterate over batches by running some computation\n",
    "        operation as you would normally do in seesion.run(..). Att the end of\n",
    "        the data, tf.errors.OutOfRangeError will be\n",
    "        thrown.\n",
    "        ```\n",
    "        while True:\n",
    "            try:\n",
    "                sess.run(y_cap)\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break\n",
    "        ```\n",
    "    '''\n",
    "\n",
    "    def __init__(self, numSubinstance, numTimesteps, numFeats, numOutput,\n",
    "                 graph=None, prefetchNum=5):\n",
    "        '''\n",
    "        numSubinstance, numTimeSteps, numFeats, numOutput:\n",
    "            Dataset characteristics. Please refer to the data preparation\n",
    "            documentation for more information provided in `examples/EMI-RNN`\n",
    "        graph: This module supports resuming/restoring from a saved metagraph. To\n",
    "            enable this behaviour, pass the restored graph as an argument. A\n",
    "            saved metagraph can be restored using the edgeml.utils.GraphManager\n",
    "            module.\n",
    "        prefetchNum: The number of asynchronous prefetch to do when iterating over\n",
    "            the data. Please refer to 'prefetching' in Tensorflow dataset API\n",
    "        '''\n",
    "\n",
    "        self.numSubinstance = numSubinstance\n",
    "        self.numTimesteps = numTimesteps\n",
    "        self.numFeats = numFeats\n",
    "        self.graph = graph\n",
    "        self.prefetchNum = prefetchNum\n",
    "        self.numOutput = numOutput\n",
    "        self.graphCreated = False\n",
    "        # Either restore or create the following\n",
    "        self.X = None\n",
    "        self.Y = None\n",
    "        self.batchSize = None\n",
    "        self.numEpochs = None\n",
    "        self.dataset_init = None\n",
    "        self.x_batch = None\n",
    "        self.y_batch = None\n",
    "        # Internal\n",
    "        self.scope = 'EMI/'\n",
    "\n",
    "    def _createGraph(self):\n",
    "        assert self.graphCreated is False\n",
    "        dim = [None, self.numSubinstance, self.numTimesteps, self.numFeats]\n",
    "        scope = self.scope + 'input-pipeline/'\n",
    "        with tf.name_scope(scope):\n",
    "            X = tf.compat.v1.placeholder(tf.float32, dim, name='inpX')\n",
    "            Y = tf.compat.v1.placeholder(tf.float32, [None, self.numSubinstance,\n",
    "                                        self.numOutput], name='inpY')\n",
    "            batchSize = tf.compat.v1.placeholder(tf.int64, name='batch-size')\n",
    "            numEpochs = tf.compat.v1.placeholder(tf.int64, name='num-epochs')\n",
    "\n",
    "            dataset_x_target = tf.data.Dataset.from_tensor_slices(X)\n",
    "            dataset_y_target = tf.data.Dataset.from_tensor_slices(Y)\n",
    "            couple = (dataset_x_target, dataset_y_target)\n",
    "            ds_target = tf.data.Dataset.zip(couple).repeat(numEpochs)\n",
    "            ds_target = ds_target.batch(batchSize)\n",
    "            ds_target = ds_target.prefetch(self.prefetchNum)\n",
    "            ds_iterator_target = tf.compat.v1.data.make_initializable_iterator(ds_target)\n",
    "            x_batch, y_batch = ds_iterator_target.get_next()\n",
    "            tf.compat.v1.add_to_collection('next-x-batch', x_batch)\n",
    "            tf.compat.v1.add_to_collection('next-y-batch', y_batch)\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.batchSize = batchSize\n",
    "        self.numEpochs = numEpochs\n",
    "        self.dataset_init = ds_iterator_target.initializer\n",
    "        self.x_batch, self.y_batch = x_batch, y_batch\n",
    "        self.graphCreated = True\n",
    "\n",
    "    def _restoreGraph(self, graph):\n",
    "        assert self.graphCreated is False\n",
    "        scope = 'EMI/input-pipeline/'\n",
    "        self.X = graph.get_tensor_by_name(scope + \"inpX:0\")\n",
    "        self.Y = graph.get_tensor_by_name(scope + \"inpY:0\")\n",
    "        self.batchSize = graph.get_tensor_by_name(scope + \"batch-size:0\")\n",
    "        self.numEpochs = graph.get_tensor_by_name(scope + \"num-epochs:0\")\n",
    "        self.dataset_init = graph.get_operation_by_name(scope + \"dataset-init\")\n",
    "        self.x_batch = graph.get_collection('next-x-batch')\n",
    "        self.y_batch = graph.get_collection('next-y-batch')\n",
    "        msg = 'More than one tensor named next-x-batch/next-y-batch. '\n",
    "        msg += 'Are you not resetting your graph?'\n",
    "        assert len(self.x_batch) == 1, msg\n",
    "        assert len(self.y_batch) == 1, msg\n",
    "        self.x_batch = self.x_batch[0]\n",
    "        self.y_batch = self.y_batch[0]\n",
    "        self.graphCreated = True\n",
    "\n",
    "    def __call__(self):\n",
    "        '''\n",
    "        The call method performs graph construction either by\n",
    "        creating a new graph or, if a restored meta graph is provided, by\n",
    "        restoring operators from this meta graph.\n",
    "\n",
    "        returns iterators (x_batch, y_batch)\n",
    "        '''\n",
    "        if self.graphCreated is True:\n",
    "            return self.x_batch, self.y_batch\n",
    "        if self.graph is None:\n",
    "            self._createGraph()\n",
    "        else:\n",
    "            self._restoreGraph(self.graph)\n",
    "        assert self.graphCreated is True\n",
    "        return self.x_batch, self.y_batch\n",
    "\n",
    "    def restoreFromGraph(self, graph, *args, **kwargs):\n",
    "        '''\n",
    "        This method provides an alternate way of restoring\n",
    "        from a saved meta graph - without having to provide the restored meta\n",
    "        graph as a parameter to __init__. This is useful when, in between\n",
    "        training, you want to reset the entire computation graph and reload a\n",
    "        new meta graph from disk. This method allows you to attach to this\n",
    "        newly loaded meta graph without having to create a new EMI_DataPipeline\n",
    "        object. Use this method only when you want to clear/reset the existing\n",
    "        computational graph.\n",
    "        '''\n",
    "        self.graphCreated = False\n",
    "        self.graph = graph\n",
    "        self._restoreGraph(graph)\n",
    "        assert self.graphCreated is True\n",
    "\n",
    "    def runInitializer(self, sess, x_data, y_data, batchSize, numEpochs):\n",
    "        '''\n",
    "        This method is used to ingest data by the dataset API. Call this method\n",
    "        with the data matrices after the graph has been initialized.\n",
    "\n",
    "        x_data, y_data, batchSize: Self explanatory.\n",
    "        numEpochs: The Tensorflow dataset API implements iteration over epochs\n",
    "            by appending the data to itself numEpochs times and then iterating\n",
    "            over the resulting data as if it was a single data set.\n",
    "        '''\n",
    "        assert self.graphCreated is True\n",
    "        msg = 'X shape should be [-1, numSubinstance, numTimesteps, numFeats]'\n",
    "        assert x_data.ndim == 4, msg\n",
    "        assert x_data.shape[1] == self.numSubinstance, msg\n",
    "        assert x_data.shape[2] == self.numTimesteps, msg\n",
    "        assert x_data.shape[3] == self.numFeats, msg\n",
    "        msg = 'X and Y sould have same first dimension'\n",
    "        assert y_data.shape[0] == x_data.shape[0], msg\n",
    "        msg = 'Y shape should be [-1, numSubinstance, numOutput]'\n",
    "        assert y_data.shape[1] == self.numSubinstance, msg\n",
    "        assert y_data.shape[2] == self.numOutput, msg\n",
    "        feed_dict = {\n",
    "            self.X: x_data,\n",
    "            self.Y: y_data,\n",
    "            self.batchSize: batchSize,\n",
    "            self.numEpochs: numEpochs\n",
    "        }\n",
    "        assert self.dataset_init is not None, 'Internal error!'\n",
    "        sess.run(self.dataset_init, feed_dict=feed_dict)\n",
    "\n",
    "\n",
    "class EMI_RNN():\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Abstract base class for RNN architectures compatible with EMI-RNN.\n",
    "        This class is extended by specific architectures like LSTM/GRU/FastGRNN\n",
    "        etc.\n",
    "\n",
    "        Note: We are not using the PEP recommended abc module since it is\n",
    "        difficult to support in both python 2 and 3 \"\"\"\n",
    "        self.graphCreated = False\n",
    "        # Model specific matrices, parameter should be saved\n",
    "        self.graph = None\n",
    "        self.varList = []\n",
    "        self.output = None\n",
    "        self.assignOps = []\n",
    "        raise NotImplementedError(\"This is intended to act similar to an \" +\n",
    "                                  \"abstract class. Instantiating is not \" +\n",
    "                                  \"allowed.\")\n",
    "\n",
    "    def __call__(self, x_batch, **kwargs):\n",
    "        '''\n",
    "        The call method performs graph construction either by\n",
    "        creating a new graph or, if a restored meta graph is provided, by\n",
    "        restoring operators from this meta graph.\n",
    "\n",
    "        x_batch: Dataset API iterators to the data.\n",
    "\n",
    "        returns forward computation output tensor\n",
    "        '''\n",
    "        if self.graphCreated is True:\n",
    "            assert self.output is not None\n",
    "            return self.output\n",
    "        if self.graph is None:\n",
    "            output = self._createBaseGraph(x_batch, **kwargs)\n",
    "            assert self.graphCreated is False\n",
    "            self._createExtendedGraph(output, **kwargs)\n",
    "        else:\n",
    "            self._restoreBaseGraph(self.graph, **kwargs)\n",
    "            assert self.graphCreated is False\n",
    "            self._restoreExtendedGraph(self.graph, **kwargs)\n",
    "        assert self.graphCreated is True\n",
    "        return self.output\n",
    "\n",
    "    def restoreFromGraph(self, graph, **kwargs):\n",
    "        '''\n",
    "        This method provides an alternate way of restoring\n",
    "        from a saved meta graph - without having to provide the restored meta\n",
    "        graph as a parameter to __init__. This is useful when, in between\n",
    "        training, you want to reset the entire computation graph and reload a\n",
    "        new meta graph from disk. This method allows you to attach to this\n",
    "        newly loaded meta graph without having to create a new EMI_DataPipeline\n",
    "        object. Use this method only when you want to clear/reset the existing\n",
    "        computational graph.\n",
    "        '''\n",
    "        self.graphCreated = False\n",
    "        self.varList = []\n",
    "        self.output = None\n",
    "        self.assignOps = []\n",
    "        self.graph = graph\n",
    "        self._restoreBaseGraph(self.graph, **kwargs)\n",
    "        assert self.graphCreated is False\n",
    "        self._restoreExtendedGraph(self.graph, **kwargs)\n",
    "        assert self.graphCreated is True\n",
    "\n",
    "    def getModelParams(self):\n",
    "        raise NotImplementedError(\"Subclass does not implement this method\")\n",
    "\n",
    "    def _createBaseGraph(self, x_batch, **kwargs):\n",
    "        raise NotImplementedError(\"Subclass does not implement this method\")\n",
    "\n",
    "    def _createExtendedGraph(self, baseOutput, **kwargs):\n",
    "        raise NotImplementedError(\"Subclass does not implement this method\")\n",
    "\n",
    "    def _restoreBaseGraph(self, graph, **kwargs):\n",
    "        raise NotImplementedError(\"Subclass does not implement this method\")\n",
    "\n",
    "    def _restoreExtendedGraph(self, graph, **kwargs):\n",
    "        raise NotImplementedError(\"Subclass does not implement this method\")\n",
    "\n",
    "    def addBaseAssignOps(self, graph, initVarList, **kwargs):\n",
    "        raise NotImplementedError(\"Subclass does not implement this method\")\n",
    "\n",
    "    def addExtendedAssignOps(self, graph, **kwargs):\n",
    "        raise NotImplementedError(\"Subclass does not implement this method\")\n",
    "\n",
    "\n",
    "class EMI_BasicLSTM(EMI_RNN):\n",
    "\n",
    "    def __init__(self, numSubinstance, numHidden, numTimeSteps,\n",
    "                 numFeats, graph=None, forgetBias=1.0, useDropout=False):\n",
    "        '''\n",
    "        EMI-RNN using LSTM cell. The architecture consists of a single LSTM\n",
    "        layer followed by a secondary classifier. The secondary classifier is\n",
    "        not defined as part of this module and is left for the user to define,\n",
    "        through the redefinition of the '_createExtendedGraph' and\n",
    "        '_restoreExtendedGraph' methods.\n",
    "\n",
    "        This class supports restoring from a meta-graph. Provide the restored\n",
    "        graph as an argument to the graph keyword to enable this behaviour.\n",
    "\n",
    "        numSubinstance: Number of sub-instance.\n",
    "        numHidden: The dimension of the hidden state.\n",
    "        numTimeSteps: The number of time steps of the RNN.\n",
    "        numFeats: The feature vector dimension for each time step.\n",
    "        graph: A restored metagraph. Provide a graph if restoring form a meta\n",
    "            graph is required.\n",
    "        forgetBias: Bias for the forget gate of the LSTM.\n",
    "        useDropout: Set to True if a dropout layer is to be added between\n",
    "            inputs and outputs to the LSTM.\n",
    "        '''\n",
    "        self.numHidden = numHidden\n",
    "        self.numTimeSteps = numTimeSteps\n",
    "        self.numFeats = numFeats\n",
    "        self.useDropout = useDropout\n",
    "        self.forgetBias = forgetBias\n",
    "        self.numSubinstance = numSubinstance\n",
    "        self.graph = graph\n",
    "        self.graphCreated = False\n",
    "        # Restore or initialize\n",
    "        self.keep_prob = None\n",
    "        self.varList = []\n",
    "        self.output = None\n",
    "        self.assignOps = []\n",
    "        # Internal\n",
    "        self._scope = 'EMI/BasicLSTM/'\n",
    "\n",
    "    def _createBaseGraph(self, X, **kwargs):\n",
    "        assert self.graphCreated is False\n",
    "        msg = 'X should be of form [-1, numSubinstance, numTimeSteps, numFeatures]'\n",
    "        assert X.get_shape().ndims == 4, msg\n",
    "        assert X.shape[1] == self.numSubinstance\n",
    "        assert X.shape[2] == self.numTimeSteps\n",
    "        assert X.shape[3] == self.numFeats\n",
    "        # Reshape into 3D such that the first dimension is -1 * numSubinstance\n",
    "        # where each numSubinstance segment corresponds to one bag\n",
    "        # then shape it back in into 4D\n",
    "        scope = self._scope\n",
    "        keep_prob = None\n",
    "        with tf.name_scope(scope):\n",
    "            x = tf.reshape(X, [-1, self.numTimeSteps, self.numFeats])\n",
    "            x = tf.unstack(x, num=self.numTimeSteps, axis=1)\n",
    "            # Get the LSTM output\n",
    "            cell = tf.nn.rnn_cell.BasicLSTMCell(self.numHidden,\n",
    "                                                forget_bias=self.forgetBias,\n",
    "                                                name='EMI-LSTM-Cell')\n",
    "            wrapped_cell = cell\n",
    "            if self.useDropout is True:\n",
    "                keep_prob = tf.placeholder(dtype=tf.float32, name='keep-prob')\n",
    "                wrapped_cell = tf.contrib.rnn.DropoutWrapper(cell,\n",
    "                                                             input_keep_prob=keep_prob,\n",
    "                                                             output_keep_prob=keep_prob)\n",
    "            outputs__, states = tf.nn.static_rnn(\n",
    "                wrapped_cell, x, dtype=tf.float32)\n",
    "            outputs = []\n",
    "            for output in outputs__:\n",
    "                outputs.append(tf.expand_dims(output, axis=1))\n",
    "            # Convert back to bag form\n",
    "            outputs = tf.concat(outputs, axis=1, name='concat-output')\n",
    "            dims = [-1, self.numSubinstance, self.numTimeSteps, self.numHidden]\n",
    "            output = tf.reshape(outputs, dims, name='bag-output')\n",
    "\n",
    "        LSTMVars = cell.variables\n",
    "        self.varList.extend(LSTMVars)\n",
    "        if self.useDropout:\n",
    "            self.keep_prob = keep_prob\n",
    "        self.output = output\n",
    "        return self.output\n",
    "\n",
    "    def _restoreBaseGraph(self, graph, **kwargs):\n",
    "        assert self.graphCreated is False\n",
    "        assert self.graph is not None\n",
    "        scope = self._scope\n",
    "        if self.useDropout:\n",
    "            self.keep_prob = graph.get_tensor_by_name(scope + 'keep-prob:0')\n",
    "        self.output = graph.get_tensor_by_name(scope + 'bag-output:0')\n",
    "        kernel = graph.get_tensor_by_name(\"rnn/EMI-LSTM-Cell/kernel:0\")\n",
    "        bias = graph.get_tensor_by_name(\"rnn/EMI-LSTM-Cell/bias:0\")\n",
    "        assert len(self.varList) == 0\n",
    "        self.varList = [kernel, bias]\n",
    "\n",
    "    def getModelParams(self):\n",
    "        '''\n",
    "        Returns the LSTM kernel and bias tensors.\n",
    "        returns [kernel, bias]\n",
    "        '''\n",
    "        assert self.graphCreated is True, \"Graph is not created\"\n",
    "        assert len(self.varList) == 2\n",
    "        return self.varList\n",
    "\n",
    "    def addBaseAssignOps(self, graph, initVarList, **kwargs):\n",
    "        '''\n",
    "        Adds Tensorflow assignment operations to all of the model tensors.\n",
    "        These operations can then be used to initialize these tensors from\n",
    "        numpy matrices by running these operators\n",
    "\n",
    "        initVarList: A list of numpy matrices that will be used for\n",
    "            initialization by the assignment operation. For EMI_BasicLSTM, this\n",
    "            should be [kernel, bias] matrices.\n",
    "        '''\n",
    "        assert initVarList is not None\n",
    "        assert len(initVarList) == 2\n",
    "        k_ = graph.get_tensor_by_name('rnn/EMI-LSTM-Cell/kernel:0')\n",
    "        b_ = graph.get_tensor_by_name('rnn/EMI-LSTM-Cell/bias:0')\n",
    "        kernel, bias = initVarList[-2], initVarList[-1]\n",
    "        k_op = tf.assign(k_, kernel)\n",
    "        b_op = tf.assign(b_, bias)\n",
    "        self.assignOps.extend([k_op, b_op])\n",
    "\n",
    "\n",
    "class EMI_GRU(EMI_RNN):\n",
    "\n",
    "    def __init__(self, numSubinstance, numHidden, numTimeSteps,\n",
    "                 numFeats, graph=None, useDropout=False):\n",
    "        '''\n",
    "        EMI-RNN using GRU cell. The architecture consists of a single GRU\n",
    "        layer followed by a secondary classifier. The secondary classifier is\n",
    "        not defined as part of this module and is left for the user to define,\n",
    "        through the redefinition of the '_createExtendedGraph' and\n",
    "        '_restoreExtendedGraph' methods.\n",
    "\n",
    "        This class supports restoring from a meta-graph. Provide the restored\n",
    "        graph as value to the graph keyword to enable this behaviour.\n",
    "\n",
    "        numSubinstance: Number of sub-instance.\n",
    "        numHidden: The dimension of the hidden state.\n",
    "        numTimeSteps: The number of time steps of the RNN.\n",
    "        numFeats: The feature vector dimension for each time step.\n",
    "        graph: A restored metagraph. Provide a graph if restoring form a meta\n",
    "            graph is required.\n",
    "        useDropout: Set to True if a dropout layer is to be added between\n",
    "            inputs and outputs to the RNN.\n",
    "        '''\n",
    "        self.numHidden = numHidden\n",
    "        self.numTimeSteps = numTimeSteps\n",
    "        self.numFeats = numFeats\n",
    "        self.useDropout = useDropout\n",
    "        self.numSubinstance = numSubinstance\n",
    "        self.graph = graph\n",
    "        self.graphCreated = False\n",
    "        # Restore or initialize\n",
    "        self.keep_prob = None\n",
    "        self.varList = []\n",
    "        self.output = None\n",
    "        self.assignOps = []\n",
    "        # Internal\n",
    "        self._scope = 'EMI/GRU/'\n",
    "\n",
    "    def _createBaseGraph(self, X, **kwargs):\n",
    "        assert self.graphCreated is False\n",
    "        msg = 'X should be of form [-1, numSubinstance, numTimeSteps, numFeatures]'\n",
    "        assert X.get_shape().ndims == 4, msg\n",
    "        assert X.shape[1] == self.numSubinstance\n",
    "        assert X.shape[2] == self.numTimeSteps\n",
    "        assert X.shape[3] == self.numFeats\n",
    "        # Reshape into 3D suself.h that the first dimension is -1 * numSubinstance\n",
    "        # where each numSubinstance segment corresponds to one bag\n",
    "        # then shape it back in into 4D\n",
    "        scope = self._scope\n",
    "        keep_prob = None\n",
    "        with tf.name_scope(scope):\n",
    "            x = tf.reshape(X, [-1, self.numTimeSteps, self.numFeats])\n",
    "            x = tf.unstack(x, num=self.numTimeSteps, axis=1)\n",
    "            # Get the GRU output\n",
    "            cell = tf.nn.rnn_cell.GRUCell(self.numHidden, name='EMI-GRU-Cell')\n",
    "            wrapped_cell = cell\n",
    "            if self.useDropout is True:\n",
    "                keep_prob = tf.placeholder(dtype=tf.float32, name='keep-prob')\n",
    "                wrapped_cell = tf.contrib.rnn.DropoutWrapper(cell,\n",
    "                                                             input_keep_prob=keep_prob,\n",
    "                                                             output_keep_prob=keep_prob)\n",
    "            outputs__, states = tf.nn.static_rnn(\n",
    "                wrapped_cell, x, dtype=tf.float32)\n",
    "            outputs = []\n",
    "            for output in outputs__:\n",
    "                outputs.append(tf.expand_dims(output, axis=1))\n",
    "            # Convert back to bag form\n",
    "            outputs = tf.concat(outputs, axis=1, name='concat-output')\n",
    "            dims = [-1, self.numSubinstance, self.numTimeSteps, self.numHidden]\n",
    "            output = tf.reshape(outputs, dims, name='bag-output')\n",
    "\n",
    "        GRUVars = cell.variables\n",
    "        self.varList.extend(GRUVars)\n",
    "        if self.useDropout:\n",
    "            self.keep_prob = keep_prob\n",
    "        self.output = output\n",
    "        return self.output\n",
    "\n",
    "    def _restoreBaseGraph(self, graph, **kwargs):\n",
    "        assert self.graphCreated is False\n",
    "        assert self.graph is not None\n",
    "        scope = self._scope\n",
    "        if self.useDropout:\n",
    "            self.keep_prob = graph.get_tensor_by_name(scope + 'keep-prob:0')\n",
    "        self.output = graph.get_tensor_by_name(scope + 'bag-output:0')\n",
    "        kernel1 = graph.get_tensor_by_name(\"rnn/EMI-GRU-Cell/gates/kernel:0\")\n",
    "        bias1 = graph.get_tensor_by_name(\"rnn/EMI-GRU-Cell/gates/bias:0\")\n",
    "        kernel2 = graph.get_tensor_by_name(\n",
    "            \"rnn/EMI-GRU-Cell/candidate/kernel:0\")\n",
    "        bias2 = graph.get_tensor_by_name(\"rnn/EMI-GRU-Cell/candidate/bias:0\")\n",
    "        assert len(self.varList) == 0\n",
    "        self.varList = [kernel1, bias1, kernel2, bias2]\n",
    "\n",
    "    def getModelParams(self):\n",
    "        '''\n",
    "        Returns the GRU kernel and bias tensors.\n",
    "        returns [kernel1, bias1, kernel2, bias2]\n",
    "        '''\n",
    "        assert self.graphCreated is True, \"Graph is not created\"\n",
    "        assert len(self.varList) == 4\n",
    "        return self.varList\n",
    "\n",
    "    def addBaseAssignOps(self, graph, initVarList, **kwargs):\n",
    "        '''\n",
    "        Adds Tensorflow assignment operations to all of the model tensors.\n",
    "        These operations can then be used to initialize these tensors from\n",
    "        numpy matrices by running these operators\n",
    "\n",
    "        initVarList: A list of numpy matrices that will be used for\n",
    "            initialization by the assignment operation. For EMI_GRU, this\n",
    "            should be list of numpy matrices corresponding to  [kernel1, bias1,\n",
    "            kernel2, bias2]\n",
    "        '''\n",
    "        assert initVarList is not None\n",
    "        assert len(initVarList) == 2\n",
    "        kernel1_ = graph.get_tensor_by_name(\"rnn/EMI-GRU-Cell/gates/kernel:0\")\n",
    "        bias1_ = graph.get_tensor_by_name(\"rnn/EMI-GRU-Cell/gates/bias:0\")\n",
    "        kernel2_ = graph.get_tensor_by_name(\n",
    "            \"rnn/EMI-GRU-Cell/candidate/kernel:0\")\n",
    "        bias2_ = graph.get_tensor_by_name(\"rnn/EMI-GRU-Cell/candidate/bias:0\")\n",
    "        kernel1, bias1, kernel2, bias2 = initVarList[\n",
    "            0], initVarList[1], initVarList[2], initVarList[3]\n",
    "        kernel1_op = tf.assign(kernel1_, kernel1)\n",
    "        bias1_op = tf.assign(bias1_, bias1)\n",
    "        kernel2_op = tf.assign(kernel2_, kernel2)\n",
    "        bias2_op = tf.assign(bias2_, bias2)\n",
    "        self.assignOps.extend([kernel1_op, bias1_op, kernel2_op, bias2_op])\n",
    "\n",
    "\n",
    "class EMI_FastRNN(EMI_RNN):\n",
    "\n",
    "    def __init__(self, numSubinstance, numHidden, numTimeSteps,\n",
    "                 numFeats, graph=None, useDropout=False,\n",
    "                 update_non_linearity=\"tanh\", wRank=None,\n",
    "                 uRank=None, alphaInit=-3.0, betaInit=3.0):\n",
    "        '''\n",
    "        EMI-RNN using FastRNN cell. The architecture consists of a single\n",
    "        FastRNN layer followed by a secondary classifier. The secondary\n",
    "        classifier is not defined as part of this module and is left for the\n",
    "        user to define, through the redefinition of the '_createExtendedGraph'\n",
    "        and '_restoreExtendedGraph' methods.\n",
    "\n",
    "        This class supports restoring from a meta-graph. Provide the restored\n",
    "        graph as value to the graph keyword to enable this behaviour.\n",
    "\n",
    "        numSubinstance: Number of sub-instance.\n",
    "        numHidden: The dimension of the hidden state.\n",
    "        numTimeSteps: The number of time steps of the RNN.\n",
    "        numFeats: The feature vector dimension for each time step.\n",
    "        graph: A restored metagraph. Provide a graph if restoring form a meta\n",
    "            graph is required.\n",
    "        useDropout: Set to True if a dropout layer is to be added\n",
    "            between inputs and outputs to the RNN.\n",
    "        update_non_linearity, wRank, uRank, _alphaInit, betaInit:\n",
    "            These are FastRNN parameters. Please refer to FastRNN documentation\n",
    "            for more information.\n",
    "        '''\n",
    "        self.numHidden = numHidden\n",
    "        self.numTimeSteps = numTimeSteps\n",
    "        self.numFeats = numFeats\n",
    "        self.useDropout = useDropout\n",
    "        self.numSubinstance = numSubinstance\n",
    "        self.graph = graph\n",
    "        self.update_non_linearity = update_non_linearity\n",
    "        self.wRank = wRank\n",
    "        self.uRank = uRank\n",
    "        self.alphaInit = alphaInit\n",
    "        self.betaInit = betaInit\n",
    "        self.graphCreated = False\n",
    "        # Restore or initialize\n",
    "        self.keep_prob = None\n",
    "        self.varList = []\n",
    "        self.output = None\n",
    "        self.assignOps = []\n",
    "        # Internal\n",
    "        self._scope = 'EMI/FastRNN/'\n",
    "\n",
    "    def _createBaseGraph(self, X, **kwargs):\n",
    "        assert self.graphCreated is False\n",
    "        msg = 'X should be of form [-1, numSubinstance, numTimeSteps,'\n",
    "        msg += ' numFeatures]'\n",
    "        assert X.get_shape().ndims == 4, msg\n",
    "        assert X.shape[1] == self.numSubinstance\n",
    "        assert X.shape[2] == self.numTimeSteps\n",
    "        assert X.shape[3] == self.numFeats\n",
    "        # Reshape into 3D suself.h that the first dimension is -1 *\n",
    "        # numSubinstance where each numSubinstance segment corresponds to one\n",
    "        # bag then shape it back in into 4D\n",
    "        scope = self._scope\n",
    "        keep_prob = None\n",
    "        with tf.name_scope(scope):\n",
    "            x = tf.reshape(X, [-1, self.numTimeSteps, self.numFeats])\n",
    "            x = tf.unstack(x, num=self.numTimeSteps, axis=1)\n",
    "            # Get the FastRNN output\n",
    "            cell = FastRNNCell(self.numHidden, self.update_non_linearity,\n",
    "                               self.wRank, self.uRank, self.alphaInit,\n",
    "                               self.betaInit, name='EMI-FastRNN-Cell')\n",
    "            wrapped_cell = cell\n",
    "            if self.useDropout is True:\n",
    "                keep_prob = tf.placeholder(dtype=tf.float32, name='keep-prob')\n",
    "                wrapped_cell = tf.contrib.rnn.DropoutWrapper(cell,\n",
    "                                                             input_keep_prob=keep_prob,\n",
    "                                                             output_keep_prob=keep_prob)\n",
    "            outputs__, states = tf.nn.static_rnn(wrapped_cell, x,\n",
    "                                                 dtype=tf.float32)\n",
    "            outputs = []\n",
    "            for output in outputs__:\n",
    "                outputs.append(tf.expand_dims(output, axis=1))\n",
    "            # Convert back to bag form\n",
    "            outputs = tf.concat(outputs, axis=1, name='concat-output')\n",
    "            dims = [-1, self.numSubinstance, self.numTimeSteps, self.numHidden]\n",
    "            output = tf.reshape(outputs, dims, name='bag-output')\n",
    "\n",
    "        FastRNNVars = cell.variables\n",
    "        self.varList.extend(FastRNNVars)\n",
    "        if self.useDropout:\n",
    "            self.keep_prob = keep_prob\n",
    "        self.output = output\n",
    "        return self.output\n",
    "\n",
    "    def _restoreBaseGraph(self, graph, **kwargs):\n",
    "        assert self.graphCreated is False\n",
    "        assert self.graph is not None\n",
    "        scope = self._scope\n",
    "        if self.useDropout:\n",
    "            self.keep_prob = graph.get_tensor_by_name(scope + 'keep-prob:0')\n",
    "        self.output = graph.get_tensor_by_name(scope + 'bag-output:0')\n",
    "\n",
    "        assert len(self.varList) == 0\n",
    "        if self.wRank is None:\n",
    "            W = graph.get_tensor_by_name(\n",
    "                \"rnn/fast_rnn_cell/EMI-FastRNN-Cell/FastRNNcell/W:0\")\n",
    "            self.varList = [W]\n",
    "        else:\n",
    "            W1 = graph.get_tensor_by_name(\n",
    "                \"rnn/fast_rnn_cell/EMI-FastRNN-Cell/FastRNNcell/W1:0\")\n",
    "            W2 = graph.get_tensor_by_name(\n",
    "                \"rnn/fast_rnn_cell/EMI-FastRNN-Cell/FastRNNcell/W2:0\")\n",
    "            self.varList = [W1, W2]\n",
    "\n",
    "        if self.uRank is None:\n",
    "            U = graph.get_tensor_by_name(\n",
    "                \"rnn/fast_rnn_cell/EMI-FastRNN-Cell/FastRNNcell/U:0\")\n",
    "            self.varList.extend([U])\n",
    "        else:\n",
    "            U1 = graph.get_tensor_by_name(\n",
    "                \"rnn/fast_rnn_cell/EMI-FastRNN-Cell/FastRNNcell/U1:0\")\n",
    "            U2 = graph.get_tensor_by_name(\n",
    "                \"rnn/fast_rnn_cell/EMI-FastRNN-Cell/FastRNNcell/U2:0\")\n",
    "            self.varList.extend([U1, U2])\n",
    "\n",
    "        alpha = graph.get_tensor_by_name(\n",
    "            \"rnn/fast_rnn_cell/EMI-FastRNN-Cell/FastRNNcell/alpha:0\")\n",
    "        beta = graph.get_tensor_by_name(\n",
    "            \"rnn/fast_rnn_cell/EMI-FastRNN-Cell/FastRNNcell/beta:0\")\n",
    "        bias = graph.get_tensor_by_name(\n",
    "            \"rnn/fast_rnn_cell/EMI-FastRNN-Cell/FastRNNcell/B_h:0\")\n",
    "        self.varList.extend([alpha, beta, bias])\n",
    "\n",
    "    def getModelParams(self):\n",
    "        '''\n",
    "        Returns the FastRNN model tensors.\n",
    "        In the order of  [W(W1, W2), U(U1,U2), alpha, beta, B_h]\n",
    "        () implies that the matrix can be replaced with the matrices inside.\n",
    "        '''\n",
    "        assert self.graphCreated is True, \"Graph is not created\"\n",
    "        return self.varList\n",
    "\n",
    "    def addBaseAssignOps(self, graph, initVarList, **kwargs):\n",
    "        '''\n",
    "        Adds Tensorflow assignment operations to all of the model tensors.\n",
    "        These operations can then be used to initialize these tensors from\n",
    "        numpy matrices by running these operators\n",
    "\n",
    "        initVarList: A list of numpy matrices that will be used for\n",
    "            initialization by the assignment operation. For EMI_FastRNN, this\n",
    "            should be list of numpy matrices corresponding to  [W(W1, W2),\n",
    "            U(U1,U2), alpha, beta, B_h]\n",
    "        '''\n",
    "        assert initVarList is not None\n",
    "        index = 0\n",
    "        if self.wRank is None:\n",
    "            W_ = graph.get_tensor_by_name(\n",
    "                \"rnn/fast_rnn_cell/EMI-FastRNN-Cell/FastRNNcell/W:0\")\n",
    "            W = initVarList[0]\n",
    "            w_op = tf.assign(W_, W)\n",
    "            self.assignOps.extend([w_op])\n",
    "            index += 1\n",
    "        else:\n",
    "            W1_ = graph.get_tensor_by_name(\n",
    "                \"rnn/fast_rnn_cell/EMI-FastRNN-Cell/FastRNNcell/W1:0\")\n",
    "            W2_ = graph.get_tensor_by_name(\n",
    "                \"rnn/fast_rnn_cell/EMI-FastRNN-Cell/FastRNNcell/W2:0\")\n",
    "            W1, W2 = initVarList[0], initVarList[1]\n",
    "            w1_op = tf.assign(W1_, W1)\n",
    "            w2_op = tf.assign(W2_, W2)\n",
    "            self.assignOps.extend([w1_op, w2_op])\n",
    "            index += 2\n",
    "\n",
    "        if self.uRank is None:\n",
    "            U_ = graph.get_tensor_by_name(\n",
    "                \"rnn/fast_rnn_cell/EMI-FastRNN-Cell/FastRNNcell/U:0\")\n",
    "            U = initVarList[index]\n",
    "            u_op = tf.assign(U_, U)\n",
    "            self.assignOps.extend([u_op])\n",
    "            index += 1\n",
    "        else:\n",
    "            U1_ = graph.get_tensor_by_name(\n",
    "                \"rnn/fast_rnn_cell/EMI-FastRNN-Cell/FastRNNcell/U1:0\")\n",
    "            U2_ = graph.get_tensor_by_name(\n",
    "                \"rnn/fast_rnn_cell/EMI-FastRNN-Cell/FastRNNcell/U2:0\")\n",
    "            U1, U2 = initVarList[index], initVarList[index + 1]\n",
    "            u1_op = tf.assign(U1_, U1)\n",
    "            u2_op = tf.assign(U2_, U2)\n",
    "            self.assignOps.extend([u1_op, u2_op])\n",
    "            index += 2\n",
    "\n",
    "        alpha_ = graph.get_tensor_by_name(\n",
    "            \"rnn/fast_rnn_cell/EMI-FastRNN-Cell/FastRNNcell/alpha:0\")\n",
    "        beta_ = graph.get_tensor_by_name(\n",
    "            \"rnn/fast_rnn_cell/EMI-FastRNN-Cell/FastRNNcell/beta:0\")\n",
    "        bias_ = graph.get_tensor_by_name(\n",
    "            \"rnn/fast_rnn_cell/EMI-FastRNN-Cell/FastRNNcell/B_h:0\")\n",
    "\n",
    "        alpha, beta, bias = initVarList[index], initVarList[\n",
    "            index + 1], initVarList[index + 2]\n",
    "        alpha_op = tf.assign(alpha_, alpha)\n",
    "        beta_op = tf.assign(beta_, beta)\n",
    "        bias_op = tf.assign(bias_, bias)\n",
    "\n",
    "        self.assignOps.extend([alpha_op, beta_op, bias_op])\n",
    "\n",
    "\n",
    "class EMI_UGRNN(EMI_RNN):\n",
    "\n",
    "    def __init__(self, numSubinstance, numHidden, numTimeSteps,\n",
    "                 numFeats, graph=None, forgetBias=1.0, useDropout=False):\n",
    "        '''\n",
    "        EMI-RNN using UGRNN cell. The architecture consists of a single UGRNN\n",
    "        layer followed by a secondary classifier. The secondary classifier is\n",
    "        not defined as part of this module and is left for the user to define,\n",
    "        through the redefinition of the '_createExtendedGraph' and\n",
    "        '_restoreExtendedGraph' methods.\n",
    "\n",
    "        This class supports restoring from a meta-graph. Provide the restored\n",
    "        graph as value to the graph keyword to enable this behaviour.\n",
    "\n",
    "        numSubinstance: Number of sub-instance.\n",
    "        numHidden: The dimension of the hidden state.\n",
    "        numTimeSteps: The number of time steps of the RNN.\n",
    "        numFeats: The feature vector dimension for each time step.\n",
    "        graph: A restored metagraph. Provide a graph if restoring form a meta\n",
    "            graph is required.\n",
    "        forgetBias: Bias for the forget gate of the UGRNN.\n",
    "        useDropout: Set to True if a dropout layer is to be added between\n",
    "            inputs and outputs to the RNN.\n",
    "        '''\n",
    "        self.numHidden = numHidden\n",
    "        self.numTimeSteps = numTimeSteps\n",
    "        self.numFeats = numFeats\n",
    "        self.useDropout = useDropout\n",
    "        self.forgetBias = forgetBias\n",
    "        self.numSubinstance = numSubinstance\n",
    "        self.graph = graph\n",
    "        self.graphCreated = False\n",
    "        # Restore or initialize\n",
    "        self.keep_prob = None\n",
    "        self.varList = []\n",
    "        self.output = None\n",
    "        self.assignOps = []\n",
    "        # Internal\n",
    "        self._scope = 'EMI/UGRNN/'\n",
    "\n",
    "    def _createBaseGraph(self, X, **kwargs):\n",
    "        assert self.graphCreated is False\n",
    "        msg = 'X should be of form [-1, numSubinstance, numTimeSteps, numFeatures]'\n",
    "        assert X.get_shape().ndims == 4, msg\n",
    "        assert X.shape[1] == self.numSubinstance\n",
    "        assert X.shape[2] == self.numTimeSteps\n",
    "        assert X.shape[3] == self.numFeats\n",
    "        # Reshape into 3D such that the first dimension is -1 * numSubinstance\n",
    "        # where each numSubinstance segment corresponds to one bag\n",
    "        # then shape it back in into 4D\n",
    "        scope = self._scope\n",
    "        keep_prob = None\n",
    "        with tf.name_scope(scope):\n",
    "            x = tf.reshape(X, [-1, self.numTimeSteps, self.numFeats])\n",
    "            x = tf.unstack(x, num=self.numTimeSteps, axis=1)\n",
    "            # Get the UGRNN output\n",
    "            cell = tf.contrib.rnn.UGRNNCell(self.numHidden,\n",
    "                                            forget_bias=self.forgetBias)\n",
    "            wrapped_cell = cell\n",
    "            if self.useDropout is True:\n",
    "                keep_prob = tf.placeholder(dtype=tf.float32, name='keep-prob')\n",
    "                wrapped_cell = tf.contrib.rnn.DropoutWrapper(cell,\n",
    "                                                             input_keep_prob=keep_prob,\n",
    "                                                             output_keep_prob=keep_prob)\n",
    "            outputs__, states = tf.nn.static_rnn(\n",
    "                wrapped_cell, x, dtype=tf.float32)\n",
    "            outputs = []\n",
    "            for output in outputs__:\n",
    "                outputs.append(tf.expand_dims(output, axis=1))\n",
    "            # Convert back to bag form\n",
    "            outputs = tf.concat(outputs, axis=1, name='concat-output')\n",
    "            dims = [-1, self.numSubinstance, self.numTimeSteps, self.numHidden]\n",
    "            output = tf.reshape(outputs, dims, name='bag-output')\n",
    "\n",
    "        UGRNNVars = cell.variables\n",
    "        self.varList.extend(UGRNNVars)\n",
    "        if self.useDropout:\n",
    "            self.keep_prob = keep_prob\n",
    "        self.output = output\n",
    "        return self.output\n",
    "\n",
    "    def _restoreBaseGraph(self, graph, **kwargs):\n",
    "        assert self.graphCreated is False\n",
    "        assert self.graph is not None\n",
    "        scope = self._scope\n",
    "        if self.useDropout:\n",
    "            self.keep_prob = graph.get_tensor_by_name(scope + 'keep-prob:0')\n",
    "        self.output = graph.get_tensor_by_name(scope + 'bag-output:0')\n",
    "        kernel = graph.get_tensor_by_name(\"rnn/ugrnn_cell/kernel:0\")\n",
    "        bias = graph.get_tensor_by_name(\"rnn/ugrnn_cell/bias:0\")\n",
    "        assert len(self.varList) == 0\n",
    "        self.varList = [kernel, bias]\n",
    "\n",
    "    def getModelParams(self):\n",
    "        '''\n",
    "        Returns the FastRRNN model tensors.\n",
    "        returns [kernel, bias]\n",
    "        '''\n",
    "        assert self.graphCreated is True, \"Graph is not created\"\n",
    "        assert len(self.varList) == 2\n",
    "        return self.varList\n",
    "\n",
    "    def addBaseAssignOps(self, graph, initVarList, **kwargs):\n",
    "        '''\n",
    "        Adds Tensorflow assignment operations to all of the model tensors.\n",
    "        These operations can then be used to initialize these tensors from\n",
    "        numpy matrices by running these operators\n",
    "\n",
    "        initVarList: A list of numpy matrices that will be used for\n",
    "            initialization by the assignment operation. For EMI_UGRNN, this\n",
    "            should be list of numpy matrices corresponding to  [kernel, bias]\n",
    "        '''\n",
    "        assert initVarList is not None\n",
    "        assert len(initVarList) == 2\n",
    "        k_ = graph.get_tensor_by_name('rnn/ugrnn_cell/kernel:0')\n",
    "        b_ = graph.get_tensor_by_name('rnn/ugrnn_cell/bias:0')\n",
    "        kernel, bias = initVarList[-2], initVarList[-1]\n",
    "        k_op = tf.assign(k_, kernel)\n",
    "        b_op = tf.assign(b_, bias)\n",
    "        self.assignOps.extend([k_op, b_op])\n",
    "\n",
    "\n",
    "class EMI_FastGRNN(EMI_RNN):\n",
    "\n",
    "    def __init__(self, numSubinstance, numHidden, numTimeSteps, numFeats,\n",
    "                 graph=None, useDropout=False, gate_non_linearity=\"sigmoid\",\n",
    "                 update_non_linearity=\"tanh\", wRank=None, uRank=None,\n",
    "                 zetaInit=1.0, nuInit=-4.0):\n",
    "        '''\n",
    "        EMI-RNN using FastGRNN cell. The architecture consists of a single\n",
    "        FastGRNN layer followed by a secondary classifier. The secondary\n",
    "        classifier is not defined as part of this module and is left for the\n",
    "        user to define, through the redefinition of the '_createExtendedGraph'\n",
    "        and '_restoreExtendedGraph' methods.\n",
    "\n",
    "        This class supports restoring from a meta-graph. Provide the restored\n",
    "        graph as value to the graph keyword to enable this behaviour.\n",
    "\n",
    "        numSubinstance: Number of sub-instance.\n",
    "        numHidden: The dimension of the hidden state.\n",
    "        numTimeSteps: The number of time steps of the RNN.\n",
    "        numFeats: The feature vector dimension for each time step.\n",
    "        graph: A restored metagraph. Provide a graph if restoring form a meta\n",
    "            graph is required.\n",
    "        useDropout: Set to True if a dropout layer is to be added\n",
    "            between inputs and outputs to the RNN.\n",
    "\n",
    "        gate_non_linearity, update_non_linearity, wRank, uRank, zetaInit,\n",
    "        nuInit:\n",
    "            These are FastGRNN parameters. Please refer to FastGRNN documentation\n",
    "            for more information.\n",
    "        '''\n",
    "        self.numHidden = numHidden\n",
    "        self.numTimeSteps = numTimeSteps\n",
    "        self.numFeats = numFeats\n",
    "        self.useDropout = useDropout\n",
    "        self.numSubinstance = numSubinstance\n",
    "        self.graph = graph\n",
    "\n",
    "        self.gate_non_linearity = gate_non_linearity\n",
    "        self.update_non_linearity = update_non_linearity\n",
    "        self.wRank = wRank\n",
    "        self.uRank = uRank\n",
    "        self.zetaInit = zetaInit\n",
    "        self.nuInit = nuInit\n",
    "\n",
    "        self.graphCreated = False\n",
    "        # Restore or initialize\n",
    "        self.keep_prob = None\n",
    "        self.varList = []\n",
    "        self.output = None\n",
    "        self.assignOps = []\n",
    "        # Internal\n",
    "        self._scope = 'EMI/FastGRNN/'\n",
    "\n",
    "    def _createBaseGraph(self, X, **kwargs):\n",
    "        assert self.graphCreated is False\n",
    "        msg = 'X should be of form [-1, numSubinstance, numTimeSteps, numFeatures]'\n",
    "        assert X.get_shape().ndims == 4, msg\n",
    "        assert X.shape[1] == self.numSubinstance\n",
    "        assert X.shape[2] == self.numTimeSteps\n",
    "        assert X.shape[3] == self.numFeats\n",
    "        # Reshape into 3D suself.h that the first dimension is -1 * numSubinstance\n",
    "        # where each numSubinstance segment corresponds to one bag\n",
    "        # then shape it back in into 4D\n",
    "        scope = self._scope\n",
    "        keep_prob = None\n",
    "        with tf.name_scope(scope):\n",
    "            x = tf.reshape(X, [-1, self.numTimeSteps, self.numFeats])\n",
    "            x = tf.unstack(x, num=self.numTimeSteps, axis=1)\n",
    "            # Get the FastGRNN output\n",
    "            # cell = FastGRNNCell(self.numHidden, self.gate_non_linearity,\n",
    "            #                     self.update_non_linearity, self.wRank,\n",
    "            #                     self.uRank, self.zetaInit, self.nuInit,\n",
    "            #                     name='EMI-FastGRNN-Cell')\n",
    "            # wrapped_cell = cell\n",
    "            # if self.useDropout is True:\n",
    "            #     keep_prob = tf.placeholder(dtype=tf.float32, name='keep-prob')\n",
    "            #     wrapped_cell = tf.contrib.rnn.DropoutWrapper(cell,\n",
    "            #                                                  input_keep_prob=keep_prob,\n",
    "            #                                                  output_keep_prob=keep_prob)\n",
    "            # outputs__, states = tf.keras.layers.LSTM(\n",
    "            #     wrapped_cell, x, dtype=tf.float32)\n",
    "            # outputs = []\n",
    "            # for output in outputs__:\n",
    "            #     outputs.append(tf.expand_dims(output, axis=1))\n",
    "            # # Convert back to bag form\n",
    "            # outputs = tf.concat(outputs, axis=1, name='concat-output')\n",
    "            # dims = [-1, self.numSubinstance, self.numTimeSteps, self.numHidden]\n",
    "            # output = tf.reshape(outputs, dims, name='bag-output')\n",
    "            # Get the FastGRNN output\n",
    "            cell = FastGRNNCell(self.numHidden, self.gate_non_linearity,\n",
    "                    self.update_non_linearity, self.wRank,\n",
    "                    self.uRank, self.zetaInit, self.nuInit,\n",
    "                    name='EMI-FastGRNN-Cell')\n",
    "            wrapped_cell = cell\n",
    "            if self.useDropout is True:\n",
    "                keep_prob = tf.placeholder(dtype=tf.float32, name='keep-prob')\n",
    "                wrapped_cell = tf.contrib.rnn.DropoutWrapper(cell,\n",
    "                                                 input_keep_prob=keep_prob,\n",
    "                                                 output_keep_prob=keep_prob)\n",
    "            outputs__, states = tf.compat.v1.nn.static_rnn(wrapped_cell, x, dtype=tf.float32)\n",
    "            outputs = []\n",
    "            for output in outputs__:\n",
    "                outputs.append(tf.expand_dims(output, axis=1))\n",
    "            # Convert back to bag form\n",
    "            outputs = tf.concat(outputs, axis=1, name='concat-output')\n",
    "            dims = [-1, self.numSubinstance, self.numTimeSteps, self.numHidden]\n",
    "            output = tf.reshape(outputs, dims, name='bag-output')\n",
    "\n",
    "\n",
    "        FastGRNNVars = cell.variables\n",
    "        self.varList.extend(FastGRNNVars)\n",
    "        if self.useDropout:\n",
    "            self.keep_prob = keep_prob\n",
    "        self.output = output\n",
    "        return self.output\n",
    "\n",
    "    def _restoreBaseGraph(self, graph, **kwargs):\n",
    "        assert self.graphCreated is False\n",
    "        assert self.graph is not None\n",
    "        scope = self._scope\n",
    "        if self.useDropout:\n",
    "            self.keep_prob = graph.get_tensor_by_name(scope + 'keep-prob:0')\n",
    "        self.output = graph.get_tensor_by_name(scope + 'bag-output:0')\n",
    "\n",
    "        assert len(self.varList) == 0\n",
    "        if self.wRank is None:\n",
    "            W = graph.get_tensor_by_name(\n",
    "                \"rnn/fast_grnn_cell/EMI-FastGRNN-Cell/FastGRNNcell/W:0\")\n",
    "            self.varList = [W]\n",
    "        else:\n",
    "            W1 = graph.get_tensor_by_name(\n",
    "                \"rnn/fast_grnn_cell/EMI-FastGRNN-Cell/FastGRNNcell/W1:0\")\n",
    "            W2 = graph.get_tensor_by_name(\n",
    "                \"rnn/fast_grnn_cell/EMI-FastGRNN-Cell/FastGRNNcell/W2:0\")\n",
    "            self.varList = [W1, W2]\n",
    "\n",
    "        if self.uRank is None:\n",
    "            U = graph.get_tensor_by_name(\n",
    "                \"rnn/fast_grnn_cell/EMI-FastGRNN-Cell/FastGRNNcell/U:0\")\n",
    "            self.varList.extend([U])\n",
    "        else:\n",
    "            U1 = graph.get_tensor_by_name(\n",
    "                \"rnn/fast_grnn_cell/EMI-FastGRNN-Cell/FastGRNNcell/U1:0\")\n",
    "            U2 = graph.get_tensor_by_name(\n",
    "                \"rnn/fast_grnn_cell/EMI-FastGRNN-Cell/FastGRNNcell/U2:0\")\n",
    "            self.varList.extend([U1, U2])\n",
    "\n",
    "        zeta = graph.get_tensor_by_name(\n",
    "            \"rnn/fast_grnn_cell/EMI-FastGRNN-Cell/FastGRNNcell/zeta:0\")\n",
    "        nu = graph.get_tensor_by_name(\n",
    "            \"rnn/fast_grnn_cell/EMI-FastGRNN-Cell/FastGRNNcell/nu:0\")\n",
    "        gate_bias = graph.get_tensor_by_name(\n",
    "            \"rnn/fast_grnn_cell/EMI-FastGRNN-Cell/FastGRNNcell/B_g:0\")\n",
    "        update_bias = graph.get_tensor_by_name(\n",
    "            \"rnn/fast_grnn_cell/EMI-FastGRNN-Cell/FastGRNNcell/B_h:0\")\n",
    "        self.varList.extend([zeta, nu, gate_bias, update_bias])\n",
    "\n",
    "    def getModelParams(self):\n",
    "        '''\n",
    "        Returns the FastGRNN model tensors.\n",
    "        In the order of  [W(W1, W2), U(U1,U2), zeta, nu, B_g, B_h]\n",
    "        () implies that the matrix can be replaced with the matrices inside.\n",
    "        '''\n",
    "        assert self.graphCreated is True, \"Graph is not created\"\n",
    "        return self.varList\n",
    "\n",
    "    def addBaseAssignOps(self, graph, initVarList, **kwargs):\n",
    "        '''\n",
    "        Adds Tensorflow assignment operations to all of the model tensors.\n",
    "        These operations can then be used to initialize these tensors from\n",
    "        numpy matrices by running these operators\n",
    "\n",
    "        initVarList: A list of numpy matrices that will be used for\n",
    "            initialization by the assignment operation. For EMI_FastGRNN, this\n",
    "            should be list of numpy matrices corresponding to  [W(W1, W2),\n",
    "            U(U1,U2), zeta, nu, B_g, B_h]\n",
    "        '''\n",
    "        assert initVarList is not None\n",
    "        index = 0\n",
    "        if self.wRank is None:\n",
    "            W_ = graph.get_tensor_by_name(\n",
    "                \"rnn/fast_grnn_cell/EMI-FastGRNN-Cell/FastGRNNcell/W:0\")\n",
    "            W = initVarList[0]\n",
    "            w_op = tf.assign(W_, W)\n",
    "            self.assignOps.extend([w_op])\n",
    "            index += 1\n",
    "        else:\n",
    "            W1_ = graph.get_tensor_by_name(\n",
    "                \"rnn/fast_grnn_cell/EMI-FastGRNN-Cell/FastGRNNcell/W1:0\")\n",
    "            W2_ = graph.get_tensor_by_name(\n",
    "                \"rnn/fast_grnn_cell/EMI-FastGRNN-Cell/FastGRNNcell/W2:0\")\n",
    "            W1, W2 = initVarList[0], initVarList[1]\n",
    "            w1_op = tf.assign(W1_, W1)\n",
    "            w2_op = tf.assign(W2_, W2)\n",
    "            self.assignOps.extend([w1_op, w2_op])\n",
    "            index += 2\n",
    "\n",
    "        if self.uRank is None:\n",
    "            U_ = graph.get_tensor_by_name(\n",
    "                \"rnn/fast_grnn_cell/EMI-FastGRNN-Cell/FastGRNNcell/U:0\")\n",
    "            U = initVarList[index]\n",
    "            u_op = tf.assign(U_, U)\n",
    "            self.assignOps.extend([u_op])\n",
    "            index += 1\n",
    "        else:\n",
    "            U1_ = graph.get_tensor_by_name(\n",
    "                \"rnn/fast_grnn_cell/EMI-FastGRNN-Cell/FastGRNNcell/U1:0\")\n",
    "            U2_ = graph.get_tensor_by_name(\n",
    "                \"rnn/fast_grnn_cell/EMI-FastGRNN-Cell/FastGRNNcell/U2:0\")\n",
    "            U1, U2 = initVarList[index], initVarList[index + 1]\n",
    "            u1_op = tf.assign(U1_, U1)\n",
    "            u2_op = tf.assign(U2_, U2)\n",
    "            self.assignOps.extend([u1_op, u2_op])\n",
    "            index += 2\n",
    "\n",
    "        zeta_ = graph.get_tensor_by_name(\n",
    "            \"rnn/fast_grnn_cell/EMI-FastGRNN-Cell/FastGRNNcell/zeta:0\")\n",
    "        nu_ = graph.get_tensor_by_name(\n",
    "            \"rnn/fast_grnn_cell/EMI-FastGRNN-Cell/FastGRNNcell/nu:0\")\n",
    "        gate_bias_ = graph.get_tensor_by_name(\n",
    "            \"rnn/fast_grnn_cell/EMI-FastGRNN-Cell/FastGRNNcell/B_g:0\")\n",
    "        update_bias_ = graph.get_tensor_by_name(\n",
    "            \"rnn/fast_grnn_cell/EMI-FastGRNN-Cell/FastGRNNcell/B_h:0\")\n",
    "\n",
    "        zeta, nu, gate_bias, update_bias = initVarList[index], initVarList[\n",
    "            index + 1], initVarList[index + 2], initVarList[index + 3]\n",
    "        zeta_op = tf.assign(zeta_, zeta)\n",
    "        nu_op = tf.assign(nu_, nu)\n",
    "        gate_bias_op = tf.assign(gate_bias_, gate_bias)\n",
    "        update_bias_op = tf.assign(update_bias_, update_bias)\n",
    "\n",
    "        self.assignOps.extend([zeta_op, nu_op, gate_bias_op, update_bias_op])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/deepin/.local/lib/python3.10/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "# Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "# Licensed under the MIT license.\n",
    "\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "import edgeml.tf.utils as utils\n",
    "import pandas as pd\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "\n",
    "class EMI_Trainer:\n",
    "    def __init__(self, numTimeSteps, numOutput, graph=None,\n",
    "                 stepSize=0.001, lossType='l2', optimizer='Adam',\n",
    "                 automode=True):\n",
    "        '''\n",
    "        The EMI-RNN trainer. This classes attaches loss functions and training\n",
    "        operations to the forward EMI-RNN graph. Currently, simple softmax loss\n",
    "        and l2 loss are supported on the outputs. For optimizers, only ADAM\n",
    "        optimizer is available.\n",
    "\n",
    "        numTimesteps: Number of time steps of the RNN model\n",
    "        numOutput: Number of output classes\n",
    "        graph: This module supports restoring from a meta graph. Provide the\n",
    "            meta graph as an argument to enable this behaviour.\n",
    "        lossType: A valid loss type string in ['l2', 'xentropy'].\n",
    "        optimizer: A valid optimizer string in ['Adam'].\n",
    "        automode: Disable or enable the automode behaviour.\n",
    "        This module takes care of all of the training procedure automatically,\n",
    "        and the default behaviour is suitable for most cases. In certain cases\n",
    "        though, the user would want to change certain aspects of the graph;\n",
    "        specifically, he would want to change to loss operation by, say, adding\n",
    "        regularization terms for the model matrices. To enable this behaviour,\n",
    "        the user can perform the following steps:\n",
    "            1. Disable automode. That is, when initializing, set automode=False\n",
    "            2. After the __call__ method has been invoked to create the loss\n",
    "            operation, the user can access the self.lossOp attribute and modify\n",
    "            it by adding regularization or other terms.\n",
    "            3. After the modification has been performed, the user needs to\n",
    "            call the `createOpCollections()` method so that the newly edited\n",
    "            operations can be added to Tensorflow collections. This helps in\n",
    "\n",
    "        HELP_WANTED: Automode is more of a hack than a systematic way of\n",
    "        supporting multiple loss functions/ optimizers. One way of\n",
    "        accomplishing this would be to make __createTrainOp and __createLossOp\n",
    "        methods protected or public, and having users override these.\n",
    "        Alternatively, we can change the structure to incorporate the\n",
    "        _createExtendedGraph and _restoreExtendedGraph operations used in\n",
    "        EMI-LSTM and so forth.\n",
    "        '''\n",
    "        self.numTimeSteps = numTimeSteps\n",
    "        self.numOutput = numOutput\n",
    "        self.graph = graph\n",
    "        self.stepSize = stepSize\n",
    "        self.lossType = lossType\n",
    "        self.optimizer = optimizer\n",
    "        self.automode = automode\n",
    "        self.__validInit = False\n",
    "        self.graphCreated = False\n",
    "        # Operations to be restored\n",
    "        self.lossOp = None\n",
    "        self.trainOp = None\n",
    "        self.softmaxPredictions = None\n",
    "        self.accTilda = None\n",
    "        self.equalTilda = None\n",
    "        self.lossIndicatorTensor = None\n",
    "        self.lossIndicatorPlaceholder = None\n",
    "        self.lossIndicatorAssignOp = None\n",
    "        # Input validation\n",
    "        self.supportedLosses = ['xentropy', 'l2']\n",
    "        self.supportedOptimizers = ['Adam']\n",
    "        assert lossType in self.supportedLosses\n",
    "        assert optimizer in self.supportedOptimizers\n",
    "        # Internal\n",
    "        self.scope = 'EMI/Trainer/'\n",
    "\n",
    "    def __validateInit(self, predicted, target):\n",
    "        msg = 'Predicted/Target tensors have incorrect dimension'\n",
    "        assert len(predicted.shape) == 4, msg\n",
    "        assert predicted.shape[3] == self.numOutput, msg\n",
    "        assert predicted.shape[2] == self.numTimeSteps, msg\n",
    "        assert predicted.shape[1] == target.shape[1], msg\n",
    "        assert len(target.shape) == 3\n",
    "        assert target.shape[2] == self.numOutput\n",
    "        self.__validInit = True\n",
    "\n",
    "    def __call__(self, predicted, target):\n",
    "        '''\n",
    "        Constructs the loss and train operations. If already created, returns\n",
    "        the created operators.\n",
    "\n",
    "        predicted: The prediction scores outputed from the forward computation\n",
    "            graph. Expects a 4 dimensional tensor with shape [-1,\n",
    "            numSubinstance, numTimeSteps, numClass].\n",
    "        target: The target labels in one hot-encoding. Expects [-1,\n",
    "            numSubinstance, numClass]\n",
    "        '''\n",
    "        if self.graphCreated is True:\n",
    "            # TODO: These statements are redundant after self.validInit call\n",
    "            # A simple check to self.__validInit should suffice. Test this.\n",
    "            assert self.lossOp is not None\n",
    "            assert self.trainOp is not None\n",
    "            return self.lossOp, self.trainOp\n",
    "        self.__validateInit(predicted, target)\n",
    "        assert self.__validInit is True\n",
    "        if self.graph is None:\n",
    "            self._createGraph(predicted, target)\n",
    "        else:\n",
    "            self._restoreGraph(predicted, target)\n",
    "        assert self.graphCreated == True\n",
    "        return self.lossOp, self.trainOp\n",
    "\n",
    "    def __transformY(self, target):\n",
    "        '''\n",
    "        Because we need output from each step and not just the last step.\n",
    "        Currently we just tile the target to each step. This method can be\n",
    "        exteneded/overridden to allow more complex behaviours\n",
    "        '''\n",
    "        with tf.name_scope(self.scope):\n",
    "            A_ = tf.expand_dims(target, axis=2)\n",
    "            A__ = tf.tile(A_, [1, 1, self.numTimeSteps, 1])\n",
    "        return A__\n",
    "\n",
    "    def __createLossOp(self, predicted, target):\n",
    "        assert self.__validInit is True, 'Initialization failure'\n",
    "        with tf.name_scope(self.scope):\n",
    "            # Loss indicator tensor\n",
    "            li = np.zeros([self.numTimeSteps, self.numOutput])\n",
    "            li[-1, :] = 1\n",
    "            liTensor = tf.Variable(li.astype('float32'),\n",
    "                                   name='loss-indicator',\n",
    "                                   trainable=False)\n",
    "            name='loss-indicator-placeholder'\n",
    "            liPlaceholder = tf.compat.v1.placeholder(tf.float32, shape=(None, None), name='loss-indicator-placeholder')\n",
    "\n",
    "\n",
    "            liAssignOp = tf.compat.v1.assign(liTensor, liPlaceholder,\n",
    "                                   name='loss-indicator-assign-op')\n",
    "            self.lossIndicatorTensor = liTensor\n",
    "            self.lossIndicatorPlaceholder = liPlaceholder\n",
    "            self.lossIndicatorAssignOp = liAssignOp\n",
    "            # predicted of dim [-1, numSubinstance, numTimeSteps, numOutput]\n",
    "            dims = [-1, self.numTimeSteps, self.numOutput]\n",
    "            logits__ = tf.reshape(predicted, dims)\n",
    "            labels__ = tf.reshape(target, dims)\n",
    "            diff = (logits__ - labels__)\n",
    "            diff = tf.multiply(self.lossIndicatorTensor, diff)\n",
    "            # take loss only for the timesteps indicated by lossIndicator for softmax\n",
    "            logits__ = tf.multiply(self.lossIndicatorTensor, logits__)\n",
    "            labels__ = tf.multiply(self.lossIndicatorTensor, labels__)\n",
    "            logits__ = tf.reshape(logits__, [-1, self.numOutput])\n",
    "            labels__ = tf.reshape(labels__, [-1, self.numOutput])\n",
    "            # Regular softmax\n",
    "            if self.lossType == 'xentropy':\n",
    "                softmax1 = tf.nn.softmax_cross_entropy_with_logits(labels=labels__,logits=logits__)\n",
    "                lossOp = tf.reduce_mean(softmax1, name='xentropy-loss')\n",
    "            elif self.lossType == 'l2':\n",
    "                lossOp = tf.nn.l2_loss(diff, name='l2-loss')\n",
    "        return lossOp\n",
    "    def __createTrainOp(self):\n",
    "        with tf.name_scope(self.scope):\n",
    "            optimizer = tf.keras.optimizers.Adam(self.stepSize)\n",
    "            trainable_variables = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.TRAINABLE_VARIABLES, scope=self.scope)\n",
    "            train_op = optimizer.minimize(self.lossOp, var_list=trainable_variables)\n",
    "        return train_op\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def _createGraph(self, predicted, target):\n",
    "        target = self.__transformY(target)\n",
    "        assert self.__validInit is True\n",
    "        with tf.name_scope(self.scope):\n",
    "            self.softmaxPredictions = tf.nn.softmax(predicted, axis=3,\n",
    "                                                    name='softmaxed-prediction')\n",
    "            pred = self.softmaxPredictions[:, :, -1, :]\n",
    "            actu = target[:, :, -1, :]\n",
    "            resPred = tf.reshape(pred, [-1, self.numOutput])\n",
    "            resActu = tf.reshape(actu, [-1, self.numOutput])\n",
    "            maxPred = tf.argmax(resPred, axis=1)\n",
    "            maxActu = tf.argmax(resActu, axis=1)\n",
    "            equal = tf.equal(maxPred, maxActu)\n",
    "            self.equalTilda = tf.cast(equal, tf.float32, name='equal-tilda')\n",
    "            self.accTilda = tf.reduce_mean(self.equalTilda, name='acc-tilda')\n",
    "\n",
    "        self.lossOp = self.__createLossOp(predicted, target)\n",
    "        self.trainOp = self.__createTrainOp()\n",
    "        if self.automode:\n",
    "            self.createOpCollections()\n",
    "        self.graphCreated = True\n",
    "\n",
    "    def _restoreGraph(self, predicted, target):\n",
    "        assert self.graphCreated is False\n",
    "        scope = self.scope\n",
    "        graph = self.graph\n",
    "        self.trainOp = tf.get_collection('EMI-train-op')\n",
    "        self.lossOp = tf.get_collection('EMI-loss-op')\n",
    "        msg0 = 'Operator or tensor not found'\n",
    "        msg1 = 'Multiple tensors with the same name in the graph. Are you not'\n",
    "        msg1 +=' resetting your graph?'\n",
    "        assert len(self.trainOp) != 0, msg0\n",
    "        assert len(self.lossOp) != 0, msg0\n",
    "        assert len(self.trainOp) == 1, msg1\n",
    "        assert len(self.lossOp) == 1, msg1\n",
    "        self.trainOp = self.trainOp[0]\n",
    "        self.lossOp = self.lossOp[0]\n",
    "        self.lossIndicatorTensor = graph.get_tensor_by_name(scope +\n",
    "                                                            'loss-indicator:0')\n",
    "        name = 'loss-indicator-placeholder:0'\n",
    "        self.lossIndicatorPlaceholder = graph.get_tensor_by_name(scope + name)\n",
    "        name = 'loss-indicator-assign-op:0'\n",
    "        self.lossIndicatorAssignOp = graph.get_tensor_by_name(scope + name)\n",
    "        name = scope + 'softmaxed-prediction:0'\n",
    "        self.softmaxPredictions = graph.get_tensor_by_name(name)\n",
    "        name = scope + 'acc-tilda:0'\n",
    "        self.accTilda = graph.get_tensor_by_name(name)\n",
    "        name = scope + 'equal-tilda:0'\n",
    "        self.equalTilda = graph.get_tensor_by_name(name)\n",
    "        self.graphCreated = True\n",
    "        self.__validInit = True\n",
    "\n",
    "    def createOpCollections(self):\n",
    "        '''\n",
    "        Adds the trainOp and lossOp to Tensorflow collections. This enables us\n",
    "        to restore these operations from saved metagraphs.\n",
    "        '''\n",
    "        tf.add_to_collection('EMI-train-op', self.trainOp)\n",
    "        tf.add_to_collection('EMI-loss-op', self.lossOp)\n",
    "\n",
    "    def __echoCB(self, sess, feedDict, currentBatch, redirFile, **kwargs):\n",
    "        _, loss = sess.run([self.trainOp, self.lossOp],\n",
    "                                feed_dict=feedDict)\n",
    "        print(\"\\rBatch %5d Loss %2.5f\" % (currentBatch, loss),\n",
    "              end='', file=redirFile)\n",
    "\n",
    "    def trainModel(self, sess, redirFile=None, echoInterval=15,\n",
    "                   echoCB=None, feedDict=None, **kwargs):\n",
    "        '''\n",
    "        The training routine.\n",
    "\n",
    "        sess: The Tensorflow session associated with the computation graph.\n",
    "        redirFile: Output from the training routine can be redirected to a file\n",
    "            on the disk. Please provide the file pointer to said file to enable\n",
    "            this behaviour. Defaults to STDOUT. To disable outputs all\n",
    "            together, please pass a file pointer to DEVNULL or equivalent as an\n",
    "            argument.\n",
    "        echoInterval: The number of batch updates between calls to echoCB.\n",
    "        echoCB: This call back method is used for printing intermittent\n",
    "            training stats such as validation accuracy or loss value. By default,\n",
    "            it defaults to self.__echoCB. The signature of the method is,\n",
    "\n",
    "            echoCB(self, session, feedDict, currentBatch, redirFile, **kwargs)\n",
    "\n",
    "            Please refer to the __echoCB implementation for a simple example.\n",
    "            A more complex example can be found in the EMI_Driver.\n",
    "        feedDict: feedDict, that is required for the session.run() calls. Will\n",
    "            be directly passed to the sess.run() calls.\n",
    "        **kwargs: Additional args to echoCB.\n",
    "        '''\n",
    "        if echoCB is None:\n",
    "            echoCB = self.__echoCB\n",
    "        currentBatch = 0\n",
    "        while True:\n",
    "            try:\n",
    "                if currentBatch % echoInterval == 0:\n",
    "                    echoCB(sess, feedDict, currentBatch, redirFile, **kwargs)\n",
    "                else:\n",
    "                    sess.run([self.trainOp], feed_dict=feedDict)\n",
    "                currentBatch += 1\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break\n",
    "\n",
    "    def restoreFromGraph(self, graph):\n",
    "        '''\n",
    "        This method provides an alternate way of restoring\n",
    "        from a saved meta graph - without having to provide the restored meta\n",
    "        graph as a parameter to __init__. This is useful when, in between\n",
    "        training, you want to reset the entire computation graph and reload a\n",
    "        new meta graph from disk. This method allows you to attach to this\n",
    "        newly loaded meta graph without having to create a new EMI_Trainer\n",
    "        object. Use this method only when you want to clear/reset the existing\n",
    "        computational graph.\n",
    "        '''\n",
    "        self.graphCreated = False\n",
    "        self.lossOp = None\n",
    "        self.trainOp = None\n",
    "        self.lossIndicatorTensor = None\n",
    "        self.softmaxPredictions = None\n",
    "        self.accTilda = None\n",
    "        self.graph = graph\n",
    "        self.__validInit = True\n",
    "        assert self.graphCreated is False\n",
    "        self._restoreGraph(None, None)\n",
    "        assert self.graphCreated is True\n",
    "\n",
    "\n",
    "class EMI_Driver:\n",
    "    def __init__(self, emiDataPipeline, emiGraph, emiTrainer,\n",
    "                 max_to_keep=1000, globalStepStart=1000):\n",
    "        '''\n",
    "        The driver class that takes care of training an EMI RNN graph. The EMI\n",
    "        RNN graph consists of three parts - a data input pipeline\n",
    "        (EMI_DataPipeline), the forward computation graph (EMI-RNN) and the\n",
    "        loss graph (EMI_Trainer). After the three parts of been created and\n",
    "        connected, they should be passed as arguments to this module.\n",
    "\n",
    "        Since EMI-RNN training requires careful handling of Sessions and\n",
    "        graphs, these details are wrapped inside EMI_Driver. For an external\n",
    "        method to access the current session, please make sure to use\n",
    "        getCurrentSession() method defined in EMI_Driver. Note that this has to\n",
    "        be done every time a reference to the current session is required.\n",
    "        (Internally, sessions are closed and opened when new models are loaded\n",
    "        from the disk, so technically, as long as no new model has been loaded\n",
    "        from disk, there is no need to call getCurrentSession() again - it is\n",
    "        better to be safe though).\n",
    "\n",
    "        emiDataPipeline: An EMI_DataPipeline object.\n",
    "        emiGraph: An EMI_RNN object.\n",
    "        emiTrainer: An EMI_Trainer object.\n",
    "        max_to_keep: Maximum number of model checkpoints to keep. Make sure\n",
    "            that this is more than [number of iterations] * [number of rounds].\n",
    "        globalStepStart: The global step  value is used as a key for naming\n",
    "        saved meta graphs. Meta graphs and checkpoints will be named from\n",
    "        globalStepStart through globalStepStart  + max_to_keep.\n",
    "        '''\n",
    "        self._dataPipe = emiDataPipeline\n",
    "        self._emiGraph = emiGraph\n",
    "        self._emiTrainer = emiTrainer\n",
    "        msg = 'Have you invoked __call__()'\n",
    "        assert self._dataPipe.graphCreated is True, msg\n",
    "        assert self._emiGraph.graphCreated is True, msg\n",
    "        assert self._emiTrainer.graphCreated is True, msg\n",
    "        self.__globalStep = globalStepStart\n",
    "        self.__saver = tf.train.Saver(max_to_keep=max_to_keep,\n",
    "                                      save_relative_paths=True)\n",
    "        self.__graphManager = utils.GraphManager()\n",
    "        self.__sess = None\n",
    "\n",
    "    def fancyEcho(self, sess, feedDict, currentBatch, redirFile,\n",
    "                  numBatches=None):\n",
    "        '''\n",
    "        A callable that is passed as argument - echoCB - to\n",
    "        EMI_Trainer.train() method.\n",
    "        '''\n",
    "        _, loss, acc = sess.run([self._emiTrainer.trainOp,\n",
    "                                 self._emiTrainer.lossOp,\n",
    "                                 self._emiTrainer.accTilda],\n",
    "                                feed_dict=feedDict)\n",
    "        epoch = int(currentBatch /  numBatches)\n",
    "        batch = int(currentBatch % max(numBatches, 1))\n",
    "        print(\"\\rEpoch %3d Batch %5d (%5d) Loss %2.5f Acc %2.5f |\" %\n",
    "              (epoch, batch, currentBatch, loss, acc),\n",
    "              end='', file=redirFile)\n",
    "\n",
    "    def assignToGraph(self, initVarList):\n",
    "        '''\n",
    "        This method should deal with restoring the entire graph\n",
    "        now'''\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def initializeSession(self, graph, reuse=False, feedDict=None):\n",
    "        '''\n",
    "        Initialize a new session with the computation graph provided in graph.\n",
    "\n",
    "        graph: The computation graph needed to be used for the current session.\n",
    "        reuse: If True, global_variables_initializer will not be invoked and\n",
    "            the graph will retain the current tensor states/values. \n",
    "        feedDict: Not used\n",
    "        '''\n",
    "        sess = self.__sess\n",
    "        if sess is not None:\n",
    "           sess.close()\n",
    "        with graph.as_default():\n",
    "            sess = tf.Session()\n",
    "        if reuse is False:\n",
    "            with graph.as_default():\n",
    "                init = tf.global_variables_initializer()\n",
    "            sess.run(init)\n",
    "        self.__sess = sess\n",
    "\n",
    "    def getCurrentSession(self):\n",
    "        '''\n",
    "        Returns the current tf.Session()\n",
    "        '''\n",
    "        return self.__sess\n",
    "\n",
    "    def setSession(self, sess):\n",
    "        '''\n",
    "        Sets sess as the session to be used by the driver. Experimental and not\n",
    "        recommended.\n",
    "        '''\n",
    "        self.__sess = sess\n",
    "\n",
    "    def runOps(self, opList, X, Y, batchSize, feedDict=None, **kwargs):\n",
    "        '''\n",
    "        Run tensorflow operations provided in opList on data X, Y.\n",
    "\n",
    "        opList: A list of operations.\n",
    "        X, Y: Numpy matrices of the data.\n",
    "        batchSize: batch size\n",
    "        feedDict: Feed dict required, if any, by the provided ops.\n",
    "\n",
    "        returns a  list of batchwise results of sess.run(opList) on the\n",
    "        provided data.\n",
    "        '''\n",
    "        sess = self.__sess\n",
    "        if feedDict is None:\n",
    "            feedDict = self.feedDictFunc(**kwargs)\n",
    "        self._dataPipe.runInitializer(sess, X, Y, batchSize,\n",
    "                                       numEpochs=1)\n",
    "        outList = []\n",
    "        while True:\n",
    "            try:\n",
    "                resList = sess.run(opList, feed_dict=feedDict)\n",
    "                outList.append(resList)\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break\n",
    "        return outList\n",
    "\n",
    "    def run(self, numClasses, x_train, y_train, bag_train, x_val, y_val,\n",
    "            bag_val, numIter, numRounds, batchSize, numEpochs, echoCB=None,\n",
    "            redirFile=None, modelPrefix='/tmp/model', updatePolicy='top-k',\n",
    "            fracEMI=0.3, lossIndicator=None, *args, **kwargs):\n",
    "        '''\n",
    "        Performs the EMI-RNN training routine.\n",
    "\n",
    "        numClasses: Number of output classes.\n",
    "        x_train, y_train, bag_train, x_val, y_val, bag_val: data matrices for\n",
    "            test and validation sets. Please refer to the data preparation\n",
    "            document for more information.\n",
    "        numIter: Number of iterations. Each iteration consists of numEpochs\n",
    "            passes of the data. A model check point is created after each\n",
    "            iteration.\n",
    "        numRounds: Number of rounds of label updates to perform. Each round\n",
    "            consists of numIter iterations of numEpochs passes over the data.\n",
    "        batchSize: Batch Size.\n",
    "        numEpochs: Number of epochs per iteration. A model checkpoint is\n",
    "            created after evey numEpochs passes over the data.\n",
    "        feedDict: Feed dict for training procedure (optional).\n",
    "        echoCB: The echo function (print function) that is passed to the\n",
    "            EMI_Trainer.trian() method. Defaults to self.fancyEcho()\n",
    "        redirFile: Provide a file pointer to redirect output to if required.\n",
    "        modelPrefix: Output directory/prefix for checkpoints and metagraphs.\n",
    "        updatePolicy: Supported values are 'top-k' and 'prune-ends'. Refer to\n",
    "            the update policy documentation for more information.\n",
    "        fracEMI: Fraction of the total rounds that use EMI-RNN loss. The\n",
    "            initial (1-fracEMI) rounds will use regular MI-RNN loss. To perform\n",
    "            only MI-RNN training, set this to 0.0.\n",
    "        lossIndicator: NotImplemented\n",
    "        *args, **kwargs: Additional arguments passed to callback methods and\n",
    "            update policy methods.\n",
    "\n",
    "        returns the updated instance level labels on the training set and a\n",
    "            list of model stats after each round.\n",
    "        '''\n",
    "        assert self.__sess is not None, 'No sessions initialized'\n",
    "        sess = self.__sess\n",
    "        assert updatePolicy in ['prune-ends', 'top-k']\n",
    "        if updatePolicy == 'top-k':\n",
    "            print(\"Update policy: top-k\", file=redirFile)\n",
    "            updatePolicyFunc = self.__policyTopK\n",
    "        else:\n",
    "            print(\"Update policy: prune-ends\", file=redirFile)\n",
    "            updatePolicyFunc = self.__policyPrune\n",
    "\n",
    "        curr_y = np.array(y_train)\n",
    "        assert fracEMI >= 0\n",
    "        assert fracEMI <= 1\n",
    "        emiSteps = int(fracEMI * numRounds)\n",
    "        emiStep = numRounds - emiSteps\n",
    "        print(\"Training with MI-RNN loss for %d rounds\" % emiStep,\n",
    "              file=redirFile)\n",
    "        modelStats = []\n",
    "        for cround in range(numRounds):\n",
    "            feedDict = self.feedDictFunc(inference=False, **kwargs)\n",
    "            print(\"Round: %d\" % cround, file=redirFile)\n",
    "            if cround == emiStep:\n",
    "                print(\"Switching to EMI-Loss function\", file=redirFile)\n",
    "                if lossIndicator is not None:\n",
    "                    raise NotImplementedError('TODO')\n",
    "                else:\n",
    "                    nTs = self._emiTrainer.numTimeSteps\n",
    "                    nOut = self._emiTrainer.numOutput\n",
    "                    lossIndicator = np.ones([nTs, nOut])\n",
    "                    sess.run(self._emiTrainer.lossIndicatorAssignOp,\n",
    "                         feed_dict={self._emiTrainer.lossIndicatorPlaceholder:\n",
    "                                    lossIndicator})\n",
    "            valAccList, globalStepList = [], []\n",
    "            # Train the best model for the current round\n",
    "            for citer in range(numIter):\n",
    "                self._dataPipe.runInitializer(sess, x_train, curr_y,\n",
    "                                               batchSize, numEpochs)\n",
    "                numBatches = int(np.ceil(len(x_train) / batchSize))\n",
    "                self._emiTrainer.trainModel(sess, echoCB=self.fancyEcho,\n",
    "                                             numBatches=numBatches,\n",
    "                                             feedDict=feedDict,\n",
    "                                             redirFile=redirFile)\n",
    "                if self._emiGraph.useDropout is True:\n",
    "                    ret = self.getInstancePredictions(x_val, y_val,\n",
    "                                                      self.__nonEarlyInstancePrediction,\n",
    "                                                      keep_prob=1.0)\n",
    "                else:\n",
    "                    ret = self.getInstancePredictions(x_val, y_val,\n",
    "                                                      self.__nonEarlyInstancePrediction)\n",
    "                predictions = ret[0]\n",
    "                numSubinstance = x_val.shape[1]\n",
    "                numOutput = self._emiTrainer.numOutput\n",
    "                df = self.analyseModel(predictions, bag_val, numSubinstance,\n",
    "                                       numOutput, silent=True)\n",
    "                acc = np.max(df['acc'].values)\n",
    "                print(\" Val acc %2.5f | \" % acc, end='', file=redirFile)\n",
    "                self.__graphManager.checkpointModel(self.__saver, sess,\n",
    "                                                    modelPrefix,\n",
    "                                                    self.__globalStep,\n",
    "                                                    redirFile=redirFile)\n",
    "                valAccList.append(acc)\n",
    "                globalStepList.append((modelPrefix, self.__globalStep))\n",
    "                self.__globalStep += 1\n",
    "\n",
    "            # Update y for the current round\n",
    "            ## Load the best val-acc model\n",
    "            argAcc = np.argmax(valAccList)\n",
    "            resPrefix, resStep = globalStepList[argAcc]\n",
    "            modelStats.append((cround, np.max(valAccList),\n",
    "                               resPrefix, resStep))\n",
    "            self.loadSavedGraphToNewSession(resPrefix, resStep, redirFile)\n",
    "            sess = self.getCurrentSession()\n",
    "            feedDict = self.feedDictFunc(inference=True, **kwargs)\n",
    "            smxOut = self.runOps([self._emiTrainer.softmaxPredictions],\n",
    "                                     x_train, y_train, batchSize, feedDict)\n",
    "            smxOut= [np.array(smxOut[i][0]) for i in range(len(smxOut))]\n",
    "            smxOut = np.concatenate(smxOut)[:, :, -1, :]\n",
    "            newY = updatePolicyFunc(curr_y, smxOut, bag_train,\n",
    "                                    numClasses, **kwargs)\n",
    "            currY = newY\n",
    "        return currY, modelStats\n",
    "\n",
    "    def loadSavedGraphToNewSession(self, modelPrefix, globalStep,\n",
    "                                      redirFile=None):\n",
    "        self.__sess.close()\n",
    "        tf.reset_default_graph()\n",
    "        sess = tf.Session()\n",
    "        graph = self.__graphManager.loadCheckpoint(sess, modelPrefix,\n",
    "                                                   globalStep=globalStep,\n",
    "                                                   redirFile=redirFile)\n",
    "        # return graph\n",
    "        self._dataPipe.restoreFromGraph(graph)\n",
    "        self._emiGraph.restoreFromGraph(graph)\n",
    "        self._emiTrainer.restoreFromGraph(graph)\n",
    "        self.__sess = sess\n",
    "        return graph\n",
    "\n",
    "    def updateLabel(self, Y, policy, softmaxOut, bagLabel, numClasses, **kwargs):\n",
    "        '''\n",
    "        Updates the current label information based on policy and the predicted\n",
    "        outputs.\n",
    "\n",
    "        Y: numpy array of current label information.\n",
    "        policy: The update policy to use. Currently supports ['top-k',\n",
    "            'prune-ends']\n",
    "        softmaxOut: The predicted instance level output from the soft-max\n",
    "            layer.\n",
    "        bagLabel: A numpy array with bag level label information.\n",
    "        numClasses: Number of output classes.\n",
    "        **kwargs: Additional keyword arguments to the update policy\n",
    "        '''\n",
    "        assert policy in ['prune-ends', 'top-k']\n",
    "        if policy == 'top-k':\n",
    "            updatePolicyFunc = self.__policyTopK\n",
    "        else:\n",
    "            updatePolicyFunc = self.__policyPrune\n",
    "        Y_ = np.array(Y)\n",
    "        newY = updatePolicyFunc(Y_, softmaxOut, bagLabel, numClasses, **kwargs)\n",
    "        return newY\n",
    "\n",
    "    def analyseModel(self, predictions, Y_bag, numSubinstance, numClass,\n",
    "                     redirFile=None, verbose=False, silent=False):\n",
    "        '''\n",
    "        Some basic analysis on predictions and true labels.\n",
    "\n",
    "        predictions: [-1, numsubinstance] is the instance level prediction.\n",
    "        Y_Bag: [-1] is the bag level labels.\n",
    "        numSubinstace: Number of sub-instance.\n",
    "        numClass: Number of classes.\n",
    "        redirFile: To redirect output to a file, provide the file pointer.\n",
    "\n",
    "        verbose: Prints verbose data frame. Includes additionally, precision\n",
    "            and recall information.\n",
    "\n",
    "        silent: Disable output to console or file pointer.\n",
    "        '''\n",
    "        assert (predictions.ndim == 2)\n",
    "        assert (predictions.shape[1] == numSubinstance)\n",
    "        assert (Y_bag.ndim == 1)\n",
    "        assert (len(Y_bag) == len(predictions))\n",
    "        pholder = [0.0] * numSubinstance\n",
    "        df = pd.DataFrame()\n",
    "        df['len'] = np.arange(1, numSubinstance + 1)\n",
    "        df['acc'] = pholder\n",
    "        df['macro-fsc'] = pholder\n",
    "        df['macro-pre'] = pholder\n",
    "        df['macro-rec'] = pholder\n",
    "\n",
    "        df['micro-fsc'] = pholder\n",
    "        df['micro-pre'] = pholder\n",
    "        df['micro-rec'] = pholder\n",
    "        colList = []\n",
    "        colList.append('acc')\n",
    "        colList.append('macro-fsc')\n",
    "        colList.append('macro-pre')\n",
    "        colList.append('macro-rec')\n",
    "\n",
    "        colList.append('micro-fsc')\n",
    "        colList.append('micro-pre')\n",
    "        colList.append('micro-rec')\n",
    "        for i in range(0, numClass):\n",
    "            pre = 'pre_%02d' % i\n",
    "            rec = 'rec_%02d' % i\n",
    "            df[pre] = pholder\n",
    "            df[rec] = pholder\n",
    "            colList.append(pre)\n",
    "            colList.append(rec)\n",
    "\n",
    "        for i in range(1, numSubinstance + 1):\n",
    "            pred_ = self.getBagPredictions(predictions, numClass=numClass,\n",
    "                                           minSubsequenceLen=i,\n",
    "                                           redirFile = redirFile)\n",
    "            correct = (pred_ == Y_bag).astype('int')\n",
    "            trueAcc = np.mean(correct)\n",
    "            cmatrix = utils.getConfusionMatrix(pred_, Y_bag, numClass)\n",
    "            df.iloc[i-1, df.columns.get_loc('acc')] = trueAcc\n",
    "\n",
    "            macro, micro = utils.getMacroMicroFScore(cmatrix)\n",
    "            df.iloc[i-1, df.columns.get_loc('macro-fsc')] = macro\n",
    "            df.iloc[i-1, df.columns.get_loc('micro-fsc')] = micro\n",
    "\n",
    "            pre, rec = utils.getMacroPrecisionRecall(cmatrix)\n",
    "            df.iloc[i-1, df.columns.get_loc('macro-pre')] = pre\n",
    "            df.iloc[i-1, df.columns.get_loc('macro-rec')] = rec\n",
    "\n",
    "            pre, rec = utils.getMicroPrecisionRecall(cmatrix)\n",
    "            df.iloc[i-1, df.columns.get_loc('micro-pre')] = pre\n",
    "            df.iloc[i-1, df.columns.get_loc('micro-rec')] = rec\n",
    "            for j in range(numClass):\n",
    "                pre, rec = utils.getPrecisionRecall(cmatrix, label=j)\n",
    "                pre_ = df.columns.get_loc('pre_%02d' % j)\n",
    "                rec_ = df.columns.get_loc('rec_%02d' % j)\n",
    "                df.iloc[i-1, pre_ ] = pre\n",
    "                df.iloc[i-1, rec_ ] = rec\n",
    "\n",
    "        df.set_index('len')\n",
    "        # Comment this line to include all columns\n",
    "        colList = ['len', 'acc', 'macro-fsc', 'macro-pre', 'macro-rec']\n",
    "        colList += ['micro-fsc', 'micro-pre', 'micro-rec']\n",
    "        if verbose:\n",
    "            for col in df.columns:\n",
    "                if col not in colList:\n",
    "                    colList.append(col)\n",
    "        if numClass == 2:\n",
    "            precisionList = df['pre_01'].values\n",
    "            recallList = df['rec_01'].values\n",
    "            denom = precisionList + recallList\n",
    "            denom[denom == 0] = 1\n",
    "            numer = 2 * precisionList * recallList\n",
    "            f_ = numer / denom\n",
    "            df['fscore_01'] = f_\n",
    "            colList.append('fscore_01')\n",
    "\n",
    "        df = df[colList]\n",
    "        if silent is True:\n",
    "            return df\n",
    "\n",
    "        with pd.option_context('display.max_rows', 100,\n",
    "                               'display.max_columns', 100,\n",
    "                               'expand_frame_repr', True):\n",
    "            print(df, file=redirFile)\n",
    "\n",
    "        idx = np.argmax(df['acc'].values)\n",
    "        val = np.max(df['acc'].values)\n",
    "        print(\"Max accuracy %f at subsequencelength %d\" % (val, idx + 1),\n",
    "              file=redirFile)\n",
    "        val = np.max(df['micro-fsc'].values)\n",
    "        idx = np.argmax(df['micro-fsc'].values)\n",
    "        print(\"Max micro-f %f at subsequencelength %d\" % (val, idx + 1),\n",
    "              file=redirFile)\n",
    "        val = df['micro-pre'].values[idx]\n",
    "        print(\"Micro-precision %f at subsequencelength %d\" % (val, idx + 1),\n",
    "              file=redirFile)\n",
    "        val = df['micro-rec'].values[idx]\n",
    "        print(\"Micro-recall %f at subsequencelength %d\" % (val, idx + 1),\n",
    "              file=redirFile)\n",
    "\n",
    "        idx = np.argmax(df['macro-fsc'].values)\n",
    "        val = np.max(df['macro-fsc'].values)\n",
    "        print(\"Max macro-f %f at subsequencelength %d\" % (val, idx + 1),\n",
    "              file=redirFile)\n",
    "        val = df['macro-pre'].values[idx]\n",
    "        print(\"macro-precision %f at subsequencelength %d\" % (val, idx + 1),\n",
    "              file=redirFile)\n",
    "        val = df['macro-rec'].values[idx]\n",
    "        print(\"macro-recall %f at subsequencelength %d\" % (val, idx + 1),\n",
    "              file=redirFile)\n",
    "        if numClass == 2 and verbose:\n",
    "            idx = np.argmax(df['fscore_01'].values)\n",
    "            val = np.max(df['fscore_01'].values)\n",
    "            print('Max fscore %f at subsequencelength %d' % (val, idx + 1),\n",
    "                  file=redirFile)\n",
    "            print('Precision %f at subsequencelength %d' %\n",
    "                  (df['pre_01'].values[idx], idx + 1), file=redirFile)\n",
    "            print('Recall %f at subsequencelength %d' %\n",
    "                  (df['rec_01'].values[idx], idx + 1), file=redirFile)\n",
    "        return df\n",
    "\n",
    "    def __nonEarlyInstancePrediction(self, instanceOut, **kwargs):\n",
    "        '''\n",
    "        A prediction policy used internally. No early prediction is performed\n",
    "        and the class with max prob at the last step of the RNN is returned.\n",
    "        '''\n",
    "        assert instanceOut.ndim == 2\n",
    "        retclass = np.argmax(instanceOut[-1])\n",
    "        step = len(instanceOut) - 1\n",
    "        return retclass, step\n",
    "\n",
    "    def getInstancePredictions(self, x, y, earlyPolicy, batchSize=1024,\n",
    "                               feedDict=None, **kwargs):\n",
    "\n",
    "        '''\n",
    "        Returns instance level predictions for data (x, y).\n",
    "\n",
    "        Takes the softmax outputs from the joint trained model and, applies\n",
    "        earlyPolicy() on each instance and returns the instance level\n",
    "        prediction as well as the step at which this prediction was made.\n",
    "\n",
    "        softmaxOut: [-1, numSubinstance, numTimeSteps, numClass]\n",
    "        earlyPolicy: callable,\n",
    "            def earlyPolicy(subinstacePrediction):\n",
    "                subinstacePrediction: [numTimeSteps, numClass]\n",
    "                ...\n",
    "                return predictedClass, predictedStep\n",
    "\n",
    "        returns: predictions, predictionStep\n",
    "            predictions: [-1, numSubinstance]\n",
    "            predictionStep: [-1, numSubinstance]\n",
    "        '''\n",
    "        opList = self._emiTrainer.softmaxPredictions\n",
    "        if 'keep_prob' in kwargs:\n",
    "            assert kwargs['keep_prob'] == 1, 'Keep prob should be 1.0'\n",
    "        smxOut = self.runOps(opList, x, y, batchSize, feedDict=feedDict,\n",
    "                             **kwargs)\n",
    "        softmaxOut = np.concatenate(smxOut, axis=0)\n",
    "        assert softmaxOut.ndim == 4\n",
    "        numSubinstance, numTimeSteps, numClass = softmaxOut.shape[1:]\n",
    "        softmaxOutFlat = np.reshape(softmaxOut, [-1, numTimeSteps, numClass])\n",
    "        flatLen = len(softmaxOutFlat)\n",
    "        predictions = np.zeros(flatLen)\n",
    "        predictionStep = np.zeros(flatLen)\n",
    "        for i, instance in enumerate(softmaxOutFlat):\n",
    "            # instance is [numTimeSteps, numClass]\n",
    "            assert instance.ndim == 2\n",
    "            assert instance.shape[0] == numTimeSteps\n",
    "            assert instance.shape[1] == numClass\n",
    "            predictedClass, predictedStep = earlyPolicy(instance, **kwargs)\n",
    "            predictions[i] = predictedClass\n",
    "            predictionStep[i] = predictedStep\n",
    "        predictions = np.reshape(predictions, [-1, numSubinstance])\n",
    "        predictionStep = np.reshape(predictionStep, [-1, numSubinstance])\n",
    "        return predictions, predictionStep\n",
    "\n",
    "    def getBagPredictions(self, Y_predicted, minSubsequenceLen = 4,\n",
    "                          numClass=2, redirFile = None):\n",
    "        '''\n",
    "        Returns bag level predictions given instance level predictions.\n",
    "\n",
    "        A bag is considered to belong to a non-zero class if\n",
    "        minSubsequenceLen is satisfied. Otherwise, it is assumed\n",
    "        to belong to class 0. class 0 is negative by default. If\n",
    "        minSubsequenceLen is satisfied by multiple classes, the smaller of the\n",
    "        two is returned\n",
    "\n",
    "        Y_predicted is the predicted instance level results\n",
    "        [-1, numsubinstance]\n",
    "        Y True is the correct instance level label\n",
    "        [-1, numsubinstance]\n",
    "        '''\n",
    "        assert(Y_predicted.ndim == 2)\n",
    "        scoreList = []\n",
    "        for x in range(1, numClass):\n",
    "            scores = self.__getLengthScores(Y_predicted, val=x)\n",
    "            length = np.max(scores, axis=1)\n",
    "            scoreList.append(length)\n",
    "        scoreList = np.array(scoreList)\n",
    "        scoreList = scoreList.T\n",
    "        assert(scoreList.ndim == 2)\n",
    "        assert(scoreList.shape[0] == Y_predicted.shape[0])\n",
    "        assert(scoreList.shape[1] == numClass - 1)\n",
    "        length = np.max(scoreList, axis=1)\n",
    "        assert(length.ndim == 1)\n",
    "        assert(length.shape[0] == Y_predicted.shape[0])\n",
    "        predictionIndex = (length >= minSubsequenceLen)\n",
    "        prediction = np.zeros((Y_predicted.shape[0]))\n",
    "        labels = np.argmax(scoreList, axis=1) + 1\n",
    "        prediction[predictionIndex] = labels[predictionIndex]\n",
    "        return prediction.astype(int)\n",
    "\n",
    "    def __getLengthScores(self, Y_predicted, val=1):\n",
    "        '''\n",
    "        Returns an matrix which contains the length of the longest positive\n",
    "        subsequence of val ending at that index.\n",
    "        Y_predicted: [-1, numSubinstance] Is the instance level class\n",
    "            labels.\n",
    "        '''\n",
    "        scores = np.zeros(Y_predicted.shape)\n",
    "        for i, bag in enumerate(Y_predicted):\n",
    "            for j, instance in enumerate(bag):\n",
    "                prev = 0\n",
    "                if j > 0:\n",
    "                    prev = scores[i, j-1]\n",
    "                if instance == val:\n",
    "                    scores[i, j] = prev + 1\n",
    "                else:\n",
    "                    scores[i, j] = 0\n",
    "        return scores\n",
    "\n",
    "    def __policyPrune(self, currentY, softmaxOut, bagLabel, numClasses,\n",
    "                      minNegativeProb=0.0, updatesPerCall=3,\n",
    "                      maxAllowedUpdates=3, **kwargs):\n",
    "        '''\n",
    "        CurrentY: [-1, numsubinstance, numClass]\n",
    "        softmaxOut: [-1, numsubinstance, numClass]\n",
    "        bagLabel: [-1]\n",
    "        numClasses: Number of output classes\n",
    "        minNegativeProb: A instance predicted as negative is labeled as\n",
    "            negative iff prob. negative >= minNegativeProb\n",
    "        updatesPerCall: At most number of updates to per function call\n",
    "        maxAllowedUpdates: Total updates on positive bag cannot exceed\n",
    "            maxAllowedUpdate.\n",
    "\n",
    "        This policy incrementally increases the prefix/suffix of negative\n",
    "        labels in currentY.  An instance is labelled as a negative if:\n",
    "\n",
    "            1. All the instances preceding it in case of a prefix and all\n",
    "            instances succeeding it in case of a continuous prefix and/or\n",
    "            suffix of negative is labeled as a negative.\n",
    "            2. The probability of the instance being negative > negativeProb.\n",
    "            3. The instance is indeed predicted as negative (i.e. prob class 0\n",
    "            is max)\n",
    "            4. If the sequence length is less than maxSamples.\n",
    "\n",
    "        All four conditions must hold. In case of a tie between instances near\n",
    "        the suffix and prefix, the one with maximum probability is updated. If\n",
    "        probabilities are same, then the left prefix is updated.\n",
    "\n",
    "        CLASS 0 is assumed to be negative class\n",
    "        '''\n",
    "        assert currentY.ndim == 3\n",
    "        assert softmaxOut.ndim == 3\n",
    "        assert bagLabel.ndim == 1\n",
    "        assert len(currentY) == len(softmaxOut)\n",
    "        assert len(softmaxOut) == len(bagLabel)\n",
    "        numSubinstance = currentY.shape[1]\n",
    "        assert maxAllowedUpdates < numSubinstance\n",
    "        assert softmaxOut.shape[1] == numSubinstance\n",
    "\n",
    "        index = (bagLabel != 0)\n",
    "        indexList = np.where(bagLabel)[0]\n",
    "        newY = np.array(currentY)\n",
    "        for i in indexList:\n",
    "            currLabel = currentY[i]\n",
    "            currProbabilities = softmaxOut[i]\n",
    "            prevPrefix = 0\n",
    "            prevSuffix = 0\n",
    "            for inst in currLabel:\n",
    "                if np.argmax(inst) == 0:\n",
    "                    prevPrefix += 1\n",
    "                else:\n",
    "                    break\n",
    "            for inst in reversed(currLabel):\n",
    "                if np.argmax(inst) == 0:\n",
    "                    prevSuffix += 1\n",
    "                else:\n",
    "                    break\n",
    "            assert (prevPrefix + prevSuffix <= maxAllowedUpdates)\n",
    "            leftIdx = int(prevPrefix)\n",
    "            rightIdx = numSubinstance - int(prevSuffix) - 1\n",
    "            possibleUpdates = min(updatesPerCall, maxAllowedUpdates - prevPrefix - prevSuffix)\n",
    "            while (possibleUpdates > 0):\n",
    "                assert leftIdx < numSubinstance\n",
    "                assert leftIdx >= 0\n",
    "                assert rightIdx < numSubinstance\n",
    "                assert rightIdx >= 0\n",
    "                leftLbl = np.argmax(currProbabilities[leftIdx])\n",
    "                leftProb = np.max(currProbabilities[leftIdx])\n",
    "                rightLbl = np.argmax(currProbabilities[rightIdx])\n",
    "                rightProb = np.max(currProbabilities[rightIdx])\n",
    "                if (leftLbl != 0 and rightLbl !=0):\n",
    "                    break\n",
    "                elif (leftLbl == 0 and rightLbl != 0):\n",
    "                    if leftProb >= minNegativeProb:\n",
    "                        newY[i, leftIdx, :] = 0\n",
    "                        newY[i, leftIdx, 0] = 1\n",
    "                        leftIdx += 1\n",
    "                    else:\n",
    "                        break\n",
    "                elif (leftLbl != 0 and rightLbl == 0):\n",
    "                    if rightProb >= minNegativeProb:\n",
    "                        newY[i, rightIdx, :] = 0\n",
    "                        newY[i, rightIdx, 0] = 1\n",
    "                        rightIdx -= 1\n",
    "                    else:\n",
    "                        break\n",
    "                elif leftProb >= rightProb:\n",
    "                    if leftProb >= minNegativeProb:\n",
    "                        newY[i, leftIdx, :] = 0\n",
    "                        newY[i, leftIdx, 0] = 1\n",
    "                        leftIdx += 1\n",
    "                    else:\n",
    "                        break\n",
    "                elif rightProb > leftProb:\n",
    "                    if rightProb >= minNegativeProb:\n",
    "                        newY[i, rightIdx, :] = 0\n",
    "                        newY[i, rightIdx, 0] = 1\n",
    "                        rightIdx -= 1\n",
    "                    else:\n",
    "                        break\n",
    "                possibleUpdates -= 1\n",
    "        return newY\n",
    "\n",
    "    def __policyTopK(self, currentY, softmaxOut, bagLabel, numClasses, k=1,\n",
    "                     **kwargs):\n",
    "        '''\n",
    "        currentY: [-1, numsubinstance, numClass]\n",
    "        softmaxOut: [-1, numsubinstance, numClass]\n",
    "        bagLabel [-1]\n",
    "        k: minimum length of continuous non-zero examples\n",
    "\n",
    "        Algorithm:\n",
    "            For each bag:\n",
    "                1. Find the longest continuous subsequence of a label.\n",
    "                2. If this label is the same as the bagLabel, and if the length\n",
    "                of the subsequence is at least k:\n",
    "                    2.1 Set the label of these instances as the bagLabel.\n",
    "                    2.2 Set all other labels as 0\n",
    "        '''\n",
    "        assert currentY.ndim == 3\n",
    "        assert k <= currentY.shape[1]\n",
    "        assert k > 0\n",
    "        # predicted label for each instance is max of softmax\n",
    "        predictedLabels = np.argmax(softmaxOut, axis=2)\n",
    "        scoreList = []\n",
    "        # classScores[i] is a 2d array where a[j,k] is the longest\n",
    "        # string of consecutive class labels i in bag j ending at instance k\n",
    "        classScores = [-1]\n",
    "        for i in range(1, numClasses):\n",
    "            scores = self.__getLengthScores(predictedLabels, val=i)\n",
    "            classScores.append(scores)\n",
    "            length = np.max(scores, axis=1)\n",
    "            scoreList.append(length)\n",
    "        scoreList = np.array(scoreList)\n",
    "        scoreList = scoreList.T\n",
    "        # longestContinuousClass[i] is the class label having\n",
    "        # longest substring in bag i\n",
    "        longestContinuousClass = np.argmax(scoreList, axis=1) + 1\n",
    "        # longestContinuousClassLength[i] is length of \n",
    "        # longest class substring in bag i\n",
    "        longestContinuousClassLength = np.max(scoreList, axis=1)\n",
    "        assert longestContinuousClass.ndim == 1\n",
    "        assert longestContinuousClass.shape[0] == bagLabel.shape[0]\n",
    "        assert longestContinuousClassLength.ndim == 1\n",
    "        assert longestContinuousClassLength.shape[0] == bagLabel.shape[0]\n",
    "        newY = np.array(currentY)\n",
    "        index = (bagLabel != 0)\n",
    "        indexList = np.where(index)[0]\n",
    "        # iterate through all non-zero bags\n",
    "        for i in indexList:\n",
    "            # longest continuous class for this bag\n",
    "            lcc = longestContinuousClass[i]\n",
    "            # length of longest continuous class for this bag\n",
    "            lccl = int(longestContinuousClassLength[i])\n",
    "            # if bagLabel is not the same as longest continuous\n",
    "            # class, don't update\n",
    "            if lcc != bagLabel[i]:\n",
    "                continue\n",
    "            # we check for longest string to be at least k\n",
    "            if lccl < k:\n",
    "                continue\n",
    "            lengths = classScores[lcc][i]\n",
    "            assert np.max(lengths) == lccl\n",
    "            possibleCandidates = np.where(lengths == lccl)[0]\n",
    "            # stores (candidateIndex, sum of probabilities\n",
    "            # over window for this index) pairs\n",
    "            sumProbsAcrossLongest = {}\n",
    "            for candidate in possibleCandidates:\n",
    "                sumProbsAcrossLongest[candidate] = 0.0\n",
    "                # sum the probabilities over the continuous substring\n",
    "                for j in range(0, lccl):\n",
    "                    sumProbsAcrossLongest[candidate] += softmaxOut[i, candidate-j, lcc]\n",
    "            # we want only the one with maximum sum of\n",
    "            # probabilities; sort dict by value\n",
    "            sortedProbs = sorted(sumProbsAcrossLongest.items(),key=lambda x: x[1], reverse=True)\n",
    "            bestCandidate = sortedProbs[0][0]\n",
    "            # apart from (bestCanditate-lcc,bestCandidate] label\n",
    "            # everything else as 0\n",
    "            newY[i, :, :] = 0\n",
    "            newY[i, :, 0] = 1\n",
    "            newY[i, bestCandidate-lccl+1:bestCandidate+1, 0] = 0\n",
    "            newY[i, bestCandidate-lccl+1:bestCandidate+1, lcc] = 1\n",
    "        return newY\n",
    "\n",
    "    def feedDictFunc(self, **kwargs):\n",
    "        '''\n",
    "        Construct feed dict from graph objects\n",
    "        '''\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "# Licensed under the MIT license.\n",
    "\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.cluster\n",
    "import scipy.spatial\n",
    "import os\n",
    "\n",
    "\n",
    "def medianHeuristic(data, projectionDimension, numPrototypes, W_init=None):\n",
    "    '''\n",
    "    This method can be used to estimate gamma for ProtoNN. An approximation to\n",
    "    median heuristic is used here.\n",
    "    1. First the data is collapsed into the projectionDimension by W_init. If\n",
    "    W_init is not provided, it is initialized from a random normal(0, 1). Hence\n",
    "    data normalization is essential.\n",
    "    2. Prototype are computed by running a  k-means clustering on the projected\n",
    "    data.\n",
    "    3. The median distance is then estimated by calculating median distance\n",
    "    between prototypes and projected data points.\n",
    "\n",
    "    data needs to be [-1, numFeats]\n",
    "    If using this method to initialize gamma, please use the W and B as well.\n",
    "\n",
    "    TODO: Return estimate of Z (prototype labels) based on cluster centroids\n",
    "    andand labels\n",
    "\n",
    "    TODO: Clustering fails due to singularity error if projecting upwards\n",
    "\n",
    "    W [dxd_cap]\n",
    "    B [d_cap, m]\n",
    "    returns gamma, W, B\n",
    "    '''\n",
    "    assert data.ndim == 2\n",
    "    X = data\n",
    "    featDim = data.shape[1]\n",
    "    if projectionDimension > featDim:\n",
    "        print(\"Warning: Projection dimension > feature dimension. Gamma\")\n",
    "        print(\"\\t estimation due to median heuristic could fail.\")\n",
    "        print(\"\\tTo retain the projection dataDimension, provide\")\n",
    "        print(\"\\ta value for gamma.\")\n",
    "\n",
    "    if W_init is None:\n",
    "        W_init = np.random.normal(size=[featDim, projectionDimension])\n",
    "    W = W_init\n",
    "    XW = np.matmul(X, W)\n",
    "    assert XW.shape[1] == projectionDimension\n",
    "    assert XW.shape[0] == len(X)\n",
    "    # Requires [N x d_cap] data matrix of N observations of d_cap-dimension and\n",
    "    # the number of centroids m. Returns, [n x d_cap] centroids and\n",
    "    # elementwise center information.\n",
    "    B, centers = scipy.cluster.vq.kmeans2(XW, numPrototypes)\n",
    "    # Requires two matrices. Number of observations x dimension of observation\n",
    "    # space. Distances[i,j] is the distance between XW[i] and B[j]\n",
    "    distances = scipy.spatial.distance.cdist(XW, B, metric='euclidean')\n",
    "    distances = np.reshape(distances, [-1])\n",
    "    gamma = np.median(distances)\n",
    "    gamma = 1 / (2.5 * gamma)\n",
    "    return gamma.astype('float32'), W.astype('float32'), B.T.astype('float32')\n",
    "\n",
    "\n",
    "def multiClassHingeLoss(logits, label, batch_th):\n",
    "    '''\n",
    "    MultiClassHingeLoss to match C++ Version - No TF internal version\n",
    "    '''\n",
    "    flatLogits = tf.reshape(logits, [-1, ])\n",
    "    label_ = tf.argmax(label, 1)\n",
    "\n",
    "    correctId = tf.range(0, batch_th) * label.shape[1] + label_\n",
    "    correctLogit = tf.gather(flatLogits, correctId)\n",
    "\n",
    "    maxLabel = tf.argmax(logits, 1)\n",
    "    top2, _ = tf.nn.top_k(logits, k=2, sorted=True)\n",
    "\n",
    "    wrongMaxLogit = tf.where(\n",
    "        tf.equal(maxLabel, label_), top2[:, 1], top2[:, 0])\n",
    "\n",
    "    return tf.reduce_mean(tf.nn.relu(1. + wrongMaxLogit - correctLogit))\n",
    "\n",
    "\n",
    "def crossEntropyLoss(logits, label):\n",
    "    '''\n",
    "    Cross Entropy loss for MultiClass case in joint training for\n",
    "    faster convergence\n",
    "    '''\n",
    "    return tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,\n",
    "                                                   labels=tf.stop_gradient(label)))\n",
    "\n",
    "\n",
    "def mean_absolute_error(logits, label):\n",
    "    '''\n",
    "    Function to compute the mean absolute error.\n",
    "    '''\n",
    "    return tf.reduce_mean(tf.abs(tf.subtract(logits, label)))\n",
    "\n",
    "\n",
    "def hardThreshold(A, s):\n",
    "    '''\n",
    "    Hard thresholding function on Tensor A with sparsity s\n",
    "    '''\n",
    "    A_ = np.copy(A)\n",
    "    A_ = A_.ravel()\n",
    "    if len(A_) > 0:\n",
    "        th = np.percentile(np.abs(A_), (1 - s) * 100.0, interpolation='higher')\n",
    "        A_[np.abs(A_) < th] = 0.0\n",
    "    A_ = A_.reshape(A.shape)\n",
    "    return A_\n",
    "\n",
    "\n",
    "def copySupport(src, dest):\n",
    "    '''\n",
    "    copy support of src tensor to dest tensor\n",
    "    '''\n",
    "    support = np.nonzero(src)\n",
    "    dest_ = dest\n",
    "    dest = np.zeros(dest_.shape)\n",
    "    dest[support] = dest_[support]\n",
    "    return dest\n",
    "\n",
    "\n",
    "def countnnZ(A, s, bytesPerVar=4):\n",
    "    '''\n",
    "    Returns # of non-zeros and representative size of the tensor\n",
    "    Uses dense for s >= 0.5 - 4 byte\n",
    "    Else uses sparse - 8 byte\n",
    "    '''\n",
    "    params = 1\n",
    "    hasSparse = False\n",
    "    for i in range(0, len(A.shape)):\n",
    "        params *= int(A.shape[i])\n",
    "    if s < 0.5:\n",
    "        nnZ = np.ceil(params * s)\n",
    "        hasSparse = True\n",
    "        return nnZ, nnZ * 2 * bytesPerVar, hasSparse\n",
    "    else:\n",
    "        nnZ = params\n",
    "        return nnZ, nnZ * bytesPerVar, hasSparse\n",
    "\n",
    "\n",
    "def getConfusionMatrix(predicted, target, numClasses):\n",
    "    '''\n",
    "    Returns a confusion matrix for a multiclass classification\n",
    "    problem. `predicted` is a 1-D array of integers representing\n",
    "    the predicted classes and `target` is the target classes.\n",
    "\n",
    "    confusion[i][j]: Number of elements of class j\n",
    "        predicted as class i\n",
    "    Labels are assumed to be in range(0, numClasses)\n",
    "    Use`printFormattedConfusionMatrix` to echo the confusion matrix\n",
    "    in a user friendly form.\n",
    "    '''\n",
    "    assert(predicted.ndim == 1)\n",
    "    assert(target.ndim == 1)\n",
    "    arr = np.zeros([numClasses, numClasses])\n",
    "\n",
    "    for i in range(len(predicted)):\n",
    "        arr[predicted[i]][target[i]] += 1\n",
    "    return arr\n",
    "\n",
    "\n",
    "def printFormattedConfusionMatrix(matrix):\n",
    "    '''\n",
    "    Given a 2D confusion matrix, prints it in a human readable way.\n",
    "    The confusion matrix is expected to be a 2D numpy array with\n",
    "    square dimensions\n",
    "    '''\n",
    "    assert(matrix.ndim == 2)\n",
    "    assert(matrix.shape[0] == matrix.shape[1])\n",
    "    RECALL = 'Recall'\n",
    "    PRECISION = 'PRECISION'\n",
    "    print(\"|%s|\" % ('True->'), end='')\n",
    "    for i in range(matrix.shape[0]):\n",
    "        print(\"%7d|\" % i, end='')\n",
    "    print(\"%s|\" % 'Precision')\n",
    "\n",
    "    print(\"|%s|\" % ('-' * len(RECALL)), end='')\n",
    "    for i in range(matrix.shape[0]):\n",
    "        print(\"%s|\" % ('-' * 7), end='')\n",
    "    print(\"%s|\" % ('-' * len(PRECISION)))\n",
    "\n",
    "    precisionlist = np.sum(matrix, axis=1)\n",
    "    recalllist = np.sum(matrix, axis=0)\n",
    "    precisionlist = [matrix[i][i] / x if x !=\n",
    "                     0 else -1 for i, x in enumerate(precisionlist)]\n",
    "    recalllist = [matrix[i][i] / x if x !=\n",
    "                  0 else -1 for i, x in enumerate(recalllist)]\n",
    "    for i in range(matrix.shape[0]):\n",
    "        # len recall = 6\n",
    "        print(\"|%6d|\" % (i), end='')\n",
    "        for j in range(matrix.shape[0]):\n",
    "            print(\"%7d|\" % (matrix[i][j]), end='')\n",
    "        print(\"%s\" % (\" \" * (len(PRECISION) - 7)), end='')\n",
    "        if precisionlist[i] != -1:\n",
    "            print(\"%1.5f|\" % precisionlist[i])\n",
    "        else:\n",
    "            print(\"%7s|\" % \"nan\")\n",
    "\n",
    "    print(\"|%s|\" % ('-' * len(RECALL)), end='')\n",
    "    for i in range(matrix.shape[0]):\n",
    "        print(\"%s|\" % ('-' * 7), end='')\n",
    "    print(\"%s|\" % ('-' * len(PRECISION)))\n",
    "    print(\"|%s|\" % ('Recall'), end='')\n",
    "\n",
    "    for i in range(matrix.shape[0]):\n",
    "        if recalllist[i] != -1:\n",
    "            print(\"%1.5f|\" % (recalllist[i]), end='')\n",
    "        else:\n",
    "            print(\"%7s|\" % \"nan\", end='')\n",
    "\n",
    "    print('%s|' % (' ' * len(PRECISION)))\n",
    "\n",
    "\n",
    "def getPrecisionRecall(cmatrix, label=1):\n",
    "    trueP = cmatrix[label][label]\n",
    "    denom = np.sum(cmatrix, axis=0)[label]\n",
    "    if denom == 0:\n",
    "        denom = 1\n",
    "    recall = trueP / denom\n",
    "    denom = np.sum(cmatrix, axis=1)[label]\n",
    "    if denom == 0:\n",
    "        denom = 1\n",
    "    precision = trueP / denom\n",
    "    return precision, recall\n",
    "\n",
    "\n",
    "def getMacroPrecisionRecall(cmatrix):\n",
    "    # TP + FP\n",
    "    precisionlist = np.sum(cmatrix, axis=1)\n",
    "    # TP + FN\n",
    "    recalllist = np.sum(cmatrix, axis=0)\n",
    "    precisionlist__ = [cmatrix[i][i] / x if x !=\n",
    "                       0 else 0 for i, x in enumerate(precisionlist)]\n",
    "    recalllist__ = [cmatrix[i][i] / x if x !=\n",
    "                    0 else 0 for i, x in enumerate(recalllist)]\n",
    "    precision = np.sum(precisionlist__)\n",
    "    precision /= len(precisionlist__)\n",
    "    recall = np.sum(recalllist__)\n",
    "    recall /= len(recalllist__)\n",
    "    return precision, recall\n",
    "\n",
    "\n",
    "def getMicroPrecisionRecall(cmatrix):\n",
    "    # TP + FP\n",
    "    precisionlist = np.sum(cmatrix, axis=1)\n",
    "    # TP + FN\n",
    "    recalllist = np.sum(cmatrix, axis=0)\n",
    "    num = 0.0\n",
    "    for i in range(len(cmatrix)):\n",
    "        num += cmatrix[i][i]\n",
    "\n",
    "    precision = num / np.sum(precisionlist)\n",
    "    recall = num / np.sum(recalllist)\n",
    "    return precision, recall\n",
    "\n",
    "\n",
    "def getMacroMicroFScore(cmatrix):\n",
    "    '''\n",
    "    Returns macro and micro f-scores.\n",
    "    Refer: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.104.8244&rep=rep1&type=pdf\n",
    "    '''\n",
    "    precisionlist = np.sum(cmatrix, axis=1)\n",
    "    recalllist = np.sum(cmatrix, axis=0)\n",
    "    precisionlist__ = [cmatrix[i][i] / x if x !=\n",
    "                       0 else 0 for i, x in enumerate(precisionlist)]\n",
    "    recalllist__ = [cmatrix[i][i] / x if x !=\n",
    "                    0 else 0 for i, x in enumerate(recalllist)]\n",
    "    macro = 0.0\n",
    "    for i in range(len(precisionlist)):\n",
    "        denom = precisionlist__[i] + recalllist__[i]\n",
    "        numer = precisionlist__[i] * recalllist__[i] * 2\n",
    "        if denom == 0:\n",
    "            denom = 1\n",
    "        macro += numer / denom\n",
    "    macro /= len(precisionlist)\n",
    "\n",
    "    num = 0.0\n",
    "    for i in range(len(precisionlist)):\n",
    "        num += cmatrix[i][i]\n",
    "\n",
    "    denom1 = np.sum(precisionlist)\n",
    "    denom2 = np.sum(recalllist)\n",
    "    pi = num / denom1\n",
    "    rho = num / denom2\n",
    "    denom = pi + rho\n",
    "    if denom == 0:\n",
    "        denom = 1\n",
    "    micro = 2 * pi * rho / denom\n",
    "    return macro, micro\n",
    "\n",
    "\n",
    "def restructreMatrixBonsaiSeeDot(A, nClasses, nNodes):\n",
    "    '''\n",
    "    Restructures a matrix from [nNodes*nClasses, Proj] to \n",
    "    [nClasses*nNodes, Proj] for SeeDot\n",
    "    '''\n",
    "    tempMatrix = np.zeros(A.shape)\n",
    "    rowIndex = 0\n",
    "\n",
    "    for i in range(0, nClasses):\n",
    "        for j in range(0, nNodes):\n",
    "            tempMatrix[rowIndex] = A[j * nClasses + i]\n",
    "            rowIndex += 1\n",
    "\n",
    "    return tempMatrix\n",
    "\n",
    "\n",
    "class GraphManager:\n",
    "    '''\n",
    "    Manages saving and restoring graphs. Designed to be used with EMI-RNN\n",
    "    though is general enough to be useful otherwise as well.\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def checkpointModel(self, saver, sess, modelPrefix,\n",
    "                        globalStep=1000, redirFile=None):\n",
    "        saver.save(sess, modelPrefix, global_step=globalStep)\n",
    "        print('Model saved to %s, global_step %d' % (modelPrefix, globalStep),\n",
    "              file=redirFile)\n",
    "\n",
    "    def loadCheckpoint(self, sess, modelPrefix, globalStep,\n",
    "                       redirFile=None):\n",
    "        metaname = modelPrefix + '-%d.meta' % globalStep\n",
    "        basename = os.path.basename(metaname)\n",
    "        fileList = os.listdir(os.path.dirname(modelPrefix))\n",
    "        fileList = [x for x in fileList if x.startswith(basename)]\n",
    "        assert len(fileList) > 0, 'Checkpoint file not found'\n",
    "        msg = 'Too many or too few checkpoint files for globalStep: %d' % globalStep\n",
    "        assert len(fileList) == 1, msg\n",
    "        chkpt = basename + '/' + fileList[0]\n",
    "        saver = tf.train.import_meta_graph(metaname)\n",
    "        metaname = metaname[:-5]\n",
    "        saver.restore(sess, metaname)\n",
    "        graph = tf.get_default_graph()\n",
    "        return graph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computation Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-19T12:39:06.653612Z",
     "start_time": "2018-08-19T12:39:06.412290Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the linear secondary classifier\n",
    "def createExtendedGraph(self, baseOutput, *args, **kwargs):\n",
    "    W1 = tf.Variable(np.random.normal(size=[NUM_HIDDEN, NUM_OUTPUT]).astype('float32'), name='W1')\n",
    "    B1 = tf.Variable(np.random.normal(size=[NUM_OUTPUT]).astype('float32'), name='B1')\n",
    "    y_cap = tf.add(tf.tensordot(baseOutput, W1, axes=1), B1, name='y_cap_tata')\n",
    "    self.output = y_cap\n",
    "    self.graphCreated = True\n",
    "\n",
    "def restoreExtendedGraph(self, graph, *args, **kwargs):\n",
    "    y_cap = graph.get_tensor_by_name('y_cap_tata:0')\n",
    "    self.output = y_cap\n",
    "    self.graphCreated = True\n",
    "    \n",
    "def feedDictFunc(self, keep_prob=None, inference=False, **kwargs):\n",
    "    if inference is False:\n",
    "        feedDict = {self._emiGraph.keep_prob: keep_prob}\n",
    "    else:\n",
    "        feedDict = {self._emiGraph.keep_prob: 1.0}\n",
    "    return feedDict\n",
    "\n",
    "    \n",
    "EMI_FastGRNN._createExtendedGraph = createExtendedGraph\n",
    "EMI_FastGRNN._restoreExtendedGraph = restoreExtendedGraph\n",
    "if USE_DROPOUT is True:\n",
    "    EMI_FastGRNN.feedDictFunc = feedDictFunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-19T12:39:06.701740Z",
     "start_time": "2018-08-19T12:39:06.655328Z"
    }
   },
   "outputs": [],
   "source": [
    "inputPipeline = EMI_DataPipeline(NUM_SUBINSTANCE, NUM_TIMESTEPS, NUM_FEATS, NUM_OUTPUT)\n",
    "emiFastGRNN = EMI_FastGRNN(NUM_SUBINSTANCE, NUM_HIDDEN, NUM_TIMESTEPS, NUM_FEATS, wRank=WRANK, uRank=URANK, \n",
    "                           gate_non_linearity=GATE_NL, update_non_linearity=UPDATE_NL, useDropout=USE_DROPOUT)\n",
    "emiTrainer = EMI_Trainer(NUM_TIMESTEPS, NUM_OUTPUT, lossType='xentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape is: (6108, 4, 88, 8)\n",
      "y_train shape is: (6108, 4, 3)\n",
      "x_test shape is: (679, 4, 88, 8)\n",
      "y_test shape is: (679, 4, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"x_train shape is:\", x_train.shape)\n",
    "print(\"y_train shape is:\", y_train.shape)\n",
    "print(\"x_test shape is:\", x_val.shape)\n",
    "print(\"y_test shape is:\", y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-19T12:39:14.187456Z",
     "start_time": "2018-08-19T12:39:06.703481Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_4024/3576119089.py:2011: static_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell, unroll=True)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/deepin/.local/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:458: calling RandomNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/deepin/.local/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:458: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'EMI_Trainer' object has no attribute '_EMI_Trainer__createTrainOp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [16], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m y_cap \u001b[39m=\u001b[39m emiFastGRNN(x_batch)\n\u001b[1;32m      8\u001b[0m \u001b[39m# Create loss graphs and training routines\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m emiTrainer(y_cap, y_batch)\n",
      "Cell \u001b[0;32mIn [11], line 109\u001b[0m, in \u001b[0;36mEMI_Trainer.__call__\u001b[0;34m(self, predicted, target)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__validInit \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgraph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 109\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_createGraph(predicted, target)\n\u001b[1;32m    110\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    111\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restoreGraph(predicted, target)\n",
      "Cell \u001b[0;32mIn [11], line 189\u001b[0m, in \u001b[0;36mEMI_Trainer._createGraph\u001b[0;34m(self, predicted, target)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccTilda \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mreduce_mean(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mequalTilda, name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39macc-tilda\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    188\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlossOp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__createLossOp(predicted, target)\n\u001b[0;32m--> 189\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainOp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__createTrainOp()\n\u001b[1;32m    190\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mautomode:\n\u001b[1;32m    191\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreateOpCollections()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'EMI_Trainer' object has no attribute '_EMI_Trainer__createTrainOp'"
     ]
    }
   ],
   "source": [
    "#tf.reset_default_graph()\n",
    "g1 = tf.Graph()    \n",
    "with g1.as_default():\n",
    "    # Obtain the iterators to each batch of the data\n",
    "    x_batch, y_batch = inputPipeline()\n",
    "    # Create the forward computation graph based on the iterators\n",
    "    y_cap = emiFastGRNN(x_batch)\n",
    "    # Create loss graphs and training routines\n",
    "    emiTrainer(y_cap, y_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EMI Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-19T12:51:45.803360Z",
     "start_time": "2018-08-19T12:39:14.189648Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Have you invoked __call__()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [88], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mwith\u001b[39;00m g1\u001b[39m.\u001b[39mas_default():\n\u001b[0;32m----> 2\u001b[0m     emiDriver \u001b[39m=\u001b[39m EMI_Driver(inputPipeline, emiFastGRNN, emiTrainer)\n\u001b[1;32m      4\u001b[0m emiDriver\u001b[39m.\u001b[39minitializeSession(g1)\n\u001b[1;32m      5\u001b[0m y_updated, modelStats \u001b[39m=\u001b[39m emiDriver\u001b[39m.\u001b[39mrun(numClasses\u001b[39m=\u001b[39mNUM_OUTPUT, x_train\u001b[39m=\u001b[39mx_train,\n\u001b[1;32m      6\u001b[0m                                       y_train\u001b[39m=\u001b[39my_train, bag_train\u001b[39m=\u001b[39mBAG_TRAIN,\n\u001b[1;32m      7\u001b[0m                                       x_val\u001b[39m=\u001b[39mx_val, y_val\u001b[39m=\u001b[39my_val, bag_val\u001b[39m=\u001b[39mBAG_VAL,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m                                       numEpochs\u001b[39m=\u001b[39mNUM_EPOCHS, modelPrefix\u001b[39m=\u001b[39mMODEL_PREFIX,\n\u001b[1;32m     11\u001b[0m                                       fracEMI\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m, updatePolicy\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtop-k\u001b[39m\u001b[39m'\u001b[39m, k\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn [81], line 327\u001b[0m, in \u001b[0;36mEMI_Driver.__init__\u001b[0;34m(self, emiDataPipeline, emiGraph, emiTrainer, max_to_keep, globalStepStart)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataPipe\u001b[39m.\u001b[39mgraphCreated \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m, msg\n\u001b[1;32m    326\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_emiGraph\u001b[39m.\u001b[39mgraphCreated \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m, msg\n\u001b[0;32m--> 327\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_emiTrainer\u001b[39m.\u001b[39mgraphCreated \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m, msg\n\u001b[1;32m    328\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__globalStep \u001b[39m=\u001b[39m globalStepStart\n\u001b[1;32m    329\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__saver \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mtrain\u001b[39m.\u001b[39mSaver(max_to_keep\u001b[39m=\u001b[39mmax_to_keep,\n\u001b[1;32m    330\u001b[0m                               save_relative_paths\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Have you invoked __call__()"
     ]
    }
   ],
   "source": [
    "with g1.as_default():\n",
    "    emiDriver = EMI_Driver(inputPipeline, emiFastGRNN, emiTrainer)\n",
    "\n",
    "emiDriver.initializeSession(g1)\n",
    "y_updated, modelStats = emiDriver.run(numClasses=NUM_OUTPUT, x_train=x_train,\n",
    "                                      y_train=y_train, bag_train=BAG_TRAIN,\n",
    "                                      x_val=x_val, y_val=y_val, bag_val=BAG_VAL,\n",
    "                                      numIter=NUM_ITER, keep_prob=KEEP_PROB,\n",
    "                                      numRounds=NUM_ROUNDS, batchSize=BATCH_SIZE,\n",
    "                                      numEpochs=NUM_EPOCHS, modelPrefix=MODEL_PREFIX,\n",
    "                                      fracEMI=0.5, updatePolicy='top-k', k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-19T12:51:45.832728Z",
     "start_time": "2018-08-19T12:51:45.805984Z"
    }
   },
   "outputs": [],
   "source": [
    "# Early Prediction Policy: We make an early prediction based on the predicted classes\n",
    "#     probability. If the predicted class probability > minProb at some step, we make\n",
    "#     a prediction at that step.\n",
    "def earlyPolicy_minProb(instanceOut, minProb, **kwargs):\n",
    "    assert instanceOut.ndim == 2\n",
    "    classes = np.argmax(instanceOut, axis=1)\n",
    "    prob = np.max(instanceOut, axis=1)\n",
    "    index = np.where(prob >= minProb)[0]\n",
    "    if len(index) == 0:\n",
    "        assert (len(instanceOut) - 1) == (len(classes) - 1)\n",
    "        return classes[-1], len(instanceOut) - 1\n",
    "    index = index[0]\n",
    "    return classes[index], index\n",
    "\n",
    "def getEarlySaving(predictionStep, numTimeSteps, returnTotal=False):\n",
    "    predictionStep = predictionStep + 1\n",
    "    predictionStep = np.reshape(predictionStep, -1)\n",
    "    totalSteps = np.sum(predictionStep)\n",
    "    maxSteps = len(predictionStep) * numTimeSteps\n",
    "    savings = 1.0 - (totalSteps / maxSteps)\n",
    "    if returnTotal:\n",
    "        return savings, totalSteps\n",
    "    return savings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-19T12:51:46.210240Z",
     "start_time": "2018-08-19T12:51:45.834534Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at k = 2: 0.998567\n",
      "Additional savings: 0.960761\n"
     ]
    }
   ],
   "source": [
    "k = 2\n",
    "predictions, predictionStep = emiDriver.getInstancePredictions(x_test, y_test, earlyPolicy_minProb, minProb=0.99)\n",
    "bagPredictions = emiDriver.getBagPredictions(predictions, minSubsequenceLen=k, numClass=NUM_OUTPUT)\n",
    "print('Accuracy at k = %d: %f' % (k,  np.mean((bagPredictions == BAG_TEST).astype(int))))\n",
    "print('Additional savings: %f' % getEarlySaving(predictionStep, NUM_TIMESTEPS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-19T12:51:46.677691Z",
     "start_time": "2018-08-19T12:51:46.212285Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   len       acc  macro-fsc  macro-pre  macro-rec  micro-fsc  micro-pre  \\\n",
      "0    1  0.998831   0.998532   0.998504   0.998561   0.998831   0.998831   \n",
      "1    2  0.998567   0.998295   0.998517   0.998074   0.998567   0.998567   \n",
      "2    3  0.997850   0.997675   0.998287   0.997069   0.997850   0.997850   \n",
      "3    4  0.996040   0.995706   0.997310   0.994133   0.996040   0.996040   \n",
      "\n",
      "   micro-rec  \n",
      "0   0.998831  \n",
      "1   0.998567  \n",
      "2   0.997850  \n",
      "3   0.996040  \n",
      "Max accuracy 0.998831 at subsequencelength 1\n",
      "Max micro-f 0.998831 at subsequencelength 1\n",
      "Micro-precision 0.998831 at subsequencelength 1\n",
      "Micro-recall 0.998831 at subsequencelength 1\n",
      "Max macro-f 0.998532 at subsequencelength 1\n",
      "macro-precision 0.998504 at subsequencelength 1\n",
      "macro-recall 0.998561 at subsequencelength 1\n"
     ]
    }
   ],
   "source": [
    "# A slightly more detailed analysis method is provided. \n",
    "df = emiDriver.analyseModel(predictions, BAG_TEST, NUM_SUBINSTANCE, NUM_OUTPUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Picking the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-19T13:06:04.024660Z",
     "start_time": "2018-08-19T13:04:47.045787Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/sf/data/WESAD/Fast_GRNN/88_30/models/model-fgrnn-1003\n",
      "Round:  0, Validation accuracy: 0.9927, Test Accuracy (k = 2): 0.960361, Additional savings: 0.372858\n",
      "INFO:tensorflow:Restoring parameters from /home/sf/data/WESAD/Fast_GRNN/88_30/models/model-fgrnn-1007\n",
      "Round:  1, Validation accuracy: 0.9973, Test Accuracy (k = 2): 0.926303, Additional savings: 0.508829\n",
      "INFO:tensorflow:Restoring parameters from /home/sf/data/WESAD/Fast_GRNN/88_30/models/model-fgrnn-1011\n",
      "Round:  2, Validation accuracy: 0.9964, Test Accuracy (k = 2): 0.950743, Additional savings: 0.585428\n",
      "INFO:tensorflow:Restoring parameters from /home/sf/data/WESAD/Fast_GRNN/88_30/models/model-fgrnn-1015\n",
      "Round:  3, Validation accuracy: 0.9972, Test Accuracy (k = 2): 0.997435, Additional savings: 0.945828\n",
      "INFO:tensorflow:Restoring parameters from /home/sf/data/WESAD/Fast_GRNN/88_30/models/model-fgrnn-1018\n",
      "Round:  4, Validation accuracy: 0.9985, Test Accuracy (k = 2): 0.998491, Additional savings: 0.956765\n",
      "INFO:tensorflow:Restoring parameters from /home/sf/data/WESAD/Fast_GRNN/88_30/models/model-fgrnn-1021\n",
      "Round:  5, Validation accuracy: 0.9985, Test Accuracy (k = 2): 0.998567, Additional savings: 0.960761\n"
     ]
    }
   ],
   "source": [
    "devnull = open(os.devnull, 'r')\n",
    "for val in modelStats:\n",
    "    round_, acc, modelPrefix, globalStep = val\n",
    "    emiDriver.loadSavedGraphToNewSession(modelPrefix, globalStep, redirFile=devnull)\n",
    "    predictions, predictionStep = emiDriver.getInstancePredictions(x_test, y_test, earlyPolicy_minProb,\n",
    "                                                               minProb=0.99, keep_prob=1.0)\n",
    " \n",
    "    bagPredictions = emiDriver.getBagPredictions(predictions, minSubsequenceLen=k, numClass=NUM_OUTPUT)\n",
    "    print(\"Round: %2d, Validation accuracy: %.4f\" % (round_, acc), end='')\n",
    "    print(', Test Accuracy (k = %d): %f, ' % (k,  np.mean((bagPredictions == BAG_TEST).astype(int))), end='')\n",
    "    print('Additional savings: %f' % getEarlySaving(predictionStep, NUM_TIMESTEPS)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=\"WESAD\"\n",
    "model=\"fast-grnn\"\n",
    "params = {\n",
    "    \"NUM_HIDDEN\" : 128,\n",
    "    \"NUM_TIMESTEPS\" : 700, #subinstance length.\n",
    "    \"NUM_FEATS\" : 8,\n",
    "    \"FORGET_BIAS\" : 1.0,\n",
    "    \"NUM_OUTPUT\" : 3,\n",
    "    \"USE_DROPOUT\" : 0, # '1' -> True. '0' -> False\n",
    "    \"KEEP_PROB\" : 0.9,\n",
    "    \"UPDATE_NL\" : \"quantTanh\",\n",
    "    \"GATE_NL\" : \"quantSigm\",\n",
    "    \"WRANK\" : 5,\n",
    "    \"URANK\" : 6,\n",
    "    \"PREFETCH_NUM\" : 5,\n",
    "    \"BATCH_SIZE\" : 175,\n",
    "    \"NUM_EPOCHS\" : 3,\n",
    "    \"NUM_ITER\" : 4,\n",
    "    \"NUM_ROUNDS\" : 4,\n",
    "    \"MODEL_PREFIX\" : dataset + '/model-' + str(model)\n",
    "}\n",
    "\n",
    "fast_dict = {**params}\n",
    "fast_dict[\"k\"] = k\n",
    "fast_dict[\"accuracy\"] = np.mean((bagPredictions == BAG_TEST).astype(int))\n",
    "fast_dict[\"additional_savings\"] = getEarlySaving(predictionStep, NUM_TIMESTEPS)\n",
    "fast_dict[\"y_test\"] = BAG_TEST\n",
    "fast_dict[\"y_pred\"] = bagPredictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   len       acc  macro-fsc  macro-pre  macro-rec  micro-fsc  micro-pre  \\\n",
      "0    1  0.998831   0.998532   0.998504   0.998561   0.998831   0.998831   \n",
      "1    2  0.998567   0.998295   0.998517   0.998074   0.998567   0.998567   \n",
      "2    3  0.997850   0.997675   0.998287   0.997069   0.997850   0.997850   \n",
      "3    4  0.996040   0.995706   0.997310   0.994133   0.996040   0.996040   \n",
      "\n",
      "   micro-rec  \n",
      "0   0.998831  \n",
      "1   0.998567  \n",
      "2   0.997850  \n",
      "3   0.996040  \n",
      "Max accuracy 0.998831 at subsequencelength 1\n",
      "Max micro-f 0.998831 at subsequencelength 1\n",
      "Micro-precision 0.998831 at subsequencelength 1\n",
      "Micro-recall 0.998831 at subsequencelength 1\n",
      "Max macro-f 0.998532 at subsequencelength 1\n",
      "macro-precision 0.998504 at subsequencelength 1\n",
      "macro-recall 0.998561 at subsequencelength 1\n",
      "+----+-------+----------+-------------+-------------+-------------+-------------+-------------+-------------+\n",
      "|    |   len |      acc |   macro-fsc |   macro-pre |   macro-rec |   micro-fsc |   micro-pre |   micro-rec |\n",
      "+====+=======+==========+=============+=============+=============+=============+=============+=============+\n",
      "|  0 |     1 | 0.998831 |    0.998532 |    0.998504 |    0.998561 |    0.998831 |    0.998831 |    0.998831 |\n",
      "+----+-------+----------+-------------+-------------+-------------+-------------+-------------+-------------+\n",
      "|  1 |     2 | 0.998567 |    0.998295 |    0.998517 |    0.998074 |    0.998567 |    0.998567 |    0.998567 |\n",
      "+----+-------+----------+-------------+-------------+-------------+-------------+-------------+-------------+\n",
      "|  2 |     3 | 0.99785  |    0.997675 |    0.998287 |    0.997069 |    0.99785  |    0.99785  |    0.99785  |\n",
      "+----+-------+----------+-------------+-------------+-------------+-------------+-------------+-------------+\n",
      "|  3 |     4 | 0.99604  |    0.995706 |    0.99731  |    0.994133 |    0.99604  |    0.99604  |    0.99604  |\n",
      "+----+-------+----------+-------------+-------------+-------------+-------------+-------------+-------------+\n",
      "Results for this run have been saved at /home/sf/data/WESAD/Fast_GRNN/ .\n"
     ]
    }
   ],
   "source": [
    "# A slightly more detailed analysis method is provided. \n",
    "df = emiDriver.analyseModel(predictions, BAG_TEST, NUM_SUBINSTANCE, NUM_OUTPUT)\n",
    "print (tabulate(df, headers=list(df.columns), tablefmt='grid'))\n",
    "\n",
    "dirname = \"/home/sf/data/WESAD/Fast_GRNN/\"\n",
    "pathlib.Path(dirname).mkdir(parents=True, exist_ok=True)\n",
    "print (\"Results for this run have been saved at\" , dirname, \".\")\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "filename = list((str(now.year),\"-\",str(now.month),\"-\",str(now.day),\"|\",str(now.hour),\"-\",str(now.minute)))\n",
    "filename = ''.join(filename)\n",
    "\n",
    "#Save the dictionary containing the params and the results.\n",
    "pkl.dump(fast_dict,open(dirname + \"/fast_dict_\" + filename + \".pkl\",mode='wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
